{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to Bunka Documentation","text":"<p>Welcome to the documentation for Bunka, an advanced library for natural language processing and data visualization.</p> <p> </p> <p></p> <p>Bunkatopics is a package designed for Topic Modeling Visualization, Frame Analysis, and Retrieval Augmented Generation (RAG) tasks, harnessing the power of Large Language Models (LLMs). Its primary goal is to assist developers in gaining insights from unstructured data, potentially facilitating data cleansing and optimizing LLMs through fine-tuning processes. Bunkatopics is constructed using well-known libraries like langchain, chroma, and transformers, enabling seamless integration into various environments.</p> <p>Discover the different Use Case:</p> <ul> <li> <p>Content Overview: The Medium website offers a wealth of content across various categories such as Data Science, Technology, Programming, Poetry, Cryptocurrency, Machine Learning, Life, and more. While these categories facilitate data searching, they may not provide a granular overview. For instance, within the Technology category, what specific topics does Medium cover?</p> </li> <li> <p>Fine-Tuning: To achieve precise fine-tuning, it's crucial to exercise control over the data, filtering what is relevant and discarding what isn't. Bunka is a valuable tool for accomplishing this task.</p> </li> <li> <p>Framing Analysis: Data can be analyzed in countless ways, contingent on your objectives and interests. We've developed a tool that enables you to visualize data by semantically customizing your own axes.</p> </li> </ul> <p></p>"},{"location":"index.html#discover-different-examples-using-our-google-colab-notebooks","title":"Discover different examples using our Google Colab Notebooks","text":"Theme Google Colab Link Visual Topic Modeling with Bunka Cleaning dataset for fine-tuning LLM using Bunka Understanding a dataset using Frame Analysis with Bunka Full Introduction to Topic Modeling, Data Cleaning and Frame Analysis with Bunka."},{"location":"index.html#installation-via-pip","title":"Installation via Pip","text":"<pre><code>pip install bunkatopics\n</code></pre>"},{"location":"index.html#installation-via-git-clone","title":"Installation via Git Clone","text":"<pre><code>git clone https://github.com/charlesdedampierre/BunkaTopics.git\ncd BunkaTopics\npip install -e .\n</code></pre>"},{"location":"index.html#quick-start","title":"Quick Start","text":""},{"location":"index.html#uploading-sample-data","title":"Uploading Sample Data","text":"<p>To get started, let's upload a sample of Medium Articles into Bunkatopics:</p> <pre><code>from datasets import load_dataset\ndocs = load_dataset(\"bunkalab/medium-sample-technology\")[\"train\"][\"title\"] # 'docs' is a list of text [text1, text2, ..., textN]\n</code></pre>"},{"location":"index.html#choose-your-embedding-model","title":"Choose Your Embedding Model","text":"<p>Bunkatopics offers seamless integration with Huggingface's extensive collection of embedding models. You can select from a wide range of models, but be mindful of their size. Please refer to the langchain documentation for details on available models.</p> <pre><code>from bunkatopics import Bunka\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n\nembedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",# We recommend starting with a small model\n                                        model_kwargs={\"device\": \"cpu\"}, # Or cuda if you have GPU\n                                        encode_kwargs={\"show_progress_bar\": True}, # Show the progress of embeddings\n                                        multi_process=False)  # set to True if you have mutliprocessing\n\n# Initialize Bunka with your chosen model and language preference\nbunka = Bunka(embedding_model=embedding_model, language='english') # You can choose any language you prefer\n\n# Fit Bunka to your text data\nbunka.fit(docs)\n</code></pre> <pre><code>&gt;&gt;&gt; bunka.get_topics(n_clusters=15, name_length=5)# Specify the number of terms to describe each topic\n</code></pre> <p>Topics are described by the most specific terms belonging to the cluster.</p> topic_id topic_name size percent bt-12 technology - Tech - Children - student - days 322 10.73 bt-11 blockchain - Cryptocurrency - sense - Cryptocurrencies - Impact 283 9.43 bt-7 gadgets - phone - Device - specifications - screen 258 8.6 bt-8 software - Kubernetes - ETL - REST - Salesforce 258 8.6 bt-1 hackathon - review - Recap - Predictions - Lessons 257 8.57 bt-4 Reality - world - cities - future - Lot 246 8.2 bt-14 Product - Sales - day - dream - routine 241 8.03 bt-0 Words - Robots - discount - NordVPN - humans 208 6.93 bt-2 Internet - Overview - security - Work - Development 202 6.73 bt-13 Course - Difference - Step - science - Point 192 6.4 bt-6 quantum - Cars - Way - Game - quest 162 5.4 bt-3 Objects - Strings - app - Programming - Functions 119 3.97 bt-5 supply - chain - revolution - Risk - community 119 3.97 bt-9 COVID - printing - Car - work - app 89 2.97 bt-10 Episode - HD - Secrets - TV 44 1.47"},{"location":"index.html#visualize-your-topics","title":"Visualize Your Topics","text":"<p>Finally, let's visualize the topics that Bunka has computed for your text data:</p> <pre><code>&gt;&gt;&gt; bunka.visualize_topics(width=800, height=800, colorscale='YIGnBu')\n</code></pre> <p></p>"},{"location":"index.html#topic-modeling-with-genai-summarization-of-topics","title":"Topic Modeling with GenAI Summarization of Topics","text":"<p>Explore the power of Generative AI for summarizing topics! We use the 7B-instruct model of Mistral AI from the huggingface hub using the langchain framework.</p> <pre><code>from langchain.llms import HuggingFaceHub\n\n# Define the repository ID for Mistral-7B-v0.1\nrepo_id = 'mistralai/Mistral-7B-v0.1'\n\n# Using Mistral AI to Summarize the Topics\nllm = HuggingFaceHub(repo_id='mistralai/Mistral-7B-v0.1', huggingfacehub_api_token=\"HF_TOKEN\")\n\n# Obtain clean topic names using Generative Model\nbunka.get_clean_topic_name(generative_model=llm, language='english')\nbunka.visualize_topics( width=800, height=800, colorscale = 'Portland')\n</code></pre> <p>Finally, let's visualize again the topics. We can chose from different colorscales.</p> <pre><code>&gt;&gt;&gt; bunka.visualize_topics(width=800, height=800)\n</code></pre> YlGnBu Portland delta Blues <p>We can now access the newly made topics</p> <pre><code>&gt;&gt;&gt; bunka.df_topics_\n</code></pre> topic_id topic_name size percent bt-1 Cryptocurrency Impact 345 12.32 bt-3 Data Management Technologies 243 8.68 bt-14 Everyday Life 230 8.21 bt-0 Digital Learning Campaign 225 8.04 bt-12 Business Development 223 7.96 bt-2 Technology Devices 212 7.57 bt-10 Market Predictions Recap 201 7.18 bt-4 Comprehensive Learning Journey 187 6.68 bt-6 Future of Work 185 6.61 bt-11 Internet Discounts 175 6.25 bt-5 Technological Urban Water Management 172 6.14 bt-9 Electric Vehicle Technology 145 5.18 bt-8 Programming Concepts 116 4.14 bt-13 Quantum Technology Industries 105 3.75 bt-7 High Definition Television (HDTV) 36 1.29"},{"location":"index.html#manually-cleaning-the-topics","title":"Manually Cleaning the topics","text":"<p>If you are not happy with the resulting topics, you can change them manually. Click on Apply changes when you are done. In the example, we changed the topic Cryptocurrency Impact to Cryptocurrency and Internet Discounts to Advertising.</p> <pre><code>&gt;&gt;&gt; bunka.manually_clean_topics()\n</code></pre> <p></p>"},{"location":"index.html#removing-data-based-on-topics-for-fine-tuning-purposes","title":"Removing Data based on topics for fine-tuning purposes","text":"<p>You have the flexibility to construct a customized dataset by excluding topics that do not align with your interests. For instance, in the provided example, we omitted topics associated with Advertising and High-Definition television, as these clusters primarily contain promotional content that we prefer not to include in our model's training data.</p> <pre><code>&gt;&gt;&gt; bunka.clean_data_by_topics()\n</code></pre> <p></p> <pre><code>&gt;&gt;&gt; bunka.df_cleaned_\n</code></pre> doc_id content topic_id topic_name 873ba315 Invisibilize Data With JavaScript bt-8 Programming Concepts 1243d58f Why End-to-End Testing is Important for Your Team bt-3 Data Management Technologies 45fb8166 This Tiny Wearable Device Uses Your Body Heat... bt-2 Technology Devices a122d1d2 Digital Policy Salon: The Next Frontier bt-0 Digital Learning Campaign 1bbcfc1c Preparing Hardware for Outdoor Creative Technology Installations bt-5 Technological Urban Water Management 79580c34 Angular Or React ? bt-8 Programming Concepts af0b08a2 Ed-Tech Startups Are Cashing in on Parents\u2019 Insecurities bt-0 Digital Learning Campaign 2255c350 Former Google CEO Wants to Create a Government-Funded University to Train A.I. Coders bt-6 Future of Work d2bc4b33 Applying Action &amp; The Importance of Ideas bt-12 Business Development 5219675e Why You Should (not?) Use Signal bt-2 Technology Devices ... ... ... ..."},{"location":"index.html#bourdieu-map","title":"Bourdieu Map","text":"<p>The Bourdieu map provides a 2-Dimensional unsupervised scale to visualize various texts. Each region on the map represents a distinct topic, characterized by its most specific terms. Clusters are formed, and their names are succinctly summarized using Generative AI.</p> <p>The significance of this visualization lies in its ability to define axes, thereby creating continuums that reveal data distribution patterns. This concept draws inspiration from the work of the renowned French sociologist Bourdieu, who employed 2-Dimensional maps to project items and gain insights.</p> <pre><code>from langchain.llms import HuggingFaceHub\n\n# Define the HuggingFaceHub instance with the repository ID and API token\nllm = HuggingFaceHub(\n    repo_id='mistralai/Mistral-7B-v0.1',\n    huggingfacehub_api_token=\"HF_TOKEN\"\n)\n\n## Bourdieu Fig\nbourdieu_fig = bunka.visualize_bourdieu(\n        llm=llm,\n        x_left_words=[\"This is about business\"],\n        x_right_words=[\"This is about politics\"],\n        y_top_words=[\"this is about startups\"],\n        y_bottom_words=[\"This is about governments\"],\n        height=800,\n        width=800,\n        clustering=True,\n        topic_n_clusters=10,\n        density=False,\n        convex_hull=True,\n        radius_size=0.2,\n        label_size_ratio_clusters=80)\n</code></pre> <pre><code>&gt;&gt;&gt; bourdieu_fig.show()\n</code></pre> positive/negative vs humans/machines politics/business vs humans/machines politics/business vs     positive/negative politics/business vs startups/governments"},{"location":"index.html#front-end","title":"Front-end","text":"<p>This is a beta feature. First, git clone the repository</p> <pre><code>git clone https://github.com/charlesdedampierre/BunkaTopics.git\ncd BunkaTopics\npip install -e .\n</code></pre> <p>Then carry out a Topic Modeling and launch the serveur:</p> <pre><code>from bunkatopics import Bunka\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# Choose your embedding model\nembedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # We recommend starting with a small model\n\n# Initialize Bunka with your chosen model and language preference\nbunka = Bunka(embedding_model=embedding_model, language='english') # You can choose any language you prefer\n\n# Fit Bunka to your text data\nbunka.fit(docs)\nbunka.get_topics(n_clusters=15, name_length=3)# Specify the number of terms to describe each topic\n</code></pre> <pre><code>&gt;&gt;&gt; bunka.start_server() # A serveur will open on your computer at http://localhost:3000/ \n</code></pre>"},{"location":"changelog.html","title":"Bunkatopics Version 1.0.0 Release Notes","text":"<p>In this major update, we have completely revamped our package to align it with the latest framework and to better facilitate data cleaning for the next generation of Large Language Models (LLMs).</p>"},{"location":"changelog.html#key-highlights","title":"Key Highlights","text":"<p>In this major update, we have completely revamped our package to align it with the latest framework and to better facilitate data cleaning for the next generation of Large Language Models (LLMs).Bunkatopics version 0.45 continues to serve as a versatile package catering to a wide range of tasks, including Topic Modeling Visualization, Frame Analysis and cleaning capabilities.</p> <ul> <li> <p>Utilizing Large Language Models: Our package uses the power of Large Language Models (LLMs) to empower developers in extracting valuable insights from unstructured data.</p> </li> <li> <p>Integration of well-known Libraries: Bunkatopics is now constructed using renowned libraries such as langchain, chroma, and transformers, ensuring seamless integration into diverse development environments.</p> </li> <li> <p>In-depth insights: Bunkatopics excels at providing in-depth insights into specific topics within categories, such as exploring Technology topics on the Medium website.</p> </li> <li> <p>Framing Analysis: Bunkatopics introduces a 2-dimensional unsupervised scale, the Bourdieu map, facilitating the visualization of textual data and topics. This feature offers valuable insights into data distribution patterns.</p> </li> <li> <p>Manually Cleaning Topics: Users can now exercise greater flexibility and customization by manually changing topic names or labels to better suit their needs.</p> </li> <li> <p>Data Filtering by Topics. This release enables users to construct tailored datasets by excluding topics that do not align with their specific interests, simplifying the process of fine-tuning models.</p> </li> </ul>"},{"location":"contribution.html","title":"Contributing","text":""},{"location":"contribution.html#contribution","title":"Contribution","text":"<p>We greatly appreciate your interest in contributing to Bunkatopics. Being an open-source project in a fast-changing field, we enthusiastically welcome contributions in various forms, such as new features, infrastructure improvements, enhanced documentation, or bug fixes. Your contributions are invaluable to us. You can get involved in several ways:</p> <p>Documentation: Contribute to enhancing our documentation. Code: Assist in coding, bug fixes, and development. Integrations: Collaborate on integrating Bunkatopics with other projects and tools.</p> <p>\ud83d\udea9 GitHub Issues:https://github.com/charlesdedampierre/BunkaTopics/issues</p> <p>Thanks!</p>"},{"location":"fine-tuning.html","title":"Cleaning Datasets for models Fine-tuning","text":"<p>To achieve precise fine-tuning, it's crucial to exercise control over the data, filtering what is relevant and discarding what isn't. Bunka is a valuable tool for accomplishing this task. You can remove cliusters of information automatically and in a few seconds.</p> <p></p> Theme Google Colab Link Data Cleaning"},{"location":"fine-tuning.html#installation-via-pip","title":"Installation via Pip","text":"<pre><code>pip install bunkatopics\n</code></pre>"},{"location":"fine-tuning.html#installation-via-git-clone","title":"Installation via Git Clone","text":"<pre><code>git clone https://github.com/charlesdedampierre/BunkaTopics.git\ncd BunkaTopics\npip install -e .\n</code></pre>"},{"location":"fine-tuning.html#quick-start","title":"Quick Start","text":""},{"location":"fine-tuning.html#uploading-sample-data","title":"Uploading Sample Data","text":"<p>To get started, let's upload a sample of Medium Articles into Bunkatopics:</p> <pre><code>from datasets import load_dataset\ndocs = load_dataset(\"bunkalab/medium-sample-technology\")[\"train\"][\"title\"]\n</code></pre>"},{"location":"fine-tuning.html#choose-your-embedding-model","title":"Choose Your Embedding Model","text":"<p>Bunkatopics offers seamless integration with Huggingface's extensive collection of embedding models. You can select from a wide range of models, but be mindful of their size. Please refer to the langchain documentation for details on available models.</p> <pre><code>from bunkatopics import Bunka\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# Choose your embedding model\nembedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")  # set to True if you have mutliprocessing\n\n# Initialize Bunka with your chosen model\nbunka = Bunka(embedding_model=embedding_model)\n\n# Fit Bunka to your text data\nbunka.fit(docs)\n\n# Get a list of topics\nprint(df_topics)\n</code></pre> <pre><code>&gt;&gt;&gt; bunka.get_topics(n_clusters=15, name_length=3)# Specify the number of terms to describe each topic\n</code></pre> topic_id topic_name size percent bt-12 technology - Tech - Children - student - days 322 10.73 bt-11 blockchain - Cryptocurrency - sense - Cryptocurrencies - Impact 283 9.43 bt-7 gadgets - phone - Device - specifications - screen 258 8.6 bt-8 software - Kubernetes - ETL - REST - Salesforce 258 8.6 bt-1 hackathon - review - Recap - Predictions - Lessons 257 8.57 bt-4 Reality - world - cities - future - Lot 246 8.2 bt-14 Product - Sales - day - dream - routine 241 8.03 bt-0 Words - Robots - discount - NordVPN - humans 208 6.93 bt-2 Internet - Overview - security - Work - Development 202 6.73 bt-13 Course - Difference - Step - science - Point 192 6.4 bt-6 quantum - Cars - Way - Game - quest 162 5.4 bt-3 Objects - Strings - app - Programming - Functions 119 3.97 bt-5 supply - chain - revolution - Risk - community 119 3.97 bt-9 COVID - printing - Car - work - app 89 2.97 bt-10 Episode - HD - Secrets - TV 44 1.47"},{"location":"fine-tuning.html#topic-modeling-with-genai-summarization-of-topics","title":"Topic Modeling with GenAI Summarization of Topics","text":"<p>Explore the power of Generative AI for summarizing topics! We use the 7B-instruct model of Mistral AI from the huggingface hub using the langchain framework.</p> <pre><code>from langchain.llms import HuggingFaceHub\n\n# Define the repository ID for Mistral-7B-v0.1\nrepo_id = 'mistralai/Mistral-7B-v0.1'\n\n# Using Mistral AI to Summarize the Topics\nllm = HuggingFaceHub(repo_id='mistralai/Mistral-7B-v0.1', huggingfacehub_api_token=\"HF_TOKEN\")\n\n# Obtain clean topic names using Generative Model\nbunka.get_clean_topic_name(generative_model=llm)\nbunka.visualize_topics( width=800, height=800, colorscale = 'Portland')\n</code></pre> <p>Finally, let's visualize again the topics. We can chose from different colorscale.</p> <pre><code>bunka.visualize_topics(width=800, height=800)\n</code></pre> <p></p> <pre><code>&gt;&gt;&gt; bunka.df_topics_\n</code></pre> topic_id topic_name size percent bt-1 Cryptocurrency Impact 345 12.32 bt-3 Data Management Technologies 243 8.68 bt-14 Everyday Life 230 8.21 bt-0 Digital Learning Campaign 225 8.04 bt-12 Business Development 223 7.96 bt-2 Technology Devices 212 7.57 bt-10 Market Predictions Recap 201 7.18 bt-4 Comprehensive Learning Journey 187 6.68 bt-6 Future of Work 185 6.61 bt-11 Internet Discounts 175 6.25 bt-5 Technological Urban Water Management 172 6.14 bt-9 Electric Vehicle Technology 145 5.18 bt-8 Programming Concepts 116 4.14 bt-13 Quantum Technology Industries 105 3.75 bt-7 High Definition Television (HDTV) 36 1.29"},{"location":"fine-tuning.html#removing-data-based-on-topics-for-fine-tuning-purposes","title":"Removing Data based on topics for fine-tuning purposes","text":"<p>You have the flexibility to construct a customized dataset by excluding topics that do not align with your interests. For instance, in the provided example, we omitted topics associated with Advertising and High-Definition television, as these clusters primarily contain promotional content that we prefer not to include in our model's training data.</p> <pre><code>&gt;&gt;&gt; bunka.clean_data_by_topics()\n</code></pre> <p></p> <pre><code>&gt;&gt;&gt; bunka.df_cleaned_\n</code></pre> doc_id content topic_id topic_name 873ba315 Invisibilize Data With JavaScript bt-8 Programming Concepts 1243d58f Why End-to-End Testing is Important for Your Team bt-3 Data Management Technologies 45fb8166 This Tiny Wearable Device Uses Your Body Heat... bt-2 Technology Devices a122d1d2 Digital Policy Salon: The Next Frontier bt-0 Digital Learning Campaign 1bbcfc1c Preparing Hardware for Outdoor Creative Technology Installations bt-5 Technological Urban Water Management 79580c34 Angular Or React ? bt-8 Programming Concepts af0b08a2 Ed-Tech Startups Are Cashing in on Parents\u2019 Insecurities bt-0 Digital Learning Campaign 2255c350 Former Google CEO Wants to Create a Government-Funded University to Train A.I. Coders bt-6 Future of Work d2bc4b33 Applying Action &amp; The Importance of Ideas bt-12 Business Development 5219675e Why You Should (not?) Use Signal bt-2 Technology Devices ... ... ... ..."},{"location":"framing-analysis.html","title":"Framing Analysis","text":"<p>Data can be analyzed in countless ways, contingent on your objectives and interests. We've developed a tool that enables you to visualize data by semantically customizing your own axes.</p>"},{"location":"framing-analysis.html#discover-different-examples-using-our-google-colab-notebooks","title":"Discover different examples using our Google Colab Notebooks","text":"Theme Google Colab Link Understanding a dataset using Frame Analysis with Bunka"},{"location":"framing-analysis.html#installation-via-pip","title":"Installation via Pip","text":"<pre><code>pip install bunkatopics\n</code></pre>"},{"location":"framing-analysis.html#installation-via-git-clone","title":"Installation via Git Clone","text":"<pre><code>git clone https://github.com/charlesdedampierre/BunkaTopics.git\ncd BunkaTopics\npip install -e .\n</code></pre>"},{"location":"framing-analysis.html#quick-start","title":"Quick Start","text":""},{"location":"framing-analysis.html#uploading-sample-data","title":"Uploading Sample Data","text":"<p>To get started, let's upload a sample of Medium Articles into Bunkatopics:</p> <pre><code>from datasets import load_dataset\ndocs = load_dataset(\"bunkalab/medium-sample-technology\")[\"train\"][\"title\"] # 'docs' is a list of text [text1, text2, ..., textN]\n</code></pre>"},{"location":"framing-analysis.html#choose-your-embedding-model-and-fit-the-model","title":"Choose Your Embedding Model and fit the model","text":"<p>Bunkatopics offers seamless integration with Huggingface's extensive collection of embedding models. You can select from a wide range of models, but be mindful of their size. Please refer to the langchain documentation for details on available models.</p> <pre><code>from bunkatopics import Bunka\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# Choose your embedding model\nembedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\n# Initialize Bunka with your chosen model and language preference\nbunka = Bunka(embedding_model=embedding_model)\n\n# Fit Bunka to your text data\nbunka.fit(docs)\n</code></pre>"},{"location":"framing-analysis.html#bourdieu-map","title":"Bourdieu Map","text":"<p>The Bourdieu map provides a 2-Dimensional unsupervised scale to visualize various texts. Each region on the map represents a distinct topic, characterized by its most specific terms. Clusters are formed, and their names are succinctly summarized using Generative AI.</p> <p>The significance of this visualization lies in its ability to define axes, thereby creating continuums that reveal data distribution patterns. This concept draws inspiration from the work of the renowned French sociologist Bourdieu, who employed 2-Dimensional maps to project items and gain insights.</p> <pre><code>from langchain.llms import HuggingFaceHub\n\n# Define the HuggingFaceHub instance with the repository ID and API token\nllm = HuggingFaceHub(\n    repo_id='mistralai/Mistral-7B-v0.1',\n    huggingfacehub_api_token=\"HF_TOKEN\"\n)\n\n## Bourdieu Fig\nbourdieu_fig = bunka.visualize_bourdieu(\n        llm=llm, # Set to None if you don't need  GenAI summarization\n        x_left_words=[\"This is about business\"],\n        x_right_words=[\"This is about politics\"],\n        y_top_words=[\"this is about startups\"],\n        y_bottom_words=[\"This is about governments\"],\n        height=800,\n        width=800,\n        clustering=True,\n        topic_n_clusters=10,\n        density=False,\n        convex_hull=True,\n        radius_size=0.2,\n        min_docs_per_cluster = 5, \n        label_size_ratio_clusters=80)\n\n# Display the Bourdieu map\nbourdieu_fig.show()\n</code></pre> positive/negative vs humans/machines politics/business vs humans/machines politics/business vs     positive/negative politics/business vs startups/governments"},{"location":"getting-started.html","title":"Getting Started","text":""},{"location":"getting-started.html#installation-via-pip","title":"Installation via Pip","text":"<pre><code>pip install bunkatopics\n</code></pre>"},{"location":"getting-started.html#installation-via-git-clone","title":"Installation via Git Clone","text":"<pre><code>git clone https://github.com/charlesdedampierre/BunkaTopics.git\ncd BunkaTopics\npip install -e .\n</code></pre>"},{"location":"getting-started.html#quick-start","title":"Quick Start","text":""},{"location":"getting-started.html#uploading-sample-data","title":"Uploading Sample Data","text":"<p>To get started, let's upload a sample of Medium Articles into Bunkatopics:</p> <pre><code>from datasets import load_dataset\ndataset = load_dataset(\"bunkalab/medium-sample-technology\")[\"train\"] # 'docs' is a list of text [text1, text2, ..., \ndocs = list(dataset['title'])\n</code></pre>"},{"location":"getting-started.html#choose-your-embedding-model","title":"Choose Your Embedding Model","text":"<p>Bunkatopics offers seamless integration with Huggingface's extensive collection of embedding models. You can select from a wide range of models, but be mindful of their size. Please refer to the langchain documentation for details on available models.</p> <pre><code># Load Embedding model\nfrom sentence_transformers import SentenceTransformer\nembedding_model = SentenceTransformer(model_name_or_path=\"all-MiniLM-L6-v2\")\n\n# Load Projection Model\nimport umap\nprojection_model = umap.UMAP(\n                n_components=2,\n                random_state=42,\n            )\n\nfrom bunkatopics import Bunka\n\nbunka = Bunka(embedding_model=embedding_model, \n            projection_model=projection_model)\n\n# Fit Bunka to your text data\n bunka.fit(docs)\n</code></pre> <pre><code># Get a clustering model\nfrom sklearn.cluster import KMeans\nclustering_model = KMeans(n_clusters=15)\n&gt;&gt;&gt; bunka.get_topics(name_length=5, \n                    custom_clustering_model=clustering_model)# Specify the number of terms to describe each topic\n</code></pre> <p>Topics are described by the most specific terms belonging to the cluster.</p> topic_id topic_name size percent bt-12 technology - Tech - Children - student - days 322 10.73 bt-11 blockchain - Cryptocurrency - sense - Cryptocurrencies - Impact 283 9.43 bt-7 gadgets - phone - Device - specifications - screen 258 8.6 bt-8 software - Kubernetes - ETL - REST - Salesforce 258 8.6 bt-1 hackathon - review - Recap - Predictions - Lessons 257 8.57 bt-4 Reality - world - cities - future - Lot 246 8.2 bt-14 Product - Sales - day - dream - routine 241 8.03 bt-0 Words - Robots - discount - NordVPN - humans 208 6.93 bt-2 Internet - Overview - security - Work - Development 202 6.73 bt-13 Course - Difference - Step - science - Point 192 6.4 bt-6 quantum - Cars - Way - Game - quest 162 5.4 bt-3 Objects - Strings - app - Programming - Functions 119 3.97 bt-5 supply - chain - revolution - Risk - community 119 3.97 bt-9 COVID - printing - Car - work - app 89 2.97 bt-10 Episode - HD - Secrets - TV 44 1.47"},{"location":"getting-started.html#visualize-your-topics","title":"Visualize Your Topics","text":"<p>Finally, let's visualize the topics that Bunka has computed for your text data:</p> <pre><code>&gt;&gt;&gt; bunka.visualize_topics(width=800, height=800, colorscale='YIGnBu')\n</code></pre> <p></p>"},{"location":"getting-started.html#topic-modeling-with-genai-summarization-of-topics","title":"Topic Modeling with GenAI Summarization of Topics","text":"<p>Explore the power of Generative AI for summarizing topics! We use the 7B-instruct model of Mistral AI from the huggingface hub using the langchain framework.</p> <pre><code>from langchain.llms import HuggingFaceHub\n\n# Define the repository ID for Mistral-7B-v0.1\nrepo_id = 'mistralai/Mistral-7B-v0.1'\n\n# Using Mistral AI to Summarize the Topics\nllm = HuggingFaceHub(repo_id='mistralai/Mistral-7B-v0.1', huggingfacehub_api_token=\"HF_TOKEN\")\n\n# Obtain clean topic names using Generative Model\nbunka.get_clean_topic_name(generative_model=llm)\nbunka.visualize_topics( width=800, height=800, colorscale = 'Portland')\n</code></pre> <p>Finally, let's visualize again the topics. We can chose from different colorscales.</p> <pre><code>&gt;&gt;&gt; bunka.visualize_topics(width=800, height=800)\n</code></pre> YlGnBu Portland delta Blues <p>We can now access the newly made topics</p> <pre><code>&gt;&gt;&gt; bunka.df_topics_\n</code></pre> topic_id topic_name size percent bt-1 Cryptocurrency Impact 345 12.32 bt-3 Data Management Technologies 243 8.68 bt-14 Everyday Life 230 8.21 bt-0 Digital Learning Campaign 225 8.04 bt-12 Business Development 223 7.96 bt-2 Technology Devices 212 7.57 bt-10 Market Predictions Recap 201 7.18 bt-4 Comprehensive Learning Journey 187 6.68 bt-6 Future of Work 185 6.61 bt-11 Internet Discounts 175 6.25 bt-5 Technological Urban Water Management 172 6.14 bt-9 Electric Vehicle Technology 145 5.18 bt-8 Programming Concepts 116 4.14 bt-13 Quantum Technology Industries 105 3.75 bt-7 High Definition Television (HDTV) 36 1.29"},{"location":"getting-started.html#visualise-dimensions-on-topics","title":"visualise Dimensions on topics","text":"<pre><code>dataset = load_dataset(\"bunkalab/medium-sample-technology-tags\")['train']\ndocs = list(dataset['title'])\nids = list(dataset['doc_id'])\ntags = list(dataset['tags'])\n\nmetadata = {'tags':tags}\n\nfrom bunkatopics import Bunka\n\nbunka = Bunka()\n\n# Fit Bunka to your text data\nbunka.fit(docs=docs, ids=ids, metadata=metadata)\nbunka.get_topics(n_clusters=10)\nbunka.visualize_topics(color = 'tags', width=800, height=800) # Add the metadata names\n</code></pre>"},{"location":"getting-started.html#manually-cleaning-the-topics","title":"Manually Cleaning the topics","text":"<p>If you are not happy with the resulting topics, you can change them manually. Click on Apply changes when you are done. In the example, we changed the topic Cryptocurrency Impact to Cryptocurrency and Internet Discounts to Advertising.</p> <pre><code>&gt;&gt;&gt; bunka.manually_clean_topics()\n</code></pre> <p></p>"},{"location":"getting-started.html#removing-data-based-on-topics-for-fine-tuning-purposes","title":"Removing Data based on topics for fine-tuning purposes","text":"<p>You have the flexibility to construct a customized dataset by excluding topics that do not align with your interests. For instance, in the provided example, we omitted topics associated with Advertising and High-Definition television, as these clusters primarily contain promotional content that we prefer not to include in our model's training data.</p> <pre><code>&gt;&gt;&gt; bunka.clean_data_by_topics()\n</code></pre> <p></p> <pre><code>&gt;&gt;&gt; bunka.df_cleaned_\n</code></pre> doc_id content topic_id topic_name 873ba315 Invisibilize Data With JavaScript bt-8 Programming Concepts 1243d58f Why End-to-End Testing is Important for Your Team bt-3 Data Management Technologies 45fb8166 This Tiny Wearable Device Uses Your Body Heat... bt-2 Technology Devices a122d1d2 Digital Policy Salon: The Next Frontier bt-0 Digital Learning Campaign 1bbcfc1c Preparing Hardware for Outdoor Creative Technology Installations bt-5 Technological Urban Water Management 79580c34 Angular Or React ? bt-8 Programming Concepts af0b08a2 Ed-Tech Startups Are Cashing in on Parents\u2019 Insecurities bt-0 Digital Learning Campaign 2255c350 Former Google CEO Wants to Create a Government-Funded University to Train A.I. Coders bt-6 Future of Work d2bc4b33 Applying Action &amp; The Importance of Ideas bt-12 Business Development 5219675e Why You Should (not?) Use Signal bt-2 Technology Devices ... ... ... ..."},{"location":"getting-started.html#bourdieu-map","title":"Bourdieu Map","text":"<p>The Bourdieu map provides a 2-Dimensional unsupervised scale to visualize various texts. Each region on the map represents a distinct topic, characterized by its most specific terms. Clusters are formed, and their names are succinctly summarized using Generative AI.</p> <p>The significance of this visualization lies in its ability to define axes, thereby creating continuums that reveal data distribution patterns. This concept draws inspiration from the work of the renowned French sociologist Bourdieu, who employed 2-Dimensional maps to project items and gain insights.</p> <pre><code>from langchain.llms import HuggingFaceHub\n\n# Define the HuggingFaceHub instance with the repository ID and API token\nllm = HuggingFaceHub(\n    repo_id='mistralai/Mistral-7B-v0.1',\n    huggingfacehub_api_token=\"HF_TOKEN\"\n)\n\n## Bourdieu Fig\nbourdieu_fig = bunka.visualize_bourdieu(\n        llm=llm,\n        x_left_words=[\"This is about business\"],\n        x_right_words=[\"This is about politics\"],\n        y_top_words=[\"this is about startups\"],\n        y_bottom_words=[\"This is about governments\"],\n        height=800,\n        width=800,\n        clustering=True,\n        topic_n_clusters=10,\n        density=False,\n        convex_hull=True,\n        radius_size=0.2,\n        min_docs_per_cluster = 5, \n        label_size_ratio_clusters=80)\n</code></pre> <pre><code>&gt;&gt;&gt; bourdieu_fig.show()\n</code></pre> positive/negative vs humans/machines politics/business vs humans/machines politics/business vs     positive/negative politics/business vs startups/governments"},{"location":"getting-started.html#saving-and-loading-bunka","title":"Saving and loading Bunka","text":"<pre><code>bunka.save_bunka(\"bunka_dump\")\n...\n\nfrom bunkatopics import Bunka\nbunka = Bunka().load_bunka(\"bunka_dump\")\n\n\nfrom sklearn.cluster import KMeans\nclustering_model = KMeans(n_clusters=15)\n&gt;&gt;&gt; bunka.get_topics(name_length=5, \n                    custom_clustering_model=clustering_model)# Specify the number of terms to describe each topic\n</code></pre>"},{"location":"getting-started.html#loading-customed-embeddings","title":"Loading customed embeddings","text":"<pre><code>'''\nids = ['doc_1', 'doc_2'...., 'doc_n']\nembeddings = [[0.05121125280857086,\n  -0.03985324501991272,\n  -0.05017390474677086,\n  -0.03173152357339859,\n  -0.07367539405822754,\n  0.0331297293305397,\n  -0.00685789855197072...]]\n\n'''\n\npre_computed_embeddings = [{'doc_id': doc_id, 'embedding': embedding} for doc_id, embedding in zip(ids, embeddings)]\n...\n\nfrom bunkatopics import Bunka\nbunka = Bunka()\nbunka.fit(docs=docs, ids = ids, pre_computed_embeddings = pre_computed_embeddings)\n\n\nfrom sklearn.cluster import KMeans\nclustering_model = KMeans(n_clusters=15)\n&gt;&gt;&gt; bunka.get_topics(name_length=5, \n                    custom_clustering_model=clustering_model)# Specify the number of terms to describe each topic\n</code></pre>"},{"location":"getting-started.html#front-end","title":"Front-end","text":"<p>This is a beta feature. First, git clone the repository</p> <pre><code>git clone https://github.com/charlesdedampierre/BunkaTopics.git\ncd BunkaTopics\npip install -e .\n</code></pre> <p>Then carry out a Topic Modeling and launch the serveur:</p> <pre><code>from bunkatopics import Bunka\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# Choose your embedding model\nembedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # We recommend starting with a small model\n\n# Initialize Bunka with your chosen model\nbunka = Bunka(embedding_model=embedding_model) \n\n# Fit Bunka to your text data\nbunka.fit(docs)\nbunka.get_topics(n_clusters=15, name_length=3)# Specify the number of terms to describe each topic\n</code></pre> <pre><code>&gt;&gt;&gt; bunka.start_server() # A serveur will open on your computer at http://localhost:3000/ \n</code></pre>"},{"location":"topic_exploration.html","title":"Topic Exploration of a Textual Dataset","text":"<p>Bunkatopics is a package designed for Topic Exploration.</p>"},{"location":"topic_exploration.html#discover-different-examples-using-our-google-colab-notebooks","title":"Discover different examples using our Google Colab Notebooks","text":"Theme Google Colab Link Visual Topic Modeling with Bunka and datasets from HuggingFace"},{"location":"topic_exploration.html#installation-via-pip","title":"Installation via Pip","text":"<pre><code>pip install bunkatopics\n</code></pre>"},{"location":"topic_exploration.html#installation-via-git-clone","title":"Installation via Git Clone","text":"<pre><code>git clone https://github.com/charlesdedampierre/BunkaTopics.git\ncd BunkaTopics\npip install -e .\n</code></pre>"},{"location":"topic_exploration.html#quick-start","title":"Quick Start","text":""},{"location":"topic_exploration.html#uploading-sample-data","title":"Uploading Sample Data","text":"<p>To get started, let's upload a sample of Medium Articles into Bunkatopics:</p> <pre><code>from datasets import load_dataset\ndocs = load_dataset(\"bunkalab/medium-sample-technology\")[\"train\"][\"title\"] # 'docs' is a list of text [text1, text2, ..., textN]\n</code></pre>"},{"location":"topic_exploration.html#choose-your-embedding-model","title":"Choose Your Embedding Model","text":"<p>Bunkatopics offers seamless integration with Huggingface's extensive collection of embedding models. You can select from a wide range of models, but be mindful of their size. Please refer to the langchain documentation for details on available models.</p> <pre><code>from bunkatopics import Bunka\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# Choose your embedding model\nembedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")  # set to True if you have mutliprocessing\n\n# Initialize Bunka with your chosen model\nbunka = Bunka(embedding_model=embedding_model)\n\n# Fit Bunka to your text data\nbunka.fit(docs)\n</code></pre> <p>You can use other models like OpenAI thanks to langchain integration</p> <pre><code>from langchain_openai import OpenAIEmbeddings\nembedding_model = OpenAIEmbeddings(openai_api_key='OPEN_AI_KEY')\nbunka = Bunka(embedding_model=embedding_model)\n</code></pre> <pre><code># Get a list of topics\nbunka.get_topics(n_clusters=15, name_length=3)# Specify the number of terms to describe each topic\n</code></pre> <p>Topics are described by the most specific terms belonging to the cluster.</p> topic_id topic_name size percent bt-12 technology - Tech - Children - student - days 322 10.73 bt-11 blockchain - Cryptocurrency - sense - Cryptocurrencies - Impact 283 9.43 bt-7 gadgets - phone - Device - specifications - screen 258 8.6 bt-8 software - Kubernetes - ETL - REST - Salesforce 258 8.6 bt-1 hackathon - review - Recap - Predictions - Lessons 257 8.57 bt-4 Reality - world - cities - future - Lot 246 8.2 bt-14 Product - Sales - day - dream - routine 241 8.03 bt-0 Words - Robots - discount - NordVPN - humans 208 6.93 bt-2 Internet - Overview - security - Work - Development 202 6.73 bt-13 Course - Difference - Step - science - Point 192 6.4 bt-6 quantum - Cars - Way - Game - quest 162 5.4 bt-3 Objects - Strings - app - Programming - Functions 119 3.97 bt-5 supply - chain - revolution - Risk - community 119 3.97 bt-9 COVID - printing - Car - work - app 89 2.97 bt-10 Episode - HD - Secrets - TV 44 1.47"},{"location":"topic_exploration.html#visualize-your-topics","title":"Visualize Your Topics","text":"<p>Finally, let's visualize the topics that Bunka has computed for your text data:</p> <pre><code>bunka.visualize_topics(width=800, height=800, colorscale='YIGnBu')\n</code></pre> <p></p>"},{"location":"topic_exploration.html#topic-modeling-with-genai-summarization-of-topics","title":"Topic Modeling with GenAI Summarization of Topics","text":"<p>Explore the power of Generative AI for summarizing topics! We use the 7B-instruct model of Mistral AI from the huggingface hub using the langchain framework.</p> <pre><code>from langchain.llms import HuggingFaceHub\n\n# Define the repository ID for Mistral-7B-v0.1\nrepo_id = 'mistralai/Mistral-7B-v0.1'\n\n# Using Mistral AI to Summarize the Topics\nllm = HuggingFaceHub(repo_id='mistralai/Mistral-7B-v0.1', huggingfacehub_api_token=\"HF_TOKEN\")\n\n# Obtain clean topic names using Generative Model\nbunka.get_clean_topic_name(llm=llm)\nbunka.visualize_topics( width=800, height=800, colorscale = 'Portland')\n</code></pre> <p>You can also use a model from OpenAI thanks to the langchain integration</p> <pre><code>from langchain.llms import OpenAI\n\nllm = OpenAI(openai_api_key = 'OPEN_AI_KEY')\nbunka.get_clean_topic_name(llm=llm)\n</code></pre> <p>Finally, let's visualize again the topics. We can chose from different colorscale.</p> <pre><code>bunka.visualize_topics(width=800, height=800)\n</code></pre> <p></p> <p>We can now access the newly made topics</p> <pre><code>&gt;&gt;&gt; bunka.df_topics_\n</code></pre> topic_id topic_name size percent bt-1 Cryptocurrency Impact 345 12.32 bt-3 Data Management Technologies 243 8.68 bt-14 Everyday Life 230 8.21 bt-0 Digital Learning Campaign 225 8.04 bt-12 Business Development 223 7.96 bt-2 Technology Devices 212 7.57 bt-10 Market Predictions Recap 201 7.18 bt-4 Comprehensive Learning Journey 187 6.68 bt-6 Future of Work 185 6.61 bt-11 Internet Discounts 175 6.25 bt-5 Technological Urban Water Management 172 6.14 bt-9 Electric Vehicle Technology 145 5.18 bt-8 Programming Concepts 116 4.14 bt-13 Quantum Technology Industries 105 3.75 bt-7 High Definition Television (HDTV) 36 1.29"},{"location":"topic_exploration.html#manually-cleaning-the-topics","title":"Manually Cleaning the topics","text":"<p>Are you happy with the topics yes ? Let's change them manually. Click on Apply changes when you are done. In the example, we changed the topic Cryptocurrency Impact to Cryptocurrency and Internet Discounts to Advertising.</p> <p>The new topics will also appear on the Map.</p> <pre><code>bunka.manually_clean_topics()\n</code></pre> <p></p>"},{"location":"topic_exploration.html#exploring-topics-on-a-react-front-end","title":"Exploring topics on a REACT Front-end","text":"<p>Start the serveur to run the React Application</p> <pre><code>bunka.start_server() # A serveur will open on your computer at http://localhost:3000/ \n</code></pre>"},{"location":"topic_exploration.html#using-other-llm-for-summarizing-titles","title":"Using other LLM for Summarizing titles","text":"<pre><code>from langchain.llms import HuggingFacePipeline\nfrom langchain.prompts import PromptTemplate\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\n\ntext_generation_pipeline = transformers.pipeline(\n   model=model,\n   tokenizer=tokenizer,\n   task=\"text-generation\",\n   temperature=0.2,\n   repetition_penalty=1.1,\n   return_full_text=True,\n   max_new_tokens=300,\n)\n\nmistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n# Obtain clean topic names using Generative Model\nbunka.get_clean_topic_name(llm=mistral_llm)\nbunka.visualize_topics( width=800, height=800, colorscale = 'Portland')\n</code></pre>"},{"location":"bunka-api/bunkatopics.html","title":"Bunka","text":"<p>The Bunka class for managing and analyzing textual data using various NLP techniques.</p> <p>Examples: <pre><code>from bunkatopics import Bunka\nfrom datasets import load_dataset\nimport random\n\n# Extract Data\ndataset = load_dataset(\"rguo123/trump_tweets\")[\"train\"][\"content\"]\ndocs = random.sample(dataset, 1000)\n\nbunka = Bunka()\ntopics = bunka.fit_transform(docs)\nbunka.visualize_topics(width=800, height=800)\n</code></pre></p> Source code in <code>bunkatopics/_bunkatopics.py</code> <pre><code>class Bunka:\n    \"\"\"The Bunka class for managing and analyzing textual data using various NLP techniques.\n\n    Examples:\n    ```python\n    from bunkatopics import Bunka\n    from datasets import load_dataset\n    import random\n\n    # Extract Data\n    dataset = load_dataset(\"rguo123/trump_tweets\")[\"train\"][\"content\"]\n    docs = random.sample(dataset, 1000)\n\n    bunka = Bunka()\n    topics = bunka.fit_transform(docs)\n    bunka.visualize_topics(width=800, height=800)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_model: Embeddings = None,\n        projection_model=None,\n        language: str = \"english\",  # will be removed in the future\n    ):\n        \"\"\"Initialize a BunkaTopics instance.\n\n        Args:\n            embedding_model (Embeddings, optional): An optional embedding model for generating document embeddings.\n                If not provided, a default model will be used based on the specified language.\n                Default is None.\n            projection_model (optional): An optional projection model to reduce the dimensionality of the embeddings.\n                Default is None.\n        \"\"\"\n        warnings.filterwarnings(\"ignore\", category=LangChainDeprecationWarning)\n        if embedding_model is None:\n            embedding_model = SentenceTransformer(model_name_or_path=\"all-MiniLM-L6-v2\")\n\n        if projection_model is None:\n            projection_model = umap.UMAP(\n                n_components=2,\n                random_state=42,\n            )\n\n        self.projection_model = projection_model\n        self.embedding_model = embedding_model\n        self.df_cleaned = None\n\n    def fit(\n        self,\n        docs: t.List[str],\n        ids: t.List[DOC_ID] = None,\n        pre_computed_embeddings: t.Optional[\n            t.List[t.Dict[DOC_ID, t.List[float]]]\n        ] = None,\n        metadata: t.Optional[t.List[dict]] = None,\n        sampling_size_for_terms: t.Optional[int] = 1000,\n        language: bool = None,\n    ) -&gt; None:\n        \"\"\"\n        Fits the Bunka model to the provided list of documents.\n\n        This method processes the documents, extracts terms, generates embeddings, and\n        applies dimensionality reduction to prepare the data for topic modeling.\n\n        Args:\n            docs (t.List[str]): A list of document strings.\n            ids (t.Optional[t.List[DOC_ID]]): Optional. A list of identifiers for the documents. If not provided, UUIDs are generated.\n            metadata (t.Optional[t.List[str]): A of metadata dictionaries for the documents.\n            sampling_size_for_terms (t.Optional[int]): The number of documents to sample for term extraction. Default is 2000.\n        \"\"\"\n\n        df = pd.DataFrame(docs, columns=[\"content\"])\n\n        # Transform into a Document model\n        if ids is not None:\n            ids = [str(x) for x in ids]\n            df[\"doc_id\"] = ids\n            df = df.drop_duplicates(subset=\"doc_id\", keep=\"first\")\n\n        else:\n            df[\"doc_id\"] = [str(uuid.uuid4())[:20] for _ in range(len(df))]\n\n        if metadata is not None:\n            metadata_values = [\n                {key: metadata[key][i] for key in metadata} for i in range(len(df))\n            ]\n\n            df[\"metadata\"] = metadata_values\n\n        df = df[~df[\"content\"].isna()]\n        df = df.reset_index(drop=True)\n\n        self.docs = [Document(**row) for row in df.to_dict(orient=\"records\")]\n        sentences = [doc.content for doc in self.docs]\n\n        total_number_of_tokens = count_tokens(sentences)\n        logger.info(f\"Processing {total_number_of_tokens} tokens\")\n\n        ids = [doc.doc_id for doc in self.docs]\n\n        # Detect language\n\n        sample_size = len(sentences) // 100  # sample 1% of the dataset\n\n        # Randomly sample 1% of the dataset\n        sampled_sentences = random.sample(sentences, sample_size)\n\n        if language is None:\n            self.detected_language = detect_language(sampled_sentences)\n        else:\n            self.detected_language = language\n        self.language_name = detect_language_to_language_name.get(\n            self.detected_language, \"english\"\n        )\n\n        logger.info(f\"Detected language: {self.language_name}\")\n\n        # if self.language_name != \"english\":\n        #     embedding_model = SentenceTransformer(\n        #         model_name_or_path=\"paraphrase-multilingual-MiniLM-L12-v2\"\n        #     )\n\n        logger.info(\n            \"Embedding documents... (can take varying amounts of time depending on their size)\"\n        )\n\n        if pre_computed_embeddings is None:\n            # Determine if self.embedding_model is an instance of SentenceTransformer\n            if isinstance(self.embedding_model, SentenceTransformer):\n                bunka_embeddings = self.embedding_model.encode(\n                    sentences, show_progress_bar=True\n                )\n                bunka_embeddings = bunka_embeddings.tolist()\n\n            elif isinstance(self.embedding_model, HuggingFaceEmbeddings):\n                bunka_embeddings = self.embedding_model.embed_documents(sentences)\n\n            elif isinstance(self.embedding_model, FlagModel):\n                bunka_embeddings = self.embedding_model.encode(sentences)\n                bunka_embeddings = bunka_embeddings.tolist()\n\n            else:\n                bunka_embeddings = self.embedding_model.encode(\n                    sentences\n                )  # show_progress_bar=True\n        else:\n            pre_computed_embeddings.sort(key=lambda x: ids.index(x[\"doc_id\"]))\n            # bunka_embeddings = [x[\"embedding\"] for x in pre_computed_embeddings]\n            bunka_embeddings = []\n            for x in pre_computed_embeddings:\n                embedding = x[\"embedding\"]\n                if isinstance(embedding, list):\n                    bunka_embeddings.append(embedding)\n                else:\n                    bunka_embeddings.append(embedding.tolist())\n\n        # Add to the bunka objects\n        emb_doc_dict = {x: y for x, y in zip(ids, bunka_embeddings)}\n        for doc in self.docs:\n            doc.embedding = emb_doc_dict.get(doc.doc_id, [])\n\n        # Add to the bunka objects\n        emb_doc_dict = {x: y for x, y in zip(ids, bunka_embeddings)}\n        for doc in self.docs:\n            doc.embedding = emb_doc_dict.get(doc.doc_id, [])\n\n        # REDUCTION OF DIMENSIONS\n        logger.info(\"Reducing the dimensions of embeddings...\")\n\n        bunka_embeddings_2D = self.projection_model.fit_transform(\n            np.array(bunka_embeddings)\n        )\n\n        # Insert to the Pydantic object\n        df_embeddings_2D = pd.DataFrame(bunka_embeddings_2D, columns=[\"x\", \"y\"])\n\n        df_embeddings_2D[\"doc_id\"] = ids\n        df_embeddings_2D[\"bunka_docs\"] = sentences\n\n        xy_dict = df_embeddings_2D.set_index(\"doc_id\")[[\"x\", \"y\"]].to_dict(\"index\")\n\n        # Update the documents with the x and y values from the DataFrame\n        for doc in self.docs:\n            doc.x = xy_dict[doc.doc_id][\"x\"]\n            doc.y = xy_dict[doc.doc_id][\"y\"]\n\n        # CREATE A PLOT\n\n        self.fig_embeddings = self._quick_plot(df_embeddings_2D)\n\n        logger.info(\"Extracting meaningful terms from documents...\")\n        terms_extractor = TextacyTermsExtractor(language=self.detected_language)\n\n        if len(sentences) &gt;= sampling_size_for_terms:\n            # Pair sentences with their corresponding ids\n            paired_data = list(zip(sentences, ids))\n            random.seed(42)\n            sampled_data = random.sample(paired_data, sampling_size_for_terms)\n\n            # Unpack the sampled pairs back into sentences and ids lists\n            sampled_sentences, sampled_ids = zip(*sampled_data)\n            logger.info(\n                f\"Sampling {sampling_size_for_terms} documents for term extraction\"\n            )\n            self.terms, indexed_terms_dict = terms_extractor.fit_transform(\n                sampled_ids, sampled_sentences\n            )\n\n        else:\n            self.terms, indexed_terms_dict = terms_extractor.fit_transform(\n                ids, sentences\n            )\n\n        # add to the docs object\n        for doc in self.docs:\n            doc.term_id = indexed_terms_dict.get(doc.doc_id, [])\n\n        self.topics = None\n\n    def remove_outliers(self, threshold=6):\n        \"\"\"\n        Removes outliers from the dataset based on a specified threshold.\n\n        This method applies an outlier detection algorithm to identify and remove outliers\n        Args:\n            threshold (int): The threshold value for outlier detection. Default is 6.\n        \"\"\"\n\n        from bunkatopics.cleaning.outlier_detection import remove_outliers\n\n        cleaned_docs = remove_outliers(docs=self.docs, threshold=threshold)\n        # Calculate the number of removed documents\n\n        removed_docs_count = len(self.docs) - len(cleaned_docs)\n        logger.info(\"Number of removed documents: {}\".format(removed_docs_count))\n        self.docs = cleaned_docs\n\n    def save_bunka(self, path: str = \"bunka_dumps\"):\n        \"\"\"\n        Save the Bunka model to disk.\n\n        This method saves the Bunka model to disk by serializing its documents and terms.\n\n        Args:\n            path (str, optional): The directory path where the model will be saved.\n                Defaults to \"bunka_dumps\".\n\n        Examples:\n        ```python\n        from bunkatopics import Bunka\n        bunka = Bunka()\n        ...\n        bunka.save_bunka('bunka_dumps')```\n\n        \"\"\"\n        from .utils import save_bunka_models\n\n        save_bunka_models(path=path, bunka=self)\n\n    def load_bunka(self, path):\n        \"\"\"\n        Load the Bunka model from disk.\n\n        This method loads the Bunka model from disk by reading the serialized documents and terms.\n\n        Args:\n            path (str): The directory path from where the model will be loaded.\n\n        Returns:\n            bunka (Bunka): The loaded Bunka model.\n        \"\"\"\n        from .utils import read_documents_from_jsonl, read_terms_from_jsonl\n\n        documents = read_documents_from_jsonl(path + \"/bunka_docs.jsonl\")\n        terms = read_terms_from_jsonl(path + \"/bunka_terms.jsonl\")\n\n        self.docs = documents\n        self.terms = terms\n\n        return self\n\n    def get_topics(\n        self,\n        n_clusters: int = 5,\n        ngrams: t.List[int] = [1, 2],\n        name_length: int = 5,\n        top_terms_overall: int = 2000,\n        min_count_terms: int = 2,\n        ranking_terms: int = 20,\n        max_doc_per_topic: int = 20,\n        custom_clustering_model: bool = None,\n        min_docs_per_cluster: int = 10,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Computes and organizes topics from the documents using specified parameters.\n\n        This method uses a topic modeling process to identify and characterize topics within the data.\n\n        Args:\n            n_clusters (int): The number of clusters to form. Default is 5.\n            ngrams (t.List[int]): The n-gram range to consider for topic extraction. Default is [1, 2].\n            name_length (int): The length of the name for topics. Default is 10.\n            top_terms_overall (int): The number of top terms to consider overall. Default is 2000.\n            min_count_terms (int): The minimum count of terms to be considered. Default is 2.\n            min_docs_per_cluster (int, optional): Minimum count of documents per topic\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the topics and their associated data.\n\n        Note:\n            The method applies topic modeling using the specified parameters and updates the internal state\n            with the resulting topics. It also associates the identified topics with the documents.\n        \"\"\"\n\n        # Add the conditional check for min_count_terms and len(self.docs)\n        if min_count_terms &gt; 1 and len(self.docs) &lt;= 500:\n            logger.info(\n                f\"There is not enough data to select terms with a minimum occurrence of {min_count_terms}. Setting min_count_terms to 1\"\n            )\n            min_count_terms = 1\n\n        logger.info(\"Computing the topics\")\n\n        topic_model = BunkaTopicModeling(\n            n_clusters=n_clusters,\n            ngrams=ngrams,\n            name_length=name_length,\n            x_column=\"x\",\n            y_column=\"y\",\n            top_terms_overall=top_terms_overall,\n            min_count_terms=min_count_terms,\n            custom_clustering_model=custom_clustering_model,\n            min_docs_per_cluster=min_docs_per_cluster,\n        )\n\n        self.topics: t.List[Topic] = topic_model.fit_transform(\n            docs=self.docs,\n            terms=self.terms,\n        )\n\n        model_ranker = DocumentRanker(\n            ranking_terms=ranking_terms, max_doc_per_topic=max_doc_per_topic\n        )\n        self.docs, self.topics = model_ranker.fit_transform(self.docs, self.topics)\n\n        (\n            self.topics,\n            self.docs,\n        ) = _filter_hdbscan(self.topics, self.docs)\n\n        self.df_topics_, self.df_top_docs_per_topic_ = _create_topic_dfs(\n            self.topics, self.docs\n        )\n\n        return self.df_topics_\n\n    def get_clean_topic_name(\n        self,\n        llm: LLM,\n        use_doc: bool = False,\n        context: str = \"everything\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Enhances topic names using a language model for cleaner and more meaningful representations.\n\n        Args:\n            llm: The language model used for cleaning topic names.\n            use_doc (bool): Flag to determine whether to use document context in the cleaning process. Default is False.\n            context (str): The broader context within which the topics are related Default is \"everything\". For instance, if you are looking at Computer Science, then update context = 'Computer Science'\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the topics with cleaned names.\n\n        Note:\n            This method leverages a language model to refine the names of the topics generated by the model,\n            aiming for more understandable and relevant topic descriptors.\n        \"\"\"\n\n        logger.info(\"Using LLM to make topic names cleaner\")\n\n        model_cleaning = LLMCleaningTopic(\n            llm,\n            language=self.language_name,\n            use_doc=use_doc,\n            context=context,\n        )\n        self.topics: t.List[Topic] = model_cleaning.fit_transform(\n            self.topics,\n            self.docs,\n        )\n\n        self.df_topics_, self.df_top_docs_per_topic_ = _create_topic_dfs(\n            self.topics, self.docs\n        )\n\n        return self.df_topics_\n\n    def visualize_topics(\n        self,\n        show_text: bool = True,\n        label_size_ratio: int = 100,\n        point_size_ratio: int = 100,\n        width: int = 1000,\n        height: int = 1000,\n        colorscale: str = \"delta\",\n        density: bool = True,\n        convex_hull: bool = True,\n        color: str = None,\n        # search: str = None,\n    ) -&gt; go.Figure:\n        \"\"\"\n        Generates a visualization of the identified topics in the document set.\n\n        Args:\n            show_text (bool): Whether to display text labels on the visualization. Default is True.\n            label_size_ratio (int): The size ratio of the labels in the visualization. Default is 100.\n            width (int): The width of the visualization figure. Default is 1000.\n            height (int): The height of the visualization figure. Default is 1000.\n            colorscale (str): colorscale for the Density Plot (Default is delta)\n            density (bool): Whether to display a density map\n            convex_hull (bool): Whether to display lines around the clusters\n            color (str): What category to use to display the color\n\n        Returns:\n            go.Figure: A Plotly graph object figure representing the topic visualization.\n\n        Note:\n            This method creates a 'Bunka Map', a graphical representation of the topics,\n            using Plotly for interactive visualization. It displays how documents are grouped\n            into topics and can include text labels for clarity.\n        \"\"\"\n        logger.info(\"Creating the Bunka Map\")\n\n        model_visualizer = TopicVisualizer(\n            width=width,\n            height=height,\n            show_text=show_text,\n            label_size_ratio=label_size_ratio,\n            point_size_ratio=point_size_ratio,\n            colorscale=colorscale,\n            density=density,\n            convex_hull=convex_hull,\n        )\n        fig = model_visualizer.fit_transform(self.docs, self.topics, color=color)\n\n        return fig\n\n    def visualize_bourdieu(\n        self,\n        llm: t.Optional[LLM] = None,\n        x_left_words: t.List[str] = [\"war\"],\n        x_right_words: t.List[str] = [\"peace\"],\n        y_top_words: t.List[str] = [\"men\"],\n        y_bottom_words: t.List[str] = [\"women\"],\n        height: int = 1500,\n        width: int = 1500,\n        display_percent: bool = True,\n        clustering: bool = False,\n        topic_n_clusters: int = 10,\n        topic_terms: int = 2,\n        topic_ngrams: t.List[int] = [1, 2],\n        topic_top_terms_overall: int = 1000,\n        gen_topic_language: str = \"english\",\n        manual_axis_name: t.Optional[dict] = None,\n        use_doc_gen_topic: bool = False,\n        radius_size: float = 0.3,\n        convex_hull: bool = True,\n        density: bool = True,\n        colorscale: str = \"delta\",\n        label_size_ratio_clusters: int = 100,\n        label_size_ratio_label: int = 50,\n        label_size_ratio_percent: int = 10,\n        min_docs_per_cluster: int = 5,\n    ) -&gt; go.Figure:\n        \"\"\"\n        Creates and visualizes a Bourdieu Map using specified parameters and a generative model.\n\n        Args:\n            llm (t.Optional[str]): The generative model to be used. Default is None.\n            x_left_words (t.List[str]): Words defining the left and left x axes.\n            x_right_words (t.List[str]): Words defining the left and right x axes.\n            y_top_words (t.List[str]): Words defining the left and top y axes.\n            y_bottom_words (t.List[str]): Words defining the top and bottom y axes.\n            height (int): Dimensions of the visualization. Default to 1500.\n            width (int): Dimensions of the visualization. Default to 1500.\n            display_percent (bool): Flag to display percentages on the map. Default is True.\n            clustering (bool): Whether to apply clustering on the map. Default is False.\n            topic_n_clusters (int): Number of clusters for topic modeling. Default is 10.\n            topic_terms (int): Length of topic names. Default is 2.\n            topic_ngrams (t.List[int]): N-gram range for topic modeling. Default is [1, 2].\n            topic_top_terms_overall (int): Top terms to consider overall. Default is 1000.\n            gen_topic_language (str): Language for topic generation. Default is \"english\".\n            manual_axis_name (t.Optional[dict]): Custom axis names for the map. Default is None.\n            use_doc_gen_topic (bool): Flag to use document context in topic generation. Default is False.\n            radius_size (float): Radius size for the map isualization. Default is 0.3.\n            convex_hull (bool): Whether to include a convex hull in the visualization. Default is True.\n            colorscale (str): colorscale for the Density Plot (Default is delta)\n            density (bool): Whether to display a density map\n\n        Returns:\n            go.Figure: A Plotly graph object figure representing the Bourdieu Map.\n\n        Note:\n            The Bourdieu Map is a sophisticated visualization that plots documents and topics\n            based on specified word axes, using a generative model for dynamic analysis.\n            This method handles the complex process of generating and plotting this map,\n            offering a range of customization options for detailed analysis.\n        \"\"\"\n\n        logger.info(\"Creating the Bourdieu Map\")\n        topic_gen_param = TopicGenParam(\n            language=gen_topic_language,\n            top_doc=3,\n            top_terms=10,\n            use_doc=use_doc_gen_topic,\n            context=\"everything\",\n        )\n\n        topic_param = TopicParam(\n            n_clusters=topic_n_clusters,\n            ngrams=topic_ngrams,\n            name_lenght=topic_terms,\n            top_terms_overall=topic_top_terms_overall,\n        )\n\n        self.bourdieu_query = BourdieuQuery(\n            x_left_words=x_left_words,\n            x_right_words=x_right_words,\n            y_top_words=y_top_words,\n            y_bottom_words=y_bottom_words,\n            radius_size=radius_size,\n        )\n\n        # Request Bourdieu API\n\n        bourdieu_api = BourdieuAPI(\n            llm=llm,\n            embedding_model=self.embedding_model,\n            bourdieu_query=self.bourdieu_query,\n            topic_param=topic_param,\n            topic_gen_param=topic_gen_param,\n            min_docs_per_cluster=min_docs_per_cluster,\n        )\n\n        new_docs = copy.deepcopy(self.docs)\n        new_terms = copy.deepcopy(self.terms)\n\n        res = bourdieu_api.fit_transform(\n            docs=new_docs,\n            terms=new_terms,\n        )\n\n        self.bourdieu_docs = res[0]\n        self.bourdieu_topics = res[1]\n\n        visualizer = BourdieuVisualizer(\n            height=height,\n            width=width,\n            display_percent=display_percent,\n            convex_hull=convex_hull,\n            clustering=clustering,\n            manual_axis_name=manual_axis_name,\n            density=density,\n            colorscale=colorscale,\n            label_size_ratio_clusters=label_size_ratio_clusters,\n            label_size_ratio_label=label_size_ratio_label,\n            label_size_ratio_percent=label_size_ratio_percent,\n        )\n\n        fig = visualizer.fit_transform(self.bourdieu_docs, self.bourdieu_topics)\n\n        return fig\n\n    def visualize_bourdieu_one_dimension(\n        self,\n        left: t.List[str] = [\"negative\"],\n        right: t.List[str] = [\"positive\"],\n        width: int = 800,\n        height: int = 800,\n        explainer: bool = False,\n    ) -&gt; t.Tuple[go.Figure, t.Union[plt.Figure, None]]:\n        \"\"\"\n        Visualizes the document set on a one-dimensional Bourdieu axis.\n\n        Args:\n            left (t.List[str]): List of words representing the left side of the axis.\n            right (t.List[str]): List of words representing the right side of the axis.\n            width (int): Width of the generated visualization. Default is 800.\n            height (int): Height of the generated visualization. Default is 800.\n            explainer (bool): Flag to include an explainer figure. Default is False.\n\n        Returns:\n            t.Tuple[go.Figure, t.Union[plt.Figure, None]]: A tuple containing the main visualization figure\n            and an optional explainer figure (if explainer is True).\n\n        Note:\n            This method creates a one-dimensional Bourdieu-style visualization, plotting documents along an\n            axis defined by contrasting word sets. It helps in understanding the distribution of documents\n            in terms of these contrasting word concepts. An optional explainer figure can provide additional\n            insight into specific terms used in the visualization.\n        \"\"\"\n\n        model_bourdieu = BourdieuOneDimensionVisualizer(\n            embedding_model=self.embedding_model,\n            left=left,\n            right=right,\n            width=width,\n            height=height,\n            explainer=explainer,\n        )\n\n        fig = model_bourdieu.fit_transform(\n            docs=self.docs,\n        )\n\n        return fig\n\n    def visualize_query(\n        self,\n        query=\"What is America?\",\n        min_score: float = 0.2,\n        width: int = 600,\n        height: int = 300,\n    ):\n        # Create a visualization plot using plot_query function\n        fig, percent = plot_query(\n            embedding_model=self.embedding_model,\n            docs=self.docs,\n            query=query,\n            min_score=min_score,\n            width=width,\n            height=height,\n        )\n\n        # Return the visualization figure and percentage\n        return fig, percent\n\n    def visualize_dimensions(\n        self,\n        dimensions: t.List[str] = [\"positive\", \"negative\", \"fear\", \"love\"],\n        width=500,\n        height=500,\n        template=\"plotly_dark\",\n    ) -&gt; go.Figure:\n        \"\"\"\n        Visualizes the similarity scores between a given query and the document set.\n\n        Args:\n            width (int): Width of the visualization. Default is 600.\n            height (int): Height of the visualization. Default is 300.\n\n        Returns:\n            A tuple (fig, percent) where 'fig' is a Plotly graph object figure representing the\n            visualization and 'percent' is the percentage of documents above the similarity threshold.\n\n        Note:\n            This method creates a visualization showing how closely documents in the set relate to\n            the specified query. Documents with similarity scores above the threshold are highlighted,\n            providing a visual representation of their relevance to the query.\n        \"\"\"\n\n        final_df = []\n        logger.info(\"Computing Similarities\")\n        scaler = MinMaxScaler(feature_range=(0, 1))\n        for dim in tqdm(dimensions):\n            df_search = self.search(dim)\n            df_search = self.vectorstore.similarity_search_with_score(dim, k=3)\n            df_search[\"score\"] = scaler.fit_transform(\n                df_search[[\"cosine_similarity_score\"]]\n            )\n            df_search[\"source\"] = dim\n            final_df.append(df_search)\n        final_df = pd.concat([x for x in final_df])\n\n        final_df_mean = (\n            final_df.groupby(\"source\")[\"score\"]\n            .mean()\n            .rename(\"mean_score\")\n            .reset_index()\n        )\n        final_df_mean = final_df_mean.sort_values(\n            \"mean_score\", ascending=True\n        ).reset_index(drop=True)\n        final_df_mean[\"rank\"] = final_df_mean.index + 1\n\n        self.df_dimensions = final_df_mean\n\n        fig = px.line_polar(\n            final_df_mean,\n            r=\"mean_score\",\n            theta=\"source\",\n            line_close=True,\n            template=template,\n            width=width,\n            height=height,\n        )\n        return fig\n\n    def get_topic_repartition(self, width: int = 1200, height: int = 800) -&gt; go.Figure:\n        \"\"\"\n        Creates a bar plot to visualize the distribution of topics by size.\n\n        Args:\n            width (int): The width of the bar plot. Default is 1200.\n            height (int): The height of the bar plot. Default is 800.\n\n        Returns:\n            go.Figure: A Plotly graph object figure representing the topic distribution bar plot.\n\n        Note:\n            This method generates a visualization that illustrates the number of documents\n            associated with each topic, helping to understand the prevalence and distribution\n            of topics within the document set. It provides a clear and concise bar plot for\n            easy interpretation of the topic sizes.\n        \"\"\"\n\n        fig = get_topic_repartition(self.topics, width=width, height=height)\n        return fig\n\n    def clean_data_by_topics(self):\n        \"\"\"\n        Filters and cleans the dataset based on user-selected topics.\n\n        This method presents a UI with checkboxes for each topic in the dataset.\n        The user can select topics to keep, and the data will be filtered accordingly.\n        It merges the filtered documents and topics data, renames columns for clarity,\n        and calculates the percentage of data retained after cleaning.\n\n        Attributes Updated:\n            - self.df_cleaned: DataFrame containing the merged and cleaned documents and topics.\n\n        Logging:\n            - Logs the percentage of data retained after cleaning.\n\n        Side Effects:\n            - Updates `self.df_cleaned` with the cleaned data.\n            - Displays interactive widgets for user input.\n            - Logs information about the data cleaning process.\n\n        Note:\n            - This method uses interactive widgets (checkboxes and a button) for user input.\n            - The cleaning process is triggered by clicking the 'Clean Data' button.\n\n        \"\"\"\n\n        def on_button_clicked(b):\n            selected_topics = [\n                checkbox.description for checkbox in checkboxes if checkbox.value\n            ]\n            topic_filtered = [x for x in self.topics if x.name in selected_topics]\n            topic_id_filtered = [x.topic_id for x in topic_filtered]\n            docs_filtered = [x for x in self.docs if x.topic_id in topic_id_filtered]\n\n            df_docs_cleaned = pd.DataFrame([doc.model_dump() for doc in docs_filtered])\n            df_docs_cleaned = df_docs_cleaned[[\"doc_id\", \"content\", \"topic_id\"]]\n            df_topics = pd.DataFrame([topic.model_dump() for topic in topic_filtered])\n            df_topics = df_topics[[\"topic_id\", \"name\"]]\n            self.df_cleaned_ = pd.merge(df_docs_cleaned, df_topics, on=\"topic_id\")\n            self.df_cleaned_ = self.df_cleaned_.rename(columns={\"name\": \"topic_name\"})\n\n            len_kept = len(docs_filtered)\n            len_docs = len(self.docs)\n            percent_kept = round(len_kept / len_docs, 2) * 100\n            percent_kept = str(percent_kept) + \"%\"\n\n            logger.info(f\"After cleaning, you've kept {percent_kept} of your data\")\n\n        # Optionally, return or display df_cleaned\n        topic_names = [x.name for x in self.topics]\n        checkboxes = [\n            Checkbox(description=name, value=True, layout=Layout(width=\"auto\"))\n            for name in topic_names\n        ]\n\n        title_label = Label(\"Click on the topics you want to remove \ud83e\uddf9\u2728\ud83e\uddfc\ud83e\uddfd\")\n        checkbox_container = VBox(\n            [title_label] + checkboxes, layout=Layout(overflow=\"scroll hidden\")\n        )\n        button = Button(\n            description=\"Clean Data\",\n            style={\"button_color\": \"#2596be\", \"color\": \"#2596be\"},\n        )\n        button.on_click(on_button_clicked)\n        display(checkbox_container, button)\n\n    def manually_clean_topics(self):\n        \"\"\"\n        Allows manual renaming of topic names in the dataset.\n\n        This method facilitates the manual editing of topic names based on their IDs.\n        If no changes are made, it retains the original topic names.\n\n        The updated topic names are then applied to the `topics` attribute of the class instance.\n\n        Attributes Updated:\n            - self.topics: Each topic in this list gets its name updated based on the changes.\n\n        Logging:\n            - Logs the percentage of data retained after cleaning.\n\n        Side Effects:\n            - Modifies the `name` attribute of each topic in `self.topics` based on user input or defaults.\n            - Displays interactive widgets for user input.\n            - Logs information about the data cleaning process.\n\n        Note:\n            - This method uses interactive widgets (text fields and a button) for user input.\n            - The cleaning process is triggered by clicking the 'Apply Changes' button.\n\n        \"\"\"\n\n        def apply_changes(b):\n            for i, text_widget in enumerate(text_widgets):\n                new_name = text_widget.value.strip()\n                if new_name == \"\":\n                    new_names.append(original_topic_names[i])  # Keep the same name\n                else:\n                    new_names.append(new_name)\n\n            # Log changes applied\n            logger.info(\"Changes Applied!\")\n\n            # Update the topic names\n            topic_dict = dict(zip(original_topic_ids, new_names))\n            for topic in self.topics:\n                topic.name = topic_dict.get(topic.topic_id)\n\n            self.df_topics_, self.df_top_docs_per_topic_ = _create_topic_dfs(\n                self.topics, self.docs\n            )\n\n        original_topic_names = [x.name for x in self.topics]\n        original_topic_ids = [x.topic_id for x in self.topics]\n        new_names = []\n\n        # Create a list of Text widgets for entering new names with IDs as descriptions\n        text_widgets = []\n\n        for i, (topic, topic_id) in enumerate(\n            zip(original_topic_names, original_topic_ids)\n        ):\n            text_widget = widgets.Text(value=topic, description=f\"{topic_id}:\")\n            text_widgets.append(text_widget)\n\n        # Create a title widget\n        title_widget = widgets.HTML(\"Manually input the new topic names: \")\n\n        # Combine the title, Text widgets, and a button in a VBox\n        container = widgets.VBox([title_widget] + text_widgets)\n\n        # Create a button to apply changes with text color #2596be and bold description\n        apply_button = widgets.Button(\n            description=\"Apply Changes\",\n            style={\"button_color\": \"#2596be\", \"color\": \"#2596be\"},\n        )\n        apply_button.on_click(apply_changes)\n\n        # Display the container and apply button\n        display(container, apply_button)\n\n    def start_server(self):\n        subprocess.run([\"cp\", \"web/env.model\", \"web/.env\"], check=True)\n        if is_server_running():\n            logger.info(\"Server on port 3000 is already running. Killing it...\")\n            kill_server()\n        if not self.topics:\n            raise BunkaError(\"No topics available. Run bunka.get_topics() first.\")\n        else:\n            file_path = \"web/public\" + \"/bunka_docs.json\"\n\n            for x in self.docs:\n                x.embedding = None\n            docs_json = [x.model_dump() for x in self.docs]\n            with open(file_path, \"w\") as json_file:\n                json.dump(docs_json, json_file)\n\n            file_path = \"web/public\" + \"/bunka_topics.json\"\n\n            topics_json = [x.model_dump() for x in self.topics]\n            with open(file_path, \"w\") as json_file:\n                json.dump(topics_json, json_file)\n\n        \"\"\"try:\n            file_path = \"web/public\" + \"/bunka_bourdieu_docs.json\"\n            docs_json = [x.model_dump() for x in self.bourdieu_docs]\n\n            with open(file_path, \"w\") as json_file:\n                json.dump(docs_json, json_file)\n\n            file_path = \"web/public\" + \"/bunka_bourdieu_topics.json\"\n            topics_json = [x.model_dump() for x in self.bourdieu_topics]\n            with open(file_path, \"w\") as json_file:\n                json.dump(topics_json, json_file)\n\n            file_path = \"web/public\" + \"/bunka_bourdieu_query.json\"\n            with open(file_path, \"w\") as json_file:\n                json.dump(self.bourdieu_query.model_dump(), json_file)\n        except:\n            logger.info(\"run bunka.visualize_bourdieu() first\")\"\"\"\n\n        subprocess.Popen([\"npm\", \"start\"], cwd=\"web\")\n        logger.info(\"NPM server started.\")\n\n    def _quick_plot(self, df_embeddings_2D):\n        # Create a scatter plot\n        fig_quick_embedding = px.scatter(\n            df_embeddings_2D, x=\"x\", y=\"y\", hover_data=[\"bunka_docs\"]\n        )\n\n        # Update layout for better readability\n        fig_quick_embedding.update_layout(\n            title=\"Raw Scatter Plot of Bunka Embeddings\",\n            xaxis_title=\"X Embedding\",\n            yaxis_title=\"Y Embedding\",\n            hovermode=\"closest\",\n        )\n        # Show the plot\n\n        return fig_quick_embedding\n</code></pre>"},{"location":"bunka-api/bunkatopics.html#bunkatopics._bunkatopics.Bunka.__init__","title":"<code>__init__(embedding_model=None, projection_model=None, language='english')</code>","text":"<p>Initialize a BunkaTopics instance.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_model</code> <code>Embeddings</code> <p>An optional embedding model for generating document embeddings. If not provided, a default model will be used based on the specified language. Default is None.</p> <code>None</code> <code>projection_model</code> <code>optional</code> <p>An optional projection model to reduce the dimensionality of the embeddings. Default is None.</p> <code>None</code> Source code in <code>bunkatopics/_bunkatopics.py</code> <pre><code>def __init__(\n    self,\n    embedding_model: Embeddings = None,\n    projection_model=None,\n    language: str = \"english\",  # will be removed in the future\n):\n    \"\"\"Initialize a BunkaTopics instance.\n\n    Args:\n        embedding_model (Embeddings, optional): An optional embedding model for generating document embeddings.\n            If not provided, a default model will be used based on the specified language.\n            Default is None.\n        projection_model (optional): An optional projection model to reduce the dimensionality of the embeddings.\n            Default is None.\n    \"\"\"\n    warnings.filterwarnings(\"ignore\", category=LangChainDeprecationWarning)\n    if embedding_model is None:\n        embedding_model = SentenceTransformer(model_name_or_path=\"all-MiniLM-L6-v2\")\n\n    if projection_model is None:\n        projection_model = umap.UMAP(\n            n_components=2,\n            random_state=42,\n        )\n\n    self.projection_model = projection_model\n    self.embedding_model = embedding_model\n    self.df_cleaned = None\n</code></pre>"},{"location":"bunka-api/bunkatopics.html#bunkatopics._bunkatopics.Bunka.clean_data_by_topics","title":"<code>clean_data_by_topics()</code>","text":"<p>Filters and cleans the dataset based on user-selected topics.</p> <p>This method presents a UI with checkboxes for each topic in the dataset. The user can select topics to keep, and the data will be filtered accordingly. It merges the filtered documents and topics data, renames columns for clarity, and calculates the percentage of data retained after cleaning.</p> Attributes Updated <ul> <li>self.df_cleaned: DataFrame containing the merged and cleaned documents and topics.</li> </ul> Logging <ul> <li>Logs the percentage of data retained after cleaning.</li> </ul> Side Effects <ul> <li>Updates <code>self.df_cleaned</code> with the cleaned data.</li> <li>Displays interactive widgets for user input.</li> <li>Logs information about the data cleaning process.</li> </ul> Note <ul> <li>This method uses interactive widgets (checkboxes and a button) for user input.</li> <li>The cleaning process is triggered by clicking the 'Clean Data' button.</li> </ul> Source code in <code>bunkatopics/_bunkatopics.py</code> <pre><code>def clean_data_by_topics(self):\n    \"\"\"\n    Filters and cleans the dataset based on user-selected topics.\n\n    This method presents a UI with checkboxes for each topic in the dataset.\n    The user can select topics to keep, and the data will be filtered accordingly.\n    It merges the filtered documents and topics data, renames columns for clarity,\n    and calculates the percentage of data retained after cleaning.\n\n    Attributes Updated:\n        - self.df_cleaned: DataFrame containing the merged and cleaned documents and topics.\n\n    Logging:\n        - Logs the percentage of data retained after cleaning.\n\n    Side Effects:\n        - Updates `self.df_cleaned` with the cleaned data.\n        - Displays interactive widgets for user input.\n        - Logs information about the data cleaning process.\n\n    Note:\n        - This method uses interactive widgets (checkboxes and a button) for user input.\n        - The cleaning process is triggered by clicking the 'Clean Data' button.\n\n    \"\"\"\n\n    def on_button_clicked(b):\n        selected_topics = [\n            checkbox.description for checkbox in checkboxes if checkbox.value\n        ]\n        topic_filtered = [x for x in self.topics if x.name in selected_topics]\n        topic_id_filtered = [x.topic_id for x in topic_filtered]\n        docs_filtered = [x for x in self.docs if x.topic_id in topic_id_filtered]\n\n        df_docs_cleaned = pd.DataFrame([doc.model_dump() for doc in docs_filtered])\n        df_docs_cleaned = df_docs_cleaned[[\"doc_id\", \"content\", \"topic_id\"]]\n        df_topics = pd.DataFrame([topic.model_dump() for topic in topic_filtered])\n        df_topics = df_topics[[\"topic_id\", \"name\"]]\n        self.df_cleaned_ = pd.merge(df_docs_cleaned, df_topics, on=\"topic_id\")\n        self.df_cleaned_ = self.df_cleaned_.rename(columns={\"name\": \"topic_name\"})\n\n        len_kept = len(docs_filtered)\n        len_docs = len(self.docs)\n        percent_kept = round(len_kept / len_docs, 2) * 100\n        percent_kept = str(percent_kept) + \"%\"\n\n        logger.info(f\"After cleaning, you've kept {percent_kept} of your data\")\n\n    # Optionally, return or display df_cleaned\n    topic_names = [x.name for x in self.topics]\n    checkboxes = [\n        Checkbox(description=name, value=True, layout=Layout(width=\"auto\"))\n        for name in topic_names\n    ]\n\n    title_label = Label(\"Click on the topics you want to remove \ud83e\uddf9\u2728\ud83e\uddfc\ud83e\uddfd\")\n    checkbox_container = VBox(\n        [title_label] + checkboxes, layout=Layout(overflow=\"scroll hidden\")\n    )\n    button = Button(\n        description=\"Clean Data\",\n        style={\"button_color\": \"#2596be\", \"color\": \"#2596be\"},\n    )\n    button.on_click(on_button_clicked)\n    display(checkbox_container, button)\n</code></pre>"},{"location":"bunka-api/bunkatopics.html#bunkatopics._bunkatopics.Bunka.fit","title":"<code>fit(docs, ids=None, pre_computed_embeddings=None, metadata=None, sampling_size_for_terms=1000, language=None)</code>","text":"<p>Fits the Bunka model to the provided list of documents.</p> <p>This method processes the documents, extracts terms, generates embeddings, and applies dimensionality reduction to prepare the data for topic modeling.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[str]</code> <p>A list of document strings.</p> required <code>ids</code> <code>Optional[List[DOC_ID]]</code> <p>Optional. A list of identifiers for the documents. If not provided, UUIDs are generated.</p> <code>None</code> <code>metadata</code> <code>t.Optional[t.List[str]</code> <p>A of metadata dictionaries for the documents.</p> <code>None</code> <code>sampling_size_for_terms</code> <code>Optional[int]</code> <p>The number of documents to sample for term extraction. Default is 2000.</p> <code>1000</code> Source code in <code>bunkatopics/_bunkatopics.py</code> <pre><code>def fit(\n    self,\n    docs: t.List[str],\n    ids: t.List[DOC_ID] = None,\n    pre_computed_embeddings: t.Optional[\n        t.List[t.Dict[DOC_ID, t.List[float]]]\n    ] = None,\n    metadata: t.Optional[t.List[dict]] = None,\n    sampling_size_for_terms: t.Optional[int] = 1000,\n    language: bool = None,\n) -&gt; None:\n    \"\"\"\n    Fits the Bunka model to the provided list of documents.\n\n    This method processes the documents, extracts terms, generates embeddings, and\n    applies dimensionality reduction to prepare the data for topic modeling.\n\n    Args:\n        docs (t.List[str]): A list of document strings.\n        ids (t.Optional[t.List[DOC_ID]]): Optional. A list of identifiers for the documents. If not provided, UUIDs are generated.\n        metadata (t.Optional[t.List[str]): A of metadata dictionaries for the documents.\n        sampling_size_for_terms (t.Optional[int]): The number of documents to sample for term extraction. Default is 2000.\n    \"\"\"\n\n    df = pd.DataFrame(docs, columns=[\"content\"])\n\n    # Transform into a Document model\n    if ids is not None:\n        ids = [str(x) for x in ids]\n        df[\"doc_id\"] = ids\n        df = df.drop_duplicates(subset=\"doc_id\", keep=\"first\")\n\n    else:\n        df[\"doc_id\"] = [str(uuid.uuid4())[:20] for _ in range(len(df))]\n\n    if metadata is not None:\n        metadata_values = [\n            {key: metadata[key][i] for key in metadata} for i in range(len(df))\n        ]\n\n        df[\"metadata\"] = metadata_values\n\n    df = df[~df[\"content\"].isna()]\n    df = df.reset_index(drop=True)\n\n    self.docs = [Document(**row) for row in df.to_dict(orient=\"records\")]\n    sentences = [doc.content for doc in self.docs]\n\n    total_number_of_tokens = count_tokens(sentences)\n    logger.info(f\"Processing {total_number_of_tokens} tokens\")\n\n    ids = [doc.doc_id for doc in self.docs]\n\n    # Detect language\n\n    sample_size = len(sentences) // 100  # sample 1% of the dataset\n\n    # Randomly sample 1% of the dataset\n    sampled_sentences = random.sample(sentences, sample_size)\n\n    if language is None:\n        self.detected_language = detect_language(sampled_sentences)\n    else:\n        self.detected_language = language\n    self.language_name = detect_language_to_language_name.get(\n        self.detected_language, \"english\"\n    )\n\n    logger.info(f\"Detected language: {self.language_name}\")\n\n    # if self.language_name != \"english\":\n    #     embedding_model = SentenceTransformer(\n    #         model_name_or_path=\"paraphrase-multilingual-MiniLM-L12-v2\"\n    #     )\n\n    logger.info(\n        \"Embedding documents... (can take varying amounts of time depending on their size)\"\n    )\n\n    if pre_computed_embeddings is None:\n        # Determine if self.embedding_model is an instance of SentenceTransformer\n        if isinstance(self.embedding_model, SentenceTransformer):\n            bunka_embeddings = self.embedding_model.encode(\n                sentences, show_progress_bar=True\n            )\n            bunka_embeddings = bunka_embeddings.tolist()\n\n        elif isinstance(self.embedding_model, HuggingFaceEmbeddings):\n            bunka_embeddings = self.embedding_model.embed_documents(sentences)\n\n        elif isinstance(self.embedding_model, FlagModel):\n            bunka_embeddings = self.embedding_model.encode(sentences)\n            bunka_embeddings = bunka_embeddings.tolist()\n\n        else:\n            bunka_embeddings = self.embedding_model.encode(\n                sentences\n            )  # show_progress_bar=True\n    else:\n        pre_computed_embeddings.sort(key=lambda x: ids.index(x[\"doc_id\"]))\n        # bunka_embeddings = [x[\"embedding\"] for x in pre_computed_embeddings]\n        bunka_embeddings = []\n        for x in pre_computed_embeddings:\n            embedding = x[\"embedding\"]\n            if isinstance(embedding, list):\n                bunka_embeddings.append(embedding)\n            else:\n                bunka_embeddings.append(embedding.tolist())\n\n    # Add to the bunka objects\n    emb_doc_dict = {x: y for x, y in zip(ids, bunka_embeddings)}\n    for doc in self.docs:\n        doc.embedding = emb_doc_dict.get(doc.doc_id, [])\n\n    # Add to the bunka objects\n    emb_doc_dict = {x: y for x, y in zip(ids, bunka_embeddings)}\n    for doc in self.docs:\n        doc.embedding = emb_doc_dict.get(doc.doc_id, [])\n\n    # REDUCTION OF DIMENSIONS\n    logger.info(\"Reducing the dimensions of embeddings...\")\n\n    bunka_embeddings_2D = self.projection_model.fit_transform(\n        np.array(bunka_embeddings)\n    )\n\n    # Insert to the Pydantic object\n    df_embeddings_2D = pd.DataFrame(bunka_embeddings_2D, columns=[\"x\", \"y\"])\n\n    df_embeddings_2D[\"doc_id\"] = ids\n    df_embeddings_2D[\"bunka_docs\"] = sentences\n\n    xy_dict = df_embeddings_2D.set_index(\"doc_id\")[[\"x\", \"y\"]].to_dict(\"index\")\n\n    # Update the documents with the x and y values from the DataFrame\n    for doc in self.docs:\n        doc.x = xy_dict[doc.doc_id][\"x\"]\n        doc.y = xy_dict[doc.doc_id][\"y\"]\n\n    # CREATE A PLOT\n\n    self.fig_embeddings = self._quick_plot(df_embeddings_2D)\n\n    logger.info(\"Extracting meaningful terms from documents...\")\n    terms_extractor = TextacyTermsExtractor(language=self.detected_language)\n\n    if len(sentences) &gt;= sampling_size_for_terms:\n        # Pair sentences with their corresponding ids\n        paired_data = list(zip(sentences, ids))\n        random.seed(42)\n        sampled_data = random.sample(paired_data, sampling_size_for_terms)\n\n        # Unpack the sampled pairs back into sentences and ids lists\n        sampled_sentences, sampled_ids = zip(*sampled_data)\n        logger.info(\n            f\"Sampling {sampling_size_for_terms} documents for term extraction\"\n        )\n        self.terms, indexed_terms_dict = terms_extractor.fit_transform(\n            sampled_ids, sampled_sentences\n        )\n\n    else:\n        self.terms, indexed_terms_dict = terms_extractor.fit_transform(\n            ids, sentences\n        )\n\n    # add to the docs object\n    for doc in self.docs:\n        doc.term_id = indexed_terms_dict.get(doc.doc_id, [])\n\n    self.topics = None\n</code></pre>"},{"location":"bunka-api/bunkatopics.html#bunkatopics._bunkatopics.Bunka.get_clean_topic_name","title":"<code>get_clean_topic_name(llm, use_doc=False, context='everything')</code>","text":"<p>Enhances topic names using a language model for cleaner and more meaningful representations.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>LLM</code> <p>The language model used for cleaning topic names.</p> required <code>use_doc</code> <code>bool</code> <p>Flag to determine whether to use document context in the cleaning process. Default is False.</p> <code>False</code> <code>context</code> <code>str</code> <p>The broader context within which the topics are related Default is \"everything\". For instance, if you are looking at Computer Science, then update context = 'Computer Science'</p> <code>'everything'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the topics with cleaned names.</p> Note <p>This method leverages a language model to refine the names of the topics generated by the model, aiming for more understandable and relevant topic descriptors.</p> Source code in <code>bunkatopics/_bunkatopics.py</code> <pre><code>def get_clean_topic_name(\n    self,\n    llm: LLM,\n    use_doc: bool = False,\n    context: str = \"everything\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Enhances topic names using a language model for cleaner and more meaningful representations.\n\n    Args:\n        llm: The language model used for cleaning topic names.\n        use_doc (bool): Flag to determine whether to use document context in the cleaning process. Default is False.\n        context (str): The broader context within which the topics are related Default is \"everything\". For instance, if you are looking at Computer Science, then update context = 'Computer Science'\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the topics with cleaned names.\n\n    Note:\n        This method leverages a language model to refine the names of the topics generated by the model,\n        aiming for more understandable and relevant topic descriptors.\n    \"\"\"\n\n    logger.info(\"Using LLM to make topic names cleaner\")\n\n    model_cleaning = LLMCleaningTopic(\n        llm,\n        language=self.language_name,\n        use_doc=use_doc,\n        context=context,\n    )\n    self.topics: t.List[Topic] = model_cleaning.fit_transform(\n        self.topics,\n        self.docs,\n    )\n\n    self.df_topics_, self.df_top_docs_per_topic_ = _create_topic_dfs(\n        self.topics, self.docs\n    )\n\n    return self.df_topics_\n</code></pre>"},{"location":"bunka-api/bunkatopics.html#bunkatopics._bunkatopics.Bunka.get_topic_repartition","title":"<code>get_topic_repartition(width=1200, height=800)</code>","text":"<p>Creates a bar plot to visualize the distribution of topics by size.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>int</code> <p>The width of the bar plot. Default is 1200.</p> <code>1200</code> <code>height</code> <code>int</code> <p>The height of the bar plot. Default is 800.</p> <code>800</code> <p>Returns:</p> Type Description <code>Figure</code> <p>go.Figure: A Plotly graph object figure representing the topic distribution bar plot.</p> Note <p>This method generates a visualization that illustrates the number of documents associated with each topic, helping to understand the prevalence and distribution of topics within the document set. It provides a clear and concise bar plot for easy interpretation of the topic sizes.</p> Source code in <code>bunkatopics/_bunkatopics.py</code> <pre><code>def get_topic_repartition(self, width: int = 1200, height: int = 800) -&gt; go.Figure:\n    \"\"\"\n    Creates a bar plot to visualize the distribution of topics by size.\n\n    Args:\n        width (int): The width of the bar plot. Default is 1200.\n        height (int): The height of the bar plot. Default is 800.\n\n    Returns:\n        go.Figure: A Plotly graph object figure representing the topic distribution bar plot.\n\n    Note:\n        This method generates a visualization that illustrates the number of documents\n        associated with each topic, helping to understand the prevalence and distribution\n        of topics within the document set. It provides a clear and concise bar plot for\n        easy interpretation of the topic sizes.\n    \"\"\"\n\n    fig = get_topic_repartition(self.topics, width=width, height=height)\n    return fig\n</code></pre>"},{"location":"bunka-api/bunkatopics.html#bunkatopics._bunkatopics.Bunka.get_topics","title":"<code>get_topics(n_clusters=5, ngrams=[1, 2], name_length=5, top_terms_overall=2000, min_count_terms=2, ranking_terms=20, max_doc_per_topic=20, custom_clustering_model=None, min_docs_per_cluster=10)</code>","text":"<p>Computes and organizes topics from the documents using specified parameters.</p> <p>This method uses a topic modeling process to identify and characterize topics within the data.</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>int</code> <p>The number of clusters to form. Default is 5.</p> <code>5</code> <code>ngrams</code> <code>List[int]</code> <p>The n-gram range to consider for topic extraction. Default is [1, 2].</p> <code>[1, 2]</code> <code>name_length</code> <code>int</code> <p>The length of the name for topics. Default is 10.</p> <code>5</code> <code>top_terms_overall</code> <code>int</code> <p>The number of top terms to consider overall. Default is 2000.</p> <code>2000</code> <code>min_count_terms</code> <code>int</code> <p>The minimum count of terms to be considered. Default is 2.</p> <code>2</code> <code>min_docs_per_cluster</code> <code>int</code> <p>Minimum count of documents per topic</p> <code>10</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the topics and their associated data.</p> Note <p>The method applies topic modeling using the specified parameters and updates the internal state with the resulting topics. It also associates the identified topics with the documents.</p> Source code in <code>bunkatopics/_bunkatopics.py</code> <pre><code>def get_topics(\n    self,\n    n_clusters: int = 5,\n    ngrams: t.List[int] = [1, 2],\n    name_length: int = 5,\n    top_terms_overall: int = 2000,\n    min_count_terms: int = 2,\n    ranking_terms: int = 20,\n    max_doc_per_topic: int = 20,\n    custom_clustering_model: bool = None,\n    min_docs_per_cluster: int = 10,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Computes and organizes topics from the documents using specified parameters.\n\n    This method uses a topic modeling process to identify and characterize topics within the data.\n\n    Args:\n        n_clusters (int): The number of clusters to form. Default is 5.\n        ngrams (t.List[int]): The n-gram range to consider for topic extraction. Default is [1, 2].\n        name_length (int): The length of the name for topics. Default is 10.\n        top_terms_overall (int): The number of top terms to consider overall. Default is 2000.\n        min_count_terms (int): The minimum count of terms to be considered. Default is 2.\n        min_docs_per_cluster (int, optional): Minimum count of documents per topic\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the topics and their associated data.\n\n    Note:\n        The method applies topic modeling using the specified parameters and updates the internal state\n        with the resulting topics. It also associates the identified topics with the documents.\n    \"\"\"\n\n    # Add the conditional check for min_count_terms and len(self.docs)\n    if min_count_terms &gt; 1 and len(self.docs) &lt;= 500:\n        logger.info(\n            f\"There is not enough data to select terms with a minimum occurrence of {min_count_terms}. Setting min_count_terms to 1\"\n        )\n        min_count_terms = 1\n\n    logger.info(\"Computing the topics\")\n\n    topic_model = BunkaTopicModeling(\n        n_clusters=n_clusters,\n        ngrams=ngrams,\n        name_length=name_length,\n        x_column=\"x\",\n        y_column=\"y\",\n        top_terms_overall=top_terms_overall,\n        min_count_terms=min_count_terms,\n        custom_clustering_model=custom_clustering_model,\n        min_docs_per_cluster=min_docs_per_cluster,\n    )\n\n    self.topics: t.List[Topic] = topic_model.fit_transform(\n        docs=self.docs,\n        terms=self.terms,\n    )\n\n    model_ranker = DocumentRanker(\n        ranking_terms=ranking_terms, max_doc_per_topic=max_doc_per_topic\n    )\n    self.docs, self.topics = model_ranker.fit_transform(self.docs, self.topics)\n\n    (\n        self.topics,\n        self.docs,\n    ) = _filter_hdbscan(self.topics, self.docs)\n\n    self.df_topics_, self.df_top_docs_per_topic_ = _create_topic_dfs(\n        self.topics, self.docs\n    )\n\n    return self.df_topics_\n</code></pre>"},{"location":"bunka-api/bunkatopics.html#bunkatopics._bunkatopics.Bunka.load_bunka","title":"<code>load_bunka(path)</code>","text":"<p>Load the Bunka model from disk.</p> <p>This method loads the Bunka model from disk by reading the serialized documents and terms.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The directory path from where the model will be loaded.</p> required <p>Returns:</p> Name Type Description <code>bunka</code> <code>Bunka</code> <p>The loaded Bunka model.</p> Source code in <code>bunkatopics/_bunkatopics.py</code> <pre><code>def load_bunka(self, path):\n    \"\"\"\n    Load the Bunka model from disk.\n\n    This method loads the Bunka model from disk by reading the serialized documents and terms.\n\n    Args:\n        path (str): The directory path from where the model will be loaded.\n\n    Returns:\n        bunka (Bunka): The loaded Bunka model.\n    \"\"\"\n    from .utils import read_documents_from_jsonl, read_terms_from_jsonl\n\n    documents = read_documents_from_jsonl(path + \"/bunka_docs.jsonl\")\n    terms = read_terms_from_jsonl(path + \"/bunka_terms.jsonl\")\n\n    self.docs = documents\n    self.terms = terms\n\n    return self\n</code></pre>"},{"location":"bunka-api/bunkatopics.html#bunkatopics._bunkatopics.Bunka.manually_clean_topics","title":"<code>manually_clean_topics()</code>","text":"<p>Allows manual renaming of topic names in the dataset.</p> <p>This method facilitates the manual editing of topic names based on their IDs. If no changes are made, it retains the original topic names.</p> <p>The updated topic names are then applied to the <code>topics</code> attribute of the class instance.</p> Attributes Updated <ul> <li>self.topics: Each topic in this list gets its name updated based on the changes.</li> </ul> Logging <ul> <li>Logs the percentage of data retained after cleaning.</li> </ul> Side Effects <ul> <li>Modifies the <code>name</code> attribute of each topic in <code>self.topics</code> based on user input or defaults.</li> <li>Displays interactive widgets for user input.</li> <li>Logs information about the data cleaning process.</li> </ul> Note <ul> <li>This method uses interactive widgets (text fields and a button) for user input.</li> <li>The cleaning process is triggered by clicking the 'Apply Changes' button.</li> </ul> Source code in <code>bunkatopics/_bunkatopics.py</code> <pre><code>def manually_clean_topics(self):\n    \"\"\"\n    Allows manual renaming of topic names in the dataset.\n\n    This method facilitates the manual editing of topic names based on their IDs.\n    If no changes are made, it retains the original topic names.\n\n    The updated topic names are then applied to the `topics` attribute of the class instance.\n\n    Attributes Updated:\n        - self.topics: Each topic in this list gets its name updated based on the changes.\n\n    Logging:\n        - Logs the percentage of data retained after cleaning.\n\n    Side Effects:\n        - Modifies the `name` attribute of each topic in `self.topics` based on user input or defaults.\n        - Displays interactive widgets for user input.\n        - Logs information about the data cleaning process.\n\n    Note:\n        - This method uses interactive widgets (text fields and a button) for user input.\n        - The cleaning process is triggered by clicking the 'Apply Changes' button.\n\n    \"\"\"\n\n    def apply_changes(b):\n        for i, text_widget in enumerate(text_widgets):\n            new_name = text_widget.value.strip()\n            if new_name == \"\":\n                new_names.append(original_topic_names[i])  # Keep the same name\n            else:\n                new_names.append(new_name)\n\n        # Log changes applied\n        logger.info(\"Changes Applied!\")\n\n        # Update the topic names\n        topic_dict = dict(zip(original_topic_ids, new_names))\n        for topic in self.topics:\n            topic.name = topic_dict.get(topic.topic_id)\n\n        self.df_topics_, self.df_top_docs_per_topic_ = _create_topic_dfs(\n            self.topics, self.docs\n        )\n\n    original_topic_names = [x.name for x in self.topics]\n    original_topic_ids = [x.topic_id for x in self.topics]\n    new_names = []\n\n    # Create a list of Text widgets for entering new names with IDs as descriptions\n    text_widgets = []\n\n    for i, (topic, topic_id) in enumerate(\n        zip(original_topic_names, original_topic_ids)\n    ):\n        text_widget = widgets.Text(value=topic, description=f\"{topic_id}:\")\n        text_widgets.append(text_widget)\n\n    # Create a title widget\n    title_widget = widgets.HTML(\"Manually input the new topic names: \")\n\n    # Combine the title, Text widgets, and a button in a VBox\n    container = widgets.VBox([title_widget] + text_widgets)\n\n    # Create a button to apply changes with text color #2596be and bold description\n    apply_button = widgets.Button(\n        description=\"Apply Changes\",\n        style={\"button_color\": \"#2596be\", \"color\": \"#2596be\"},\n    )\n    apply_button.on_click(apply_changes)\n\n    # Display the container and apply button\n    display(container, apply_button)\n</code></pre>"},{"location":"bunka-api/bunkatopics.html#bunkatopics._bunkatopics.Bunka.remove_outliers","title":"<code>remove_outliers(threshold=6)</code>","text":"<p>Removes outliers from the dataset based on a specified threshold.</p> <p>This method applies an outlier detection algorithm to identify and remove outliers Args:     threshold (int): The threshold value for outlier detection. Default is 6.</p> Source code in <code>bunkatopics/_bunkatopics.py</code> <pre><code>def remove_outliers(self, threshold=6):\n    \"\"\"\n    Removes outliers from the dataset based on a specified threshold.\n\n    This method applies an outlier detection algorithm to identify and remove outliers\n    Args:\n        threshold (int): The threshold value for outlier detection. Default is 6.\n    \"\"\"\n\n    from bunkatopics.cleaning.outlier_detection import remove_outliers\n\n    cleaned_docs = remove_outliers(docs=self.docs, threshold=threshold)\n    # Calculate the number of removed documents\n\n    removed_docs_count = len(self.docs) - len(cleaned_docs)\n    logger.info(\"Number of removed documents: {}\".format(removed_docs_count))\n    self.docs = cleaned_docs\n</code></pre>"},{"location":"bunka-api/bunkatopics.html#bunkatopics._bunkatopics.Bunka.save_bunka","title":"<code>save_bunka(path='bunka_dumps')</code>","text":"<p>Save the Bunka model to disk.</p> <p>This method saves the Bunka model to disk by serializing its documents and terms.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The directory path where the model will be saved. Defaults to \"bunka_dumps\".</p> <code>'bunka_dumps'</code> <p>Examples: <code>python from bunkatopics import Bunka bunka = Bunka() ... bunka.save_bunka('bunka_dumps')</code></p> Source code in <code>bunkatopics/_bunkatopics.py</code> <pre><code>def save_bunka(self, path: str = \"bunka_dumps\"):\n    \"\"\"\n    Save the Bunka model to disk.\n\n    This method saves the Bunka model to disk by serializing its documents and terms.\n\n    Args:\n        path (str, optional): The directory path where the model will be saved.\n            Defaults to \"bunka_dumps\".\n\n    Examples:\n    ```python\n    from bunkatopics import Bunka\n    bunka = Bunka()\n    ...\n    bunka.save_bunka('bunka_dumps')```\n\n    \"\"\"\n    from .utils import save_bunka_models\n\n    save_bunka_models(path=path, bunka=self)\n</code></pre>"},{"location":"bunka-api/bunkatopics.html#bunkatopics._bunkatopics.Bunka.visualize_bourdieu","title":"<code>visualize_bourdieu(llm=None, x_left_words=['war'], x_right_words=['peace'], y_top_words=['men'], y_bottom_words=['women'], height=1500, width=1500, display_percent=True, clustering=False, topic_n_clusters=10, topic_terms=2, topic_ngrams=[1, 2], topic_top_terms_overall=1000, gen_topic_language='english', manual_axis_name=None, use_doc_gen_topic=False, radius_size=0.3, convex_hull=True, density=True, colorscale='delta', label_size_ratio_clusters=100, label_size_ratio_label=50, label_size_ratio_percent=10, min_docs_per_cluster=5)</code>","text":"<p>Creates and visualizes a Bourdieu Map using specified parameters and a generative model.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>Optional[str]</code> <p>The generative model to be used. Default is None.</p> <code>None</code> <code>x_left_words</code> <code>List[str]</code> <p>Words defining the left and left x axes.</p> <code>['war']</code> <code>x_right_words</code> <code>List[str]</code> <p>Words defining the left and right x axes.</p> <code>['peace']</code> <code>y_top_words</code> <code>List[str]</code> <p>Words defining the left and top y axes.</p> <code>['men']</code> <code>y_bottom_words</code> <code>List[str]</code> <p>Words defining the top and bottom y axes.</p> <code>['women']</code> <code>height</code> <code>int</code> <p>Dimensions of the visualization. Default to 1500.</p> <code>1500</code> <code>width</code> <code>int</code> <p>Dimensions of the visualization. Default to 1500.</p> <code>1500</code> <code>display_percent</code> <code>bool</code> <p>Flag to display percentages on the map. Default is True.</p> <code>True</code> <code>clustering</code> <code>bool</code> <p>Whether to apply clustering on the map. Default is False.</p> <code>False</code> <code>topic_n_clusters</code> <code>int</code> <p>Number of clusters for topic modeling. Default is 10.</p> <code>10</code> <code>topic_terms</code> <code>int</code> <p>Length of topic names. Default is 2.</p> <code>2</code> <code>topic_ngrams</code> <code>List[int]</code> <p>N-gram range for topic modeling. Default is [1, 2].</p> <code>[1, 2]</code> <code>topic_top_terms_overall</code> <code>int</code> <p>Top terms to consider overall. Default is 1000.</p> <code>1000</code> <code>gen_topic_language</code> <code>str</code> <p>Language for topic generation. Default is \"english\".</p> <code>'english'</code> <code>manual_axis_name</code> <code>Optional[dict]</code> <p>Custom axis names for the map. Default is None.</p> <code>None</code> <code>use_doc_gen_topic</code> <code>bool</code> <p>Flag to use document context in topic generation. Default is False.</p> <code>False</code> <code>radius_size</code> <code>float</code> <p>Radius size for the map isualization. Default is 0.3.</p> <code>0.3</code> <code>convex_hull</code> <code>bool</code> <p>Whether to include a convex hull in the visualization. Default is True.</p> <code>True</code> <code>colorscale</code> <code>str</code> <p>colorscale for the Density Plot (Default is delta)</p> <code>'delta'</code> <code>density</code> <code>bool</code> <p>Whether to display a density map</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>go.Figure: A Plotly graph object figure representing the Bourdieu Map.</p> Note <p>The Bourdieu Map is a sophisticated visualization that plots documents and topics based on specified word axes, using a generative model for dynamic analysis. This method handles the complex process of generating and plotting this map, offering a range of customization options for detailed analysis.</p> Source code in <code>bunkatopics/_bunkatopics.py</code> <pre><code>def visualize_bourdieu(\n    self,\n    llm: t.Optional[LLM] = None,\n    x_left_words: t.List[str] = [\"war\"],\n    x_right_words: t.List[str] = [\"peace\"],\n    y_top_words: t.List[str] = [\"men\"],\n    y_bottom_words: t.List[str] = [\"women\"],\n    height: int = 1500,\n    width: int = 1500,\n    display_percent: bool = True,\n    clustering: bool = False,\n    topic_n_clusters: int = 10,\n    topic_terms: int = 2,\n    topic_ngrams: t.List[int] = [1, 2],\n    topic_top_terms_overall: int = 1000,\n    gen_topic_language: str = \"english\",\n    manual_axis_name: t.Optional[dict] = None,\n    use_doc_gen_topic: bool = False,\n    radius_size: float = 0.3,\n    convex_hull: bool = True,\n    density: bool = True,\n    colorscale: str = \"delta\",\n    label_size_ratio_clusters: int = 100,\n    label_size_ratio_label: int = 50,\n    label_size_ratio_percent: int = 10,\n    min_docs_per_cluster: int = 5,\n) -&gt; go.Figure:\n    \"\"\"\n    Creates and visualizes a Bourdieu Map using specified parameters and a generative model.\n\n    Args:\n        llm (t.Optional[str]): The generative model to be used. Default is None.\n        x_left_words (t.List[str]): Words defining the left and left x axes.\n        x_right_words (t.List[str]): Words defining the left and right x axes.\n        y_top_words (t.List[str]): Words defining the left and top y axes.\n        y_bottom_words (t.List[str]): Words defining the top and bottom y axes.\n        height (int): Dimensions of the visualization. Default to 1500.\n        width (int): Dimensions of the visualization. Default to 1500.\n        display_percent (bool): Flag to display percentages on the map. Default is True.\n        clustering (bool): Whether to apply clustering on the map. Default is False.\n        topic_n_clusters (int): Number of clusters for topic modeling. Default is 10.\n        topic_terms (int): Length of topic names. Default is 2.\n        topic_ngrams (t.List[int]): N-gram range for topic modeling. Default is [1, 2].\n        topic_top_terms_overall (int): Top terms to consider overall. Default is 1000.\n        gen_topic_language (str): Language for topic generation. Default is \"english\".\n        manual_axis_name (t.Optional[dict]): Custom axis names for the map. Default is None.\n        use_doc_gen_topic (bool): Flag to use document context in topic generation. Default is False.\n        radius_size (float): Radius size for the map isualization. Default is 0.3.\n        convex_hull (bool): Whether to include a convex hull in the visualization. Default is True.\n        colorscale (str): colorscale for the Density Plot (Default is delta)\n        density (bool): Whether to display a density map\n\n    Returns:\n        go.Figure: A Plotly graph object figure representing the Bourdieu Map.\n\n    Note:\n        The Bourdieu Map is a sophisticated visualization that plots documents and topics\n        based on specified word axes, using a generative model for dynamic analysis.\n        This method handles the complex process of generating and plotting this map,\n        offering a range of customization options for detailed analysis.\n    \"\"\"\n\n    logger.info(\"Creating the Bourdieu Map\")\n    topic_gen_param = TopicGenParam(\n        language=gen_topic_language,\n        top_doc=3,\n        top_terms=10,\n        use_doc=use_doc_gen_topic,\n        context=\"everything\",\n    )\n\n    topic_param = TopicParam(\n        n_clusters=topic_n_clusters,\n        ngrams=topic_ngrams,\n        name_lenght=topic_terms,\n        top_terms_overall=topic_top_terms_overall,\n    )\n\n    self.bourdieu_query = BourdieuQuery(\n        x_left_words=x_left_words,\n        x_right_words=x_right_words,\n        y_top_words=y_top_words,\n        y_bottom_words=y_bottom_words,\n        radius_size=radius_size,\n    )\n\n    # Request Bourdieu API\n\n    bourdieu_api = BourdieuAPI(\n        llm=llm,\n        embedding_model=self.embedding_model,\n        bourdieu_query=self.bourdieu_query,\n        topic_param=topic_param,\n        topic_gen_param=topic_gen_param,\n        min_docs_per_cluster=min_docs_per_cluster,\n    )\n\n    new_docs = copy.deepcopy(self.docs)\n    new_terms = copy.deepcopy(self.terms)\n\n    res = bourdieu_api.fit_transform(\n        docs=new_docs,\n        terms=new_terms,\n    )\n\n    self.bourdieu_docs = res[0]\n    self.bourdieu_topics = res[1]\n\n    visualizer = BourdieuVisualizer(\n        height=height,\n        width=width,\n        display_percent=display_percent,\n        convex_hull=convex_hull,\n        clustering=clustering,\n        manual_axis_name=manual_axis_name,\n        density=density,\n        colorscale=colorscale,\n        label_size_ratio_clusters=label_size_ratio_clusters,\n        label_size_ratio_label=label_size_ratio_label,\n        label_size_ratio_percent=label_size_ratio_percent,\n    )\n\n    fig = visualizer.fit_transform(self.bourdieu_docs, self.bourdieu_topics)\n\n    return fig\n</code></pre>"},{"location":"bunka-api/bunkatopics.html#bunkatopics._bunkatopics.Bunka.visualize_bourdieu_one_dimension","title":"<code>visualize_bourdieu_one_dimension(left=['negative'], right=['positive'], width=800, height=800, explainer=False)</code>","text":"<p>Visualizes the document set on a one-dimensional Bourdieu axis.</p> <p>Parameters:</p> Name Type Description Default <code>left</code> <code>List[str]</code> <p>List of words representing the left side of the axis.</p> <code>['negative']</code> <code>right</code> <code>List[str]</code> <p>List of words representing the right side of the axis.</p> <code>['positive']</code> <code>width</code> <code>int</code> <p>Width of the generated visualization. Default is 800.</p> <code>800</code> <code>height</code> <code>int</code> <p>Height of the generated visualization. Default is 800.</p> <code>800</code> <code>explainer</code> <code>bool</code> <p>Flag to include an explainer figure. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>t.Tuple[go.Figure, t.Union[plt.Figure, None]]: A tuple containing the main visualization figure</p> <code>Union[Figure, None]</code> <p>and an optional explainer figure (if explainer is True).</p> Note <p>This method creates a one-dimensional Bourdieu-style visualization, plotting documents along an axis defined by contrasting word sets. It helps in understanding the distribution of documents in terms of these contrasting word concepts. An optional explainer figure can provide additional insight into specific terms used in the visualization.</p> Source code in <code>bunkatopics/_bunkatopics.py</code> <pre><code>def visualize_bourdieu_one_dimension(\n    self,\n    left: t.List[str] = [\"negative\"],\n    right: t.List[str] = [\"positive\"],\n    width: int = 800,\n    height: int = 800,\n    explainer: bool = False,\n) -&gt; t.Tuple[go.Figure, t.Union[plt.Figure, None]]:\n    \"\"\"\n    Visualizes the document set on a one-dimensional Bourdieu axis.\n\n    Args:\n        left (t.List[str]): List of words representing the left side of the axis.\n        right (t.List[str]): List of words representing the right side of the axis.\n        width (int): Width of the generated visualization. Default is 800.\n        height (int): Height of the generated visualization. Default is 800.\n        explainer (bool): Flag to include an explainer figure. Default is False.\n\n    Returns:\n        t.Tuple[go.Figure, t.Union[plt.Figure, None]]: A tuple containing the main visualization figure\n        and an optional explainer figure (if explainer is True).\n\n    Note:\n        This method creates a one-dimensional Bourdieu-style visualization, plotting documents along an\n        axis defined by contrasting word sets. It helps in understanding the distribution of documents\n        in terms of these contrasting word concepts. An optional explainer figure can provide additional\n        insight into specific terms used in the visualization.\n    \"\"\"\n\n    model_bourdieu = BourdieuOneDimensionVisualizer(\n        embedding_model=self.embedding_model,\n        left=left,\n        right=right,\n        width=width,\n        height=height,\n        explainer=explainer,\n    )\n\n    fig = model_bourdieu.fit_transform(\n        docs=self.docs,\n    )\n\n    return fig\n</code></pre>"},{"location":"bunka-api/bunkatopics.html#bunkatopics._bunkatopics.Bunka.visualize_dimensions","title":"<code>visualize_dimensions(dimensions=['positive', 'negative', 'fear', 'love'], width=500, height=500, template='plotly_dark')</code>","text":"<p>Visualizes the similarity scores between a given query and the document set.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>int</code> <p>Width of the visualization. Default is 600.</p> <code>500</code> <code>height</code> <code>int</code> <p>Height of the visualization. Default is 300.</p> <code>500</code> <p>Returns:</p> Type Description <code>Figure</code> <p>A tuple (fig, percent) where 'fig' is a Plotly graph object figure representing the</p> <code>Figure</code> <p>visualization and 'percent' is the percentage of documents above the similarity threshold.</p> Note <p>This method creates a visualization showing how closely documents in the set relate to the specified query. Documents with similarity scores above the threshold are highlighted, providing a visual representation of their relevance to the query.</p> Source code in <code>bunkatopics/_bunkatopics.py</code> <pre><code>def visualize_dimensions(\n    self,\n    dimensions: t.List[str] = [\"positive\", \"negative\", \"fear\", \"love\"],\n    width=500,\n    height=500,\n    template=\"plotly_dark\",\n) -&gt; go.Figure:\n    \"\"\"\n    Visualizes the similarity scores between a given query and the document set.\n\n    Args:\n        width (int): Width of the visualization. Default is 600.\n        height (int): Height of the visualization. Default is 300.\n\n    Returns:\n        A tuple (fig, percent) where 'fig' is a Plotly graph object figure representing the\n        visualization and 'percent' is the percentage of documents above the similarity threshold.\n\n    Note:\n        This method creates a visualization showing how closely documents in the set relate to\n        the specified query. Documents with similarity scores above the threshold are highlighted,\n        providing a visual representation of their relevance to the query.\n    \"\"\"\n\n    final_df = []\n    logger.info(\"Computing Similarities\")\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    for dim in tqdm(dimensions):\n        df_search = self.search(dim)\n        df_search = self.vectorstore.similarity_search_with_score(dim, k=3)\n        df_search[\"score\"] = scaler.fit_transform(\n            df_search[[\"cosine_similarity_score\"]]\n        )\n        df_search[\"source\"] = dim\n        final_df.append(df_search)\n    final_df = pd.concat([x for x in final_df])\n\n    final_df_mean = (\n        final_df.groupby(\"source\")[\"score\"]\n        .mean()\n        .rename(\"mean_score\")\n        .reset_index()\n    )\n    final_df_mean = final_df_mean.sort_values(\n        \"mean_score\", ascending=True\n    ).reset_index(drop=True)\n    final_df_mean[\"rank\"] = final_df_mean.index + 1\n\n    self.df_dimensions = final_df_mean\n\n    fig = px.line_polar(\n        final_df_mean,\n        r=\"mean_score\",\n        theta=\"source\",\n        line_close=True,\n        template=template,\n        width=width,\n        height=height,\n    )\n    return fig\n</code></pre>"},{"location":"bunka-api/bunkatopics.html#bunkatopics._bunkatopics.Bunka.visualize_topics","title":"<code>visualize_topics(show_text=True, label_size_ratio=100, point_size_ratio=100, width=1000, height=1000, colorscale='delta', density=True, convex_hull=True, color=None)</code>","text":"<p>Generates a visualization of the identified topics in the document set.</p> <p>Parameters:</p> Name Type Description Default <code>show_text</code> <code>bool</code> <p>Whether to display text labels on the visualization. Default is True.</p> <code>True</code> <code>label_size_ratio</code> <code>int</code> <p>The size ratio of the labels in the visualization. Default is 100.</p> <code>100</code> <code>width</code> <code>int</code> <p>The width of the visualization figure. Default is 1000.</p> <code>1000</code> <code>height</code> <code>int</code> <p>The height of the visualization figure. Default is 1000.</p> <code>1000</code> <code>colorscale</code> <code>str</code> <p>colorscale for the Density Plot (Default is delta)</p> <code>'delta'</code> <code>density</code> <code>bool</code> <p>Whether to display a density map</p> <code>True</code> <code>convex_hull</code> <code>bool</code> <p>Whether to display lines around the clusters</p> <code>True</code> <code>color</code> <code>str</code> <p>What category to use to display the color</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>go.Figure: A Plotly graph object figure representing the topic visualization.</p> Note <p>This method creates a 'Bunka Map', a graphical representation of the topics, using Plotly for interactive visualization. It displays how documents are grouped into topics and can include text labels for clarity.</p> Source code in <code>bunkatopics/_bunkatopics.py</code> <pre><code>def visualize_topics(\n    self,\n    show_text: bool = True,\n    label_size_ratio: int = 100,\n    point_size_ratio: int = 100,\n    width: int = 1000,\n    height: int = 1000,\n    colorscale: str = \"delta\",\n    density: bool = True,\n    convex_hull: bool = True,\n    color: str = None,\n    # search: str = None,\n) -&gt; go.Figure:\n    \"\"\"\n    Generates a visualization of the identified topics in the document set.\n\n    Args:\n        show_text (bool): Whether to display text labels on the visualization. Default is True.\n        label_size_ratio (int): The size ratio of the labels in the visualization. Default is 100.\n        width (int): The width of the visualization figure. Default is 1000.\n        height (int): The height of the visualization figure. Default is 1000.\n        colorscale (str): colorscale for the Density Plot (Default is delta)\n        density (bool): Whether to display a density map\n        convex_hull (bool): Whether to display lines around the clusters\n        color (str): What category to use to display the color\n\n    Returns:\n        go.Figure: A Plotly graph object figure representing the topic visualization.\n\n    Note:\n        This method creates a 'Bunka Map', a graphical representation of the topics,\n        using Plotly for interactive visualization. It displays how documents are grouped\n        into topics and can include text labels for clarity.\n    \"\"\"\n    logger.info(\"Creating the Bunka Map\")\n\n    model_visualizer = TopicVisualizer(\n        width=width,\n        height=height,\n        show_text=show_text,\n        label_size_ratio=label_size_ratio,\n        point_size_ratio=point_size_ratio,\n        colorscale=colorscale,\n        density=density,\n        convex_hull=convex_hull,\n    )\n    fig = model_visualizer.fit_transform(self.docs, self.topics, color=color)\n\n    return fig\n</code></pre>"},{"location":"bunka-api/bourdieu/bourdieu-api.html","title":"Bourdieu API","text":"<p>A class for performing Bourdieu analysis on a collection of documents.</p> <p>This class leverages an embedding model to compute Bourdieu dimensions and topics for the given documents. It supports customization of the analysis through various parameters and the use of generative AI for topic naming.</p> Source code in <code>bunkatopics/bourdieu/bourdieu_api.py</code> <pre><code>class BourdieuAPI:\n    \"\"\"\n    A class for performing Bourdieu analysis on a collection of documents.\n\n    This class leverages an embedding model to compute Bourdieu dimensions and topics\n    for the given documents. It supports customization of the analysis through various parameters\n    and the use of generative AI for topic naming.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_model: Embeddings,\n        llm: t.Optional[LLM] = None,\n        bourdieu_query: BourdieuQuery = BourdieuQuery(),\n        topic_param: TopicParam = TopicParam(),\n        topic_gen_param: TopicGenParam = TopicGenParam(),\n        min_count_terms: int = 2,\n        ranking_terms: int = 20,\n        min_docs_per_cluster: int = 20,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the BourdieuAPI with the provided models, parameters, and configurations.\n\n        Args:\n            llm: The generative AI model for topic naming.\n            embedding_model: The model used for embedding documents.\n            bourdieu_query (BourdieuQuery, optional): Configuration for Bourdieu analysis.\n                                                       Defaults to BourdieuQuery().\n            topic_param (TopicParam, optional): Parameters for topic modeling. Defaults to TopicParam().\n            topic_gen_param (TopicGenParam, optional): Parameters for the generative AI in topic naming.\n                                                       Defaults to TopicGenParam().\n            min_count_terms (int, optional): Minimum term count for topic modeling. Defaults to 2.\n        \"\"\"\n\n        self.llm = llm\n        self.embedding_model = embedding_model\n        self.bourdieu_query = bourdieu_query\n        self.topic_param = topic_param\n        self.topic_gen_param = topic_gen_param\n        self.min_count_terms = min_count_terms\n        self.ranking_terms = ranking_terms\n        self.min_docs_per_cluster = min_docs_per_cluster\n\n    def fit_transform(\n        self, docs: t.List[Document], terms: t.List[Term]\n    ) -&gt; t.Tuple[t.List[Document], t.List[Topic]]:\n        \"\"\"\n        Processes the documents and terms to compute Bourdieu dimensions and topics.\n\n        This method applies the embedding model to compute Bourdieu dimensions for each document\n        based on provided queries. It also performs topic modeling on the documents and, if enabled,\n        uses a generative AI model for naming the topics.\n\n        Arguments:\n            docs (List[Document]): List of Document objects representing the documents to be analyzed.\n            terms (List[Term]): List of Term objects representing the terms to be used in topic modeling.\n\n        Notes:\n            - The method first resets Bourdieu dimensions for all documents.\n            - It computes Bourdieu continuums based on the configured left and right words.\n            - Documents are then filtered based on their position relative to a defined radius in the Bourdieu space.\n            - Topic modeling is performed on the filtered set of documents.\n            - If `generative_ai_name` is True, topics are named using the generative AI model.\n        \"\"\"\n\n        # Reset Bourdieu dimensions for all documents\n        for doc in docs:\n            doc.bourdieu_dimensions = []\n\n        # Compute Continuums\n        new_docs = _get_continuum(\n            self.embedding_model,\n            docs,\n            cont_name=\"cont1\",\n            left_words=self.bourdieu_query.x_left_words,\n            right_words=self.bourdieu_query.x_right_words,\n        )\n        bourdieu_docs = _get_continuum(\n            self.embedding_model,\n            new_docs,\n            cont_name=\"cont2\",\n            left_words=self.bourdieu_query.y_top_words,\n            right_words=self.bourdieu_query.y_bottom_words,\n        )\n\n        # Process and transform data\n        df_bourdieu = pd.DataFrame(\n            [\n                {\n                    \"doc_id\": x.doc_id,\n                    \"coordinates\": [y.distance for y in x.bourdieu_dimensions],\n                    \"names\": [y.continuum.id for y in x.bourdieu_dimensions],\n                }\n                for x in bourdieu_docs\n            ]\n        )\n        df_bourdieu = df_bourdieu.explode([\"coordinates\", \"names\"])\n\n        df_bourdieu_pivot = df_bourdieu[[\"doc_id\", \"coordinates\", \"names\"]]\n        df_bourdieu_pivot = df_bourdieu_pivot.pivot(\n            index=\"doc_id\", columns=\"names\", values=\"coordinates\"\n        )\n\n        # Add to the bourdieu_docs\n        df_outsides = df_bourdieu_pivot.reset_index()\n        df_outsides[\"cont1\"] = df_outsides[\"cont1\"].astype(float)\n        df_outsides[\"cont2\"] = df_outsides[\"cont2\"].astype(float)\n\n        x_values = df_outsides[\"cont1\"].values\n        y_values = df_outsides[\"cont2\"].values\n\n        distances = np.sqrt(x_values**2 + y_values**2)\n        circle_radius = max(df_outsides.cont1) * self.bourdieu_query.radius_size\n\n        df_outsides[\"distances\"] = distances\n        df_outsides[\"outside\"] = \"0\"\n        df_outsides.loc[df_outsides[\"distances\"] &gt;= circle_radius, \"outside\"] = \"1\"\n\n        outside_ids = list(df_outsides[\"doc_id\"][df_outsides[\"outside\"] == \"1\"])\n        bourdieu_docs = [x for x in bourdieu_docs if x.doc_id in outside_ids]\n        bourdieu_dict = df_bourdieu_pivot.to_dict(orient=\"index\")\n\n        for doc in bourdieu_docs:\n            doc.x = bourdieu_dict.get(doc.doc_id)[\"cont1\"]\n            doc.y = bourdieu_dict.get(doc.doc_id)[\"cont2\"]\n\n        # Compute Bourdieu topics\n        topic_model = BunkaTopicModeling(\n            n_clusters=self.topic_param.n_clusters,\n            ngrams=self.topic_param.ngrams,\n            name_length=self.topic_param.name_length,\n            top_terms_overall=self.topic_param.top_terms_overall,\n            min_count_terms=self.min_count_terms,\n            min_docs_per_cluster=self.min_docs_per_cluster,\n        )\n\n        bourdieu_topics: t.List[Topic] = topic_model.fit_transform(\n            docs=bourdieu_docs,\n            terms=terms,\n        )\n        model_ranker = DocumentRanker(ranking_terms=self.ranking_terms)\n        bourdieu_docs, bourdieu_topics = model_ranker.fit_transform(\n            bourdieu_docs, bourdieu_topics\n        )\n\n        if self.llm:\n            model_cleaning = LLMCleaningTopic(\n                self.llm,\n                language=self.topic_gen_param.language,\n                use_doc=self.topic_gen_param.use_doc,\n                context=self.topic_gen_param.context,\n            )\n            bourdieu_topics: t.List[Topic] = model_cleaning.fit_transform(\n                bourdieu_topics,\n                bourdieu_docs,\n            )\n\n        return bourdieu_docs, bourdieu_topics\n</code></pre>"},{"location":"bunka-api/bourdieu/bourdieu-api.html#bunkatopics.bourdieu.bourdieu_api.BourdieuAPI.__init__","title":"<code>__init__(embedding_model, llm=None, bourdieu_query=BourdieuQuery(), topic_param=TopicParam(), topic_gen_param=TopicGenParam(), min_count_terms=2, ranking_terms=20, min_docs_per_cluster=20)</code>","text":"<p>Initializes the BourdieuAPI with the provided models, parameters, and configurations.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>Optional[LLM]</code> <p>The generative AI model for topic naming.</p> <code>None</code> <code>embedding_model</code> <code>Embeddings</code> <p>The model used for embedding documents.</p> required <code>bourdieu_query</code> <code>BourdieuQuery</code> <p>Configuration for Bourdieu analysis.                                        Defaults to BourdieuQuery().</p> <code>BourdieuQuery()</code> <code>topic_param</code> <code>TopicParam</code> <p>Parameters for topic modeling. Defaults to TopicParam().</p> <code>TopicParam()</code> <code>topic_gen_param</code> <code>TopicGenParam</code> <p>Parameters for the generative AI in topic naming.                                        Defaults to TopicGenParam().</p> <code>TopicGenParam()</code> <code>min_count_terms</code> <code>int</code> <p>Minimum term count for topic modeling. Defaults to 2.</p> <code>2</code> Source code in <code>bunkatopics/bourdieu/bourdieu_api.py</code> <pre><code>def __init__(\n    self,\n    embedding_model: Embeddings,\n    llm: t.Optional[LLM] = None,\n    bourdieu_query: BourdieuQuery = BourdieuQuery(),\n    topic_param: TopicParam = TopicParam(),\n    topic_gen_param: TopicGenParam = TopicGenParam(),\n    min_count_terms: int = 2,\n    ranking_terms: int = 20,\n    min_docs_per_cluster: int = 20,\n) -&gt; None:\n    \"\"\"\n    Initializes the BourdieuAPI with the provided models, parameters, and configurations.\n\n    Args:\n        llm: The generative AI model for topic naming.\n        embedding_model: The model used for embedding documents.\n        bourdieu_query (BourdieuQuery, optional): Configuration for Bourdieu analysis.\n                                                   Defaults to BourdieuQuery().\n        topic_param (TopicParam, optional): Parameters for topic modeling. Defaults to TopicParam().\n        topic_gen_param (TopicGenParam, optional): Parameters for the generative AI in topic naming.\n                                                   Defaults to TopicGenParam().\n        min_count_terms (int, optional): Minimum term count for topic modeling. Defaults to 2.\n    \"\"\"\n\n    self.llm = llm\n    self.embedding_model = embedding_model\n    self.bourdieu_query = bourdieu_query\n    self.topic_param = topic_param\n    self.topic_gen_param = topic_gen_param\n    self.min_count_terms = min_count_terms\n    self.ranking_terms = ranking_terms\n    self.min_docs_per_cluster = min_docs_per_cluster\n</code></pre>"},{"location":"bunka-api/bourdieu/bourdieu-api.html#bunkatopics.bourdieu.bourdieu_api.BourdieuAPI.fit_transform","title":"<code>fit_transform(docs, terms)</code>","text":"<p>Processes the documents and terms to compute Bourdieu dimensions and topics.</p> <p>This method applies the embedding model to compute Bourdieu dimensions for each document based on provided queries. It also performs topic modeling on the documents and, if enabled, uses a generative AI model for naming the topics.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Document objects representing the documents to be analyzed.</p> required <code>terms</code> <code>List[Term]</code> <p>List of Term objects representing the terms to be used in topic modeling.</p> required Notes <ul> <li>The method first resets Bourdieu dimensions for all documents.</li> <li>It computes Bourdieu continuums based on the configured left and right words.</li> <li>Documents are then filtered based on their position relative to a defined radius in the Bourdieu space.</li> <li>Topic modeling is performed on the filtered set of documents.</li> <li>If <code>generative_ai_name</code> is True, topics are named using the generative AI model.</li> </ul> Source code in <code>bunkatopics/bourdieu/bourdieu_api.py</code> <pre><code>def fit_transform(\n    self, docs: t.List[Document], terms: t.List[Term]\n) -&gt; t.Tuple[t.List[Document], t.List[Topic]]:\n    \"\"\"\n    Processes the documents and terms to compute Bourdieu dimensions and topics.\n\n    This method applies the embedding model to compute Bourdieu dimensions for each document\n    based on provided queries. It also performs topic modeling on the documents and, if enabled,\n    uses a generative AI model for naming the topics.\n\n    Arguments:\n        docs (List[Document]): List of Document objects representing the documents to be analyzed.\n        terms (List[Term]): List of Term objects representing the terms to be used in topic modeling.\n\n    Notes:\n        - The method first resets Bourdieu dimensions for all documents.\n        - It computes Bourdieu continuums based on the configured left and right words.\n        - Documents are then filtered based on their position relative to a defined radius in the Bourdieu space.\n        - Topic modeling is performed on the filtered set of documents.\n        - If `generative_ai_name` is True, topics are named using the generative AI model.\n    \"\"\"\n\n    # Reset Bourdieu dimensions for all documents\n    for doc in docs:\n        doc.bourdieu_dimensions = []\n\n    # Compute Continuums\n    new_docs = _get_continuum(\n        self.embedding_model,\n        docs,\n        cont_name=\"cont1\",\n        left_words=self.bourdieu_query.x_left_words,\n        right_words=self.bourdieu_query.x_right_words,\n    )\n    bourdieu_docs = _get_continuum(\n        self.embedding_model,\n        new_docs,\n        cont_name=\"cont2\",\n        left_words=self.bourdieu_query.y_top_words,\n        right_words=self.bourdieu_query.y_bottom_words,\n    )\n\n    # Process and transform data\n    df_bourdieu = pd.DataFrame(\n        [\n            {\n                \"doc_id\": x.doc_id,\n                \"coordinates\": [y.distance for y in x.bourdieu_dimensions],\n                \"names\": [y.continuum.id for y in x.bourdieu_dimensions],\n            }\n            for x in bourdieu_docs\n        ]\n    )\n    df_bourdieu = df_bourdieu.explode([\"coordinates\", \"names\"])\n\n    df_bourdieu_pivot = df_bourdieu[[\"doc_id\", \"coordinates\", \"names\"]]\n    df_bourdieu_pivot = df_bourdieu_pivot.pivot(\n        index=\"doc_id\", columns=\"names\", values=\"coordinates\"\n    )\n\n    # Add to the bourdieu_docs\n    df_outsides = df_bourdieu_pivot.reset_index()\n    df_outsides[\"cont1\"] = df_outsides[\"cont1\"].astype(float)\n    df_outsides[\"cont2\"] = df_outsides[\"cont2\"].astype(float)\n\n    x_values = df_outsides[\"cont1\"].values\n    y_values = df_outsides[\"cont2\"].values\n\n    distances = np.sqrt(x_values**2 + y_values**2)\n    circle_radius = max(df_outsides.cont1) * self.bourdieu_query.radius_size\n\n    df_outsides[\"distances\"] = distances\n    df_outsides[\"outside\"] = \"0\"\n    df_outsides.loc[df_outsides[\"distances\"] &gt;= circle_radius, \"outside\"] = \"1\"\n\n    outside_ids = list(df_outsides[\"doc_id\"][df_outsides[\"outside\"] == \"1\"])\n    bourdieu_docs = [x for x in bourdieu_docs if x.doc_id in outside_ids]\n    bourdieu_dict = df_bourdieu_pivot.to_dict(orient=\"index\")\n\n    for doc in bourdieu_docs:\n        doc.x = bourdieu_dict.get(doc.doc_id)[\"cont1\"]\n        doc.y = bourdieu_dict.get(doc.doc_id)[\"cont2\"]\n\n    # Compute Bourdieu topics\n    topic_model = BunkaTopicModeling(\n        n_clusters=self.topic_param.n_clusters,\n        ngrams=self.topic_param.ngrams,\n        name_length=self.topic_param.name_length,\n        top_terms_overall=self.topic_param.top_terms_overall,\n        min_count_terms=self.min_count_terms,\n        min_docs_per_cluster=self.min_docs_per_cluster,\n    )\n\n    bourdieu_topics: t.List[Topic] = topic_model.fit_transform(\n        docs=bourdieu_docs,\n        terms=terms,\n    )\n    model_ranker = DocumentRanker(ranking_terms=self.ranking_terms)\n    bourdieu_docs, bourdieu_topics = model_ranker.fit_transform(\n        bourdieu_docs, bourdieu_topics\n    )\n\n    if self.llm:\n        model_cleaning = LLMCleaningTopic(\n            self.llm,\n            language=self.topic_gen_param.language,\n            use_doc=self.topic_gen_param.use_doc,\n            context=self.topic_gen_param.context,\n        )\n        bourdieu_topics: t.List[Topic] = model_cleaning.fit_transform(\n            bourdieu_topics,\n            bourdieu_docs,\n        )\n\n    return bourdieu_docs, bourdieu_topics\n</code></pre>"},{"location":"bunka-api/bourdieu/bourdieu-visualization-one-dimension.html","title":"Bourdieu One Dimension Visualizer","text":"<p>A class to visualize data distribution along a unique continuum inspired by Bourdieu's theory using an embedding model.</p> Source code in <code>bunkatopics/bourdieu/bourdieu_one_dimension.py</code> <pre><code>class BourdieuOneDimensionVisualizer:\n    \"\"\"\n    A class to visualize data distribution along a unique continuum inspired by Bourdieu's theory using an embedding model.\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_model: Embeddings,\n        left: str = [\"aggressivity\"],\n        right: str = [\"peacefulness\"],\n        height=700,\n        width=600,\n        explainer: bool = False,\n        explainer_ngrams: list = [1, 2],\n    ) -&gt; None:\n        \"\"\"\n        Constructs all the necessary attributes for the BourdieuOneDimensionVisualizer object.\n\n        Args:\n            embedding_model: The embedding model used for encoding text.\n            left (List[str]): Keywords indicating one end of the continuum. Defaults to [\"aggressivity\"].\n            right (List[str]): Keywords indicating the other end of the continuum. Defaults to [\"peacefulness\"].\n            height (int): Height of the visualization plot. Default is 700.\n            width (int): Width of the visualization plot. Default is 600.\n            explainer (bool): If True, includes an explanation component in the visualization. Default is False.\n            explainer_ngrams (List[int]): N-grams to use for generating explanations. Default is [1, 2].\n        \"\"\"\n        pass\n        self.embedding_model = embedding_model\n        self.left = left\n        self.right = right\n        self.height = height\n        self.width = width\n        self.explainer = explainer\n        self.explainer_ngrams = explainer_ngrams\n\n    def fit_transform(self, docs: t.List[Document]) -&gt; go.Figure:\n        \"\"\"\n        Analyzes a list of Document objects and visualizes their distribution along the unique continuum.\n\n        Args:\n            docs (List[Document]): A list of Document objects to be analyzed.\n\n        Returns:\n            Tuple[go.Figure, plt]: A tuple containing a Plotly figures and a matplolib Figure. The first figure represents the\n                                         distribution of data along the continuum, and the second figure\n                                         (if explainer is True) represents specific terms that characterize\n                                         the distribution.\n        \"\"\"\n        self.id = str(random.randint(0, 10000))\n        self.docs = docs\n\n        self.new_docs = _get_continuum(\n            embedding_model=self.embedding_model,\n            docs=self.docs,\n            cont_name=self.id,\n            left_words=self.left,\n            right_words=self.right,\n            scale=False,\n        )\n\n        fig = self.plot_unique_dimension()\n        return fig\n\n    def plot_unique_dimension(self) -&gt; go.Figure:\n        \"\"\"\n        Generates a Plotly figure representing the unique dimension continuum.\n\n        This method is used internally by fit_transform to create the visualization.\n\n        Returns:\n            go.Figure: A Plotly figure visualizing the distribution of documents along the unique continuum.\n        \"\"\"\n        left = \" \".join(self.left)\n        right = \" \".join(self.right)\n\n        distances = [\n            x.distance\n            for doc in self.new_docs\n            for x in doc.bourdieu_dimensions\n            if x.continuum.id == self.id\n        ]\n        doc_id = [x.doc_id for x in self.new_docs]\n        content = [x.content for x in self.new_docs]\n\n        df_distances = pd.DataFrame(\n            {\"doc_id\": doc_id, \"distances\": distances, \"content\": content}\n        )\n\n        name = \"&lt;\" + right + \"-\" + left + \"&gt;\"\n\n        df_fig = df_distances.rename(columns={\"distances\": name})\n        df_fig[\"content\"] = df_fig[\"content\"].apply(lambda x: wrap_by_word(x, 10))\n\n        fig = px.box(\n            df_fig,\n            y=name,\n            points=\"all\",\n            hover_data=[\"content\"],\n            height=self.height,\n            width=self.width,\n            template=\"plotly_white\",\n        )\n\n        fig.add_shape(\n            dict(\n                type=\"line\",\n                x0=df_fig[name].min(),  # Set the minimum x-coordinate of the line\n                x1=df_fig[name].max(),  # Set the maximum x-coordinate of the line\n                y0=0,\n                y1=0,\n                line=dict(color=\"red\", width=4),\n            )\n        )\n\n        \"\"\"fig_specific_terms = plot_specific_terms(\n            docs=self.new_docs,\n            left_words=left,\n            right_words=right,\n            id=self.id,\n            ngrams=self.explainer_ngrams,\n            quantile=0.80,\n            top_n=20,\n        )\"\"\"\n\n        return fig\n</code></pre>"},{"location":"bunka-api/bourdieu/bourdieu-visualization-one-dimension.html#bunkatopics.bourdieu.bourdieu_one_dimension.BourdieuOneDimensionVisualizer.__init__","title":"<code>__init__(embedding_model, left=['aggressivity'], right=['peacefulness'], height=700, width=600, explainer=False, explainer_ngrams=[1, 2])</code>","text":"<p>Constructs all the necessary attributes for the BourdieuOneDimensionVisualizer object.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_model</code> <code>Embeddings</code> <p>The embedding model used for encoding text.</p> required <code>left</code> <code>List[str]</code> <p>Keywords indicating one end of the continuum. Defaults to [\"aggressivity\"].</p> <code>['aggressivity']</code> <code>right</code> <code>List[str]</code> <p>Keywords indicating the other end of the continuum. Defaults to [\"peacefulness\"].</p> <code>['peacefulness']</code> <code>height</code> <code>int</code> <p>Height of the visualization plot. Default is 700.</p> <code>700</code> <code>width</code> <code>int</code> <p>Width of the visualization plot. Default is 600.</p> <code>600</code> <code>explainer</code> <code>bool</code> <p>If True, includes an explanation component in the visualization. Default is False.</p> <code>False</code> <code>explainer_ngrams</code> <code>List[int]</code> <p>N-grams to use for generating explanations. Default is [1, 2].</p> <code>[1, 2]</code> Source code in <code>bunkatopics/bourdieu/bourdieu_one_dimension.py</code> <pre><code>def __init__(\n    self,\n    embedding_model: Embeddings,\n    left: str = [\"aggressivity\"],\n    right: str = [\"peacefulness\"],\n    height=700,\n    width=600,\n    explainer: bool = False,\n    explainer_ngrams: list = [1, 2],\n) -&gt; None:\n    \"\"\"\n    Constructs all the necessary attributes for the BourdieuOneDimensionVisualizer object.\n\n    Args:\n        embedding_model: The embedding model used for encoding text.\n        left (List[str]): Keywords indicating one end of the continuum. Defaults to [\"aggressivity\"].\n        right (List[str]): Keywords indicating the other end of the continuum. Defaults to [\"peacefulness\"].\n        height (int): Height of the visualization plot. Default is 700.\n        width (int): Width of the visualization plot. Default is 600.\n        explainer (bool): If True, includes an explanation component in the visualization. Default is False.\n        explainer_ngrams (List[int]): N-grams to use for generating explanations. Default is [1, 2].\n    \"\"\"\n    pass\n    self.embedding_model = embedding_model\n    self.left = left\n    self.right = right\n    self.height = height\n    self.width = width\n    self.explainer = explainer\n    self.explainer_ngrams = explainer_ngrams\n</code></pre>"},{"location":"bunka-api/bourdieu/bourdieu-visualization-one-dimension.html#bunkatopics.bourdieu.bourdieu_one_dimension.BourdieuOneDimensionVisualizer.fit_transform","title":"<code>fit_transform(docs)</code>","text":"<p>Analyzes a list of Document objects and visualizes their distribution along the unique continuum.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>A list of Document objects to be analyzed.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>Tuple[go.Figure, plt]: A tuple containing a Plotly figures and a matplolib Figure. The first figure represents the                          distribution of data along the continuum, and the second figure                          (if explainer is True) represents specific terms that characterize                          the distribution.</p> Source code in <code>bunkatopics/bourdieu/bourdieu_one_dimension.py</code> <pre><code>def fit_transform(self, docs: t.List[Document]) -&gt; go.Figure:\n    \"\"\"\n    Analyzes a list of Document objects and visualizes their distribution along the unique continuum.\n\n    Args:\n        docs (List[Document]): A list of Document objects to be analyzed.\n\n    Returns:\n        Tuple[go.Figure, plt]: A tuple containing a Plotly figures and a matplolib Figure. The first figure represents the\n                                     distribution of data along the continuum, and the second figure\n                                     (if explainer is True) represents specific terms that characterize\n                                     the distribution.\n    \"\"\"\n    self.id = str(random.randint(0, 10000))\n    self.docs = docs\n\n    self.new_docs = _get_continuum(\n        embedding_model=self.embedding_model,\n        docs=self.docs,\n        cont_name=self.id,\n        left_words=self.left,\n        right_words=self.right,\n        scale=False,\n    )\n\n    fig = self.plot_unique_dimension()\n    return fig\n</code></pre>"},{"location":"bunka-api/bourdieu/bourdieu-visualization-one-dimension.html#bunkatopics.bourdieu.bourdieu_one_dimension.BourdieuOneDimensionVisualizer.plot_unique_dimension","title":"<code>plot_unique_dimension()</code>","text":"<p>Generates a Plotly figure representing the unique dimension continuum.</p> <p>This method is used internally by fit_transform to create the visualization.</p> <p>Returns:</p> Type Description <code>Figure</code> <p>go.Figure: A Plotly figure visualizing the distribution of documents along the unique continuum.</p> Source code in <code>bunkatopics/bourdieu/bourdieu_one_dimension.py</code> <pre><code>def plot_unique_dimension(self) -&gt; go.Figure:\n    \"\"\"\n    Generates a Plotly figure representing the unique dimension continuum.\n\n    This method is used internally by fit_transform to create the visualization.\n\n    Returns:\n        go.Figure: A Plotly figure visualizing the distribution of documents along the unique continuum.\n    \"\"\"\n    left = \" \".join(self.left)\n    right = \" \".join(self.right)\n\n    distances = [\n        x.distance\n        for doc in self.new_docs\n        for x in doc.bourdieu_dimensions\n        if x.continuum.id == self.id\n    ]\n    doc_id = [x.doc_id for x in self.new_docs]\n    content = [x.content for x in self.new_docs]\n\n    df_distances = pd.DataFrame(\n        {\"doc_id\": doc_id, \"distances\": distances, \"content\": content}\n    )\n\n    name = \"&lt;\" + right + \"-\" + left + \"&gt;\"\n\n    df_fig = df_distances.rename(columns={\"distances\": name})\n    df_fig[\"content\"] = df_fig[\"content\"].apply(lambda x: wrap_by_word(x, 10))\n\n    fig = px.box(\n        df_fig,\n        y=name,\n        points=\"all\",\n        hover_data=[\"content\"],\n        height=self.height,\n        width=self.width,\n        template=\"plotly_white\",\n    )\n\n    fig.add_shape(\n        dict(\n            type=\"line\",\n            x0=df_fig[name].min(),  # Set the minimum x-coordinate of the line\n            x1=df_fig[name].max(),  # Set the maximum x-coordinate of the line\n            y0=0,\n            y1=0,\n            line=dict(color=\"red\", width=4),\n        )\n    )\n\n    \"\"\"fig_specific_terms = plot_specific_terms(\n        docs=self.new_docs,\n        left_words=left,\n        right_words=right,\n        id=self.id,\n        ngrams=self.explainer_ngrams,\n        quantile=0.80,\n        top_n=20,\n    )\"\"\"\n\n    return fig\n</code></pre>"},{"location":"bunka-api/bourdieu/bourdieu-visualization.html","title":"Bourdieu Visualization","text":"<p>A class for visualizing Bourdieu's field analysis through a scatter plot.</p> <p>This visualizer plots documents on a 2D space based on their coordinates derived from Bourdieu's field analysis. It offers features like displaying percentage areas, drawing convex hulls around clusters, and labeling axes with specified terms.</p> Source code in <code>bunkatopics/bourdieu/bourdieu_visualizer.py</code> <pre><code>class BourdieuVisualizer:\n    \"\"\"\n    A class for visualizing Bourdieu's field analysis through a scatter plot.\n\n    This visualizer plots documents on a 2D space based on their coordinates derived from\n    Bourdieu's field analysis. It offers features like displaying percentage areas,\n    drawing convex hulls around clusters, and labeling axes with specified terms.\n    \"\"\"\n\n    def __init__(\n        self,\n        display_percent: bool = True,\n        convex_hull: bool = True,\n        clustering: bool = True,\n        width: int = 800,\n        height: int = 800,\n        label_size_ratio_clusters: int = 100,\n        label_size_ratio_label: int = 50,\n        label_size_ratio_percent: int = 10,\n        manual_axis_name: t.Optional[dict] = None,\n        density: bool = True,\n        colorscale: str = \"delta\",\n    ) -&gt; None:\n        \"\"\"\n        Constructs all the necessary attributes for the BourdieuVisualizer object.\n\n        Args:\n            display_percent (bool): If True, display the percentage of documents in each quadrant.\n            convex_hull (bool): If True, draw convex hulls around topic clusters.\n            clustering (bool): If True, enable clustering of topics.\n            width (int): Width of the plot.\n            height (int): Height of the plot.\n            label_size_ratio_clusters (int): Size ratio for the labels of clusters.\n            label_size_ratio_label (int): Size ratio for the labels on axes.\n            label_size_ratio_percent (int): Size ratio for the percentage labels.\n            manual_axis_name (Optional[dict]): Custom names for the axes, if provided.\n            colorscale (str): The color scale for contour density representation. Defaults to \"delta\".\n            density (bool): Whether to display a density map\n        \"\"\"\n        self.display_percent = display_percent\n        self.convex_hull = convex_hull\n        self.clustering = clustering\n        self.width = width\n        self.height = height\n        self.label_size_ratio_clusters = label_size_ratio_clusters\n        self.label_size_ratio_label = label_size_ratio_label\n        self.label_size_ratio_percent = label_size_ratio_percent\n        self.manual_axis_name = manual_axis_name\n        self.density = density\n        self.colorscale = colorscale\n\n    def fit_transform(\n        self, bourdieu_docs: t.List[Document], bourdieu_topics: t.List[Topic]\n    ) -&gt; go.Figure:\n        \"\"\"\n        Transforms the given documents and topics into a Plotly figure based on Bourdieu's analysis.\n\n        This method takes a list of documents and topics, processes them, and returns a Plotly figure\n        representing the documents in a 2D space as per Bourdieu's field analysis.\n\n        Args:\n            bourdieu_docs (List[Document]): A list of Document objects to be visualized.\n            bourdieu_topics (List[Topic]): A list of Topic objects used for clustering.\n\n        Returns:\n            go.Figure: A Plotly figure object representing the visualized documents and topics.\n        \"\"\"\n        df_fig = pd.DataFrame(\n            {\n                \"doc_id\": [x.doc_id for x in bourdieu_docs],\n                \"x\": [x.x for x in bourdieu_docs],\n                \"y\": [x.y for x in bourdieu_docs],\n                \"content\": [x.content for x in bourdieu_docs],\n            }\n        )\n\n        df_fig[\"Text\"] = df_fig[\"content\"].apply(lambda x: wrap_by_word(x, 10))\n\n        if self.density:\n            fig = go.Figure(\n                go.Histogram2dContour(\n                    x=df_fig[\"x\"],\n                    y=df_fig[\"y\"],\n                    colorscale=self.colorscale,\n                    showscale=False,\n                    hoverinfo=\"none\",\n                ),\n            )\n        else:\n            fig = go.Figure()\n\n        scatter_fig = px.scatter(\n            df_fig,\n            x=\"x\",\n            y=\"y\",\n            # color=\"outside\",\n            # color_discrete_map={\"1\": \"white\", \"0\": \"grey\"},\n            # hover_data=[\"Text\"],\n            hover_data={\"x\": False, \"y\": False, \"Text\": True},\n            template=\"simple_white\",\n            opacity=0.3,\n        )\n\n        for trace in scatter_fig.data:\n            fig.add_trace(trace)\n\n        if self.density:\n            label_axe_color = \"white\"\n        else:\n            label_axe_color = \"black\"\n\n        fig.update_xaxes(\n            title_text=\"\",\n            scaleanchor=\"y\",\n            scaleratio=1,\n            showgrid=False,\n            showticklabels=False,\n            zeroline=True,\n            zerolinecolor=label_axe_color,\n            zerolinewidth=3,\n        )\n        fig.update_yaxes(\n            title_text=\"\",\n            scaleanchor=\"x\",\n            scaleratio=1,\n            showgrid=False,\n            showticklabels=False,\n            zeroline=True,\n            zerolinecolor=label_axe_color,\n            zerolinewidth=3,\n        )\n\n        # Add axis lines for x=0 and y=0\n        fig.add_shape(\n            type=\"line\",\n            x0=0,\n            x1=0,\n            y0=min(df_fig[\"y\"]),\n            y1=max(df_fig[\"y\"]),\n            line=dict(color=label_axe_color, width=3),  # Customize line color and width\n        )\n\n        fig.add_shape(\n            type=\"line\",\n            x0=min(df_fig[\"x\"]),\n            x1=max(df_fig[\"x\"]),\n            y0=0,\n            y1=0,\n            line=dict(color=label_axe_color, width=3),  # Customize line color and width\n        )\n\n        x_left_name = bourdieu_docs[0].bourdieu_dimensions[0].continuum.left_words\n        x_left_name = \" \".join(x_left_name)\n\n        x_right_name = bourdieu_docs[0].bourdieu_dimensions[0].continuum.right_words\n        x_right_name = \" \".join(x_right_name)\n\n        y_top_name = bourdieu_docs[0].bourdieu_dimensions[1].continuum.left_words\n        y_top_name = \" \".join(y_top_name)\n\n        y_bottom_name = bourdieu_docs[0].bourdieu_dimensions[1].continuum.right_words\n        y_bottom_name = \" \".join(y_bottom_name)\n\n        if self.manual_axis_name is not None:\n            y_top_name = self.manual_axis_name[\"y_top_name\"]\n            y_bottom_name = self.manual_axis_name[\"y_bottom_name\"]\n            x_left_name = self.manual_axis_name[\"x_left_name\"]\n            x_right_name = self.manual_axis_name[\"x_right_name\"]\n\n        fig.update_layout(\n            annotations=[\n                dict(\n                    x=0,\n                    # y=max_val,\n                    y=max(df_fig[\"y\"]),\n                    xref=\"x\",\n                    yref=\"y\",\n                    text=y_top_name,\n                    showarrow=False,\n                    xanchor=\"right\",\n                    yanchor=\"top\",\n                    font=dict(\n                        size=self.width / self.label_size_ratio_label,\n                        color=label_axe_color,\n                    ),\n                ),\n                dict(\n                    x=0,\n                    y=min(df_fig[\"y\"]),\n                    # y=-max_val,\n                    xref=\"x\",\n                    yref=\"y\",\n                    text=y_bottom_name,\n                    showarrow=False,\n                    xanchor=\"left\",\n                    yanchor=\"bottom\",\n                    font=dict(\n                        size=self.width / self.label_size_ratio_label,\n                        color=label_axe_color,\n                    ),\n                ),\n                dict(\n                    x=max(df_fig[\"x\"]),\n                    # x=max_val,\n                    y=0,\n                    xref=\"x\",\n                    yref=\"y\",\n                    text=x_left_name,\n                    showarrow=False,\n                    xanchor=\"right\",\n                    yanchor=\"top\",\n                    font=dict(\n                        size=self.width / self.label_size_ratio_label,\n                        color=label_axe_color,\n                    ),\n                ),\n                dict(\n                    x=min(df_fig[\"x\"]),\n                    # x=-max_val,\n                    y=0,\n                    xref=\"x\",\n                    yref=\"y\",\n                    text=x_right_name,\n                    showarrow=False,\n                    xanchor=\"left\",\n                    yanchor=\"bottom\",\n                    font=dict(\n                        size=self.width / self.label_size_ratio_label,\n                        color=label_axe_color,\n                    ),\n                ),\n            ]\n        )\n        if self.clustering:\n            topics_x = [x.x_centroid for x in bourdieu_topics]\n            topics_y = [x.y_centroid for x in bourdieu_topics]\n            topic_names = [x.name for x in bourdieu_topics]\n            topics_name_plotly = [wrap_by_word(x, 7) for x in topic_names]\n\n            # Display Topics\n            for x, y, label in zip(topics_x, topics_y, topics_name_plotly):\n                fig.add_annotation(\n                    x=x,\n                    y=y,\n                    text=label,\n                    font=dict(\n                        family=\"Courier New, monospace\",\n                        size=self.width / self.label_size_ratio_clusters,\n                        color=\"blue\",\n                    ),\n                    bordercolor=\"#c7c7c7\",\n                    borderwidth=self.width / 1000,\n                    borderpad=self.width / 500,\n                    bgcolor=\"white\",\n                    opacity=1,\n                )\n\n            if self.convex_hull:\n                try:\n                    for topic in bourdieu_topics:\n                        # Create a Scatter plot with the convex hull coordinates\n                        trace = go.Scatter(\n                            x=topic.convex_hull.x_coordinates,\n                            y=topic.convex_hull.y_coordinates,  # Assuming y=0 for simplicity\n                            mode=\"lines\",\n                            name=\"Convex Hull\",\n                            line=dict(color=\"grey\", dash=\"dot\"),\n                            showlegend=False,\n                            hoverinfo=\"none\",\n                        )\n\n                        fig.add_trace(trace)\n                except Exception as e:\n                    print(e)\n\n        if self.display_percent:\n            # Calculate the percentage for every box\n            opacity = 0.4\n            case1_count = len(df_fig[(df_fig[\"x\"] &lt; 0) &amp; (df_fig[\"y\"] &lt; 0)])\n            total_count = len(df_fig)\n            case1_percentage = str(round((case1_count / total_count) * 100, 1)) + \"%\"\n\n            fig.add_annotation(\n                x=min(df_fig[\"x\"]),\n                y=min(df_fig[\"y\"]),\n                text=case1_percentage,\n                font=dict(\n                    family=\"Courier New, monospace\",\n                    size=self.width / self.label_size_ratio_percent,\n                    color=\"grey\",\n                ),\n                opacity=opacity,\n                xanchor=\"left\",\n            )\n\n            case2_count = len(df_fig[(df_fig[\"x\"] &lt; 0) &amp; (df_fig[\"y\"] &gt; 0)])\n            case2_percentage = str(round((case2_count / total_count) * 100, 1)) + \"%\"\n\n            fig.add_annotation(\n                x=min(df_fig[\"x\"]),\n                y=max(df_fig[\"y\"]),\n                text=case2_percentage,\n                font=dict(\n                    family=\"Courier New, monospace\",\n                    size=self.width / self.label_size_ratio_percent,\n                    color=\"grey\",\n                ),\n                opacity=opacity,\n                xanchor=\"left\",\n            )\n\n            case3_count = len(df_fig[(df_fig[\"x\"] &gt; 0) &amp; (df_fig[\"y\"] &lt; 0)])\n            case3_percentage = str(round((case3_count / total_count) * 100, 1)) + \"%\"\n\n            fig.add_annotation(\n                x=max(df_fig[\"x\"]),\n                y=min(df_fig[\"y\"]),\n                text=case3_percentage,\n                font=dict(\n                    family=\"Courier New, monospace\",\n                    size=self.width / self.label_size_ratio_percent,\n                    color=\"grey\",\n                ),\n                opacity=opacity,\n                xanchor=\"left\",\n            )\n\n            case4_count = len(df_fig[(df_fig[\"x\"] &gt; 0) &amp; (df_fig[\"y\"] &gt; 0)])\n            case4_percentage = str(round((case4_count / total_count) * 100, 1)) + \"%\"\n\n            fig.add_annotation(\n                x=max(df_fig[\"x\"]),\n                y=max(df_fig[\"y\"]),\n                text=case4_percentage,\n                font=dict(\n                    family=\"Courier New, monospace\",\n                    size=self.width / self.label_size_ratio_percent,\n                    color=\"grey\",\n                ),\n                opacity=opacity,\n                xanchor=\"left\",\n            )\n\n        fig.update_layout(\n            font_size=25,\n            height=self.height,\n            width=self.width,\n            margin=dict(\n                t=self.width / 50,\n                b=self.width / 50,\n                r=self.width / 50,\n                l=self.width / 50,\n            ),\n        )\n\n        fig.update_layout(showlegend=False)\n        return fig\n</code></pre>"},{"location":"bunka-api/bourdieu/bourdieu-visualization.html#bunkatopics.bourdieu.bourdieu_visualizer.BourdieuVisualizer.__init__","title":"<code>__init__(display_percent=True, convex_hull=True, clustering=True, width=800, height=800, label_size_ratio_clusters=100, label_size_ratio_label=50, label_size_ratio_percent=10, manual_axis_name=None, density=True, colorscale='delta')</code>","text":"<p>Constructs all the necessary attributes for the BourdieuVisualizer object.</p> <p>Parameters:</p> Name Type Description Default <code>display_percent</code> <code>bool</code> <p>If True, display the percentage of documents in each quadrant.</p> <code>True</code> <code>convex_hull</code> <code>bool</code> <p>If True, draw convex hulls around topic clusters.</p> <code>True</code> <code>clustering</code> <code>bool</code> <p>If True, enable clustering of topics.</p> <code>True</code> <code>width</code> <code>int</code> <p>Width of the plot.</p> <code>800</code> <code>height</code> <code>int</code> <p>Height of the plot.</p> <code>800</code> <code>label_size_ratio_clusters</code> <code>int</code> <p>Size ratio for the labels of clusters.</p> <code>100</code> <code>label_size_ratio_label</code> <code>int</code> <p>Size ratio for the labels on axes.</p> <code>50</code> <code>label_size_ratio_percent</code> <code>int</code> <p>Size ratio for the percentage labels.</p> <code>10</code> <code>manual_axis_name</code> <code>Optional[dict]</code> <p>Custom names for the axes, if provided.</p> <code>None</code> <code>colorscale</code> <code>str</code> <p>The color scale for contour density representation. Defaults to \"delta\".</p> <code>'delta'</code> <code>density</code> <code>bool</code> <p>Whether to display a density map</p> <code>True</code> Source code in <code>bunkatopics/bourdieu/bourdieu_visualizer.py</code> <pre><code>def __init__(\n    self,\n    display_percent: bool = True,\n    convex_hull: bool = True,\n    clustering: bool = True,\n    width: int = 800,\n    height: int = 800,\n    label_size_ratio_clusters: int = 100,\n    label_size_ratio_label: int = 50,\n    label_size_ratio_percent: int = 10,\n    manual_axis_name: t.Optional[dict] = None,\n    density: bool = True,\n    colorscale: str = \"delta\",\n) -&gt; None:\n    \"\"\"\n    Constructs all the necessary attributes for the BourdieuVisualizer object.\n\n    Args:\n        display_percent (bool): If True, display the percentage of documents in each quadrant.\n        convex_hull (bool): If True, draw convex hulls around topic clusters.\n        clustering (bool): If True, enable clustering of topics.\n        width (int): Width of the plot.\n        height (int): Height of the plot.\n        label_size_ratio_clusters (int): Size ratio for the labels of clusters.\n        label_size_ratio_label (int): Size ratio for the labels on axes.\n        label_size_ratio_percent (int): Size ratio for the percentage labels.\n        manual_axis_name (Optional[dict]): Custom names for the axes, if provided.\n        colorscale (str): The color scale for contour density representation. Defaults to \"delta\".\n        density (bool): Whether to display a density map\n    \"\"\"\n    self.display_percent = display_percent\n    self.convex_hull = convex_hull\n    self.clustering = clustering\n    self.width = width\n    self.height = height\n    self.label_size_ratio_clusters = label_size_ratio_clusters\n    self.label_size_ratio_label = label_size_ratio_label\n    self.label_size_ratio_percent = label_size_ratio_percent\n    self.manual_axis_name = manual_axis_name\n    self.density = density\n    self.colorscale = colorscale\n</code></pre>"},{"location":"bunka-api/bourdieu/bourdieu-visualization.html#bunkatopics.bourdieu.bourdieu_visualizer.BourdieuVisualizer.fit_transform","title":"<code>fit_transform(bourdieu_docs, bourdieu_topics)</code>","text":"<p>Transforms the given documents and topics into a Plotly figure based on Bourdieu's analysis.</p> <p>This method takes a list of documents and topics, processes them, and returns a Plotly figure representing the documents in a 2D space as per Bourdieu's field analysis.</p> <p>Parameters:</p> Name Type Description Default <code>bourdieu_docs</code> <code>List[Document]</code> <p>A list of Document objects to be visualized.</p> required <code>bourdieu_topics</code> <code>List[Topic]</code> <p>A list of Topic objects used for clustering.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>go.Figure: A Plotly figure object representing the visualized documents and topics.</p> Source code in <code>bunkatopics/bourdieu/bourdieu_visualizer.py</code> <pre><code>def fit_transform(\n    self, bourdieu_docs: t.List[Document], bourdieu_topics: t.List[Topic]\n) -&gt; go.Figure:\n    \"\"\"\n    Transforms the given documents and topics into a Plotly figure based on Bourdieu's analysis.\n\n    This method takes a list of documents and topics, processes them, and returns a Plotly figure\n    representing the documents in a 2D space as per Bourdieu's field analysis.\n\n    Args:\n        bourdieu_docs (List[Document]): A list of Document objects to be visualized.\n        bourdieu_topics (List[Topic]): A list of Topic objects used for clustering.\n\n    Returns:\n        go.Figure: A Plotly figure object representing the visualized documents and topics.\n    \"\"\"\n    df_fig = pd.DataFrame(\n        {\n            \"doc_id\": [x.doc_id for x in bourdieu_docs],\n            \"x\": [x.x for x in bourdieu_docs],\n            \"y\": [x.y for x in bourdieu_docs],\n            \"content\": [x.content for x in bourdieu_docs],\n        }\n    )\n\n    df_fig[\"Text\"] = df_fig[\"content\"].apply(lambda x: wrap_by_word(x, 10))\n\n    if self.density:\n        fig = go.Figure(\n            go.Histogram2dContour(\n                x=df_fig[\"x\"],\n                y=df_fig[\"y\"],\n                colorscale=self.colorscale,\n                showscale=False,\n                hoverinfo=\"none\",\n            ),\n        )\n    else:\n        fig = go.Figure()\n\n    scatter_fig = px.scatter(\n        df_fig,\n        x=\"x\",\n        y=\"y\",\n        # color=\"outside\",\n        # color_discrete_map={\"1\": \"white\", \"0\": \"grey\"},\n        # hover_data=[\"Text\"],\n        hover_data={\"x\": False, \"y\": False, \"Text\": True},\n        template=\"simple_white\",\n        opacity=0.3,\n    )\n\n    for trace in scatter_fig.data:\n        fig.add_trace(trace)\n\n    if self.density:\n        label_axe_color = \"white\"\n    else:\n        label_axe_color = \"black\"\n\n    fig.update_xaxes(\n        title_text=\"\",\n        scaleanchor=\"y\",\n        scaleratio=1,\n        showgrid=False,\n        showticklabels=False,\n        zeroline=True,\n        zerolinecolor=label_axe_color,\n        zerolinewidth=3,\n    )\n    fig.update_yaxes(\n        title_text=\"\",\n        scaleanchor=\"x\",\n        scaleratio=1,\n        showgrid=False,\n        showticklabels=False,\n        zeroline=True,\n        zerolinecolor=label_axe_color,\n        zerolinewidth=3,\n    )\n\n    # Add axis lines for x=0 and y=0\n    fig.add_shape(\n        type=\"line\",\n        x0=0,\n        x1=0,\n        y0=min(df_fig[\"y\"]),\n        y1=max(df_fig[\"y\"]),\n        line=dict(color=label_axe_color, width=3),  # Customize line color and width\n    )\n\n    fig.add_shape(\n        type=\"line\",\n        x0=min(df_fig[\"x\"]),\n        x1=max(df_fig[\"x\"]),\n        y0=0,\n        y1=0,\n        line=dict(color=label_axe_color, width=3),  # Customize line color and width\n    )\n\n    x_left_name = bourdieu_docs[0].bourdieu_dimensions[0].continuum.left_words\n    x_left_name = \" \".join(x_left_name)\n\n    x_right_name = bourdieu_docs[0].bourdieu_dimensions[0].continuum.right_words\n    x_right_name = \" \".join(x_right_name)\n\n    y_top_name = bourdieu_docs[0].bourdieu_dimensions[1].continuum.left_words\n    y_top_name = \" \".join(y_top_name)\n\n    y_bottom_name = bourdieu_docs[0].bourdieu_dimensions[1].continuum.right_words\n    y_bottom_name = \" \".join(y_bottom_name)\n\n    if self.manual_axis_name is not None:\n        y_top_name = self.manual_axis_name[\"y_top_name\"]\n        y_bottom_name = self.manual_axis_name[\"y_bottom_name\"]\n        x_left_name = self.manual_axis_name[\"x_left_name\"]\n        x_right_name = self.manual_axis_name[\"x_right_name\"]\n\n    fig.update_layout(\n        annotations=[\n            dict(\n                x=0,\n                # y=max_val,\n                y=max(df_fig[\"y\"]),\n                xref=\"x\",\n                yref=\"y\",\n                text=y_top_name,\n                showarrow=False,\n                xanchor=\"right\",\n                yanchor=\"top\",\n                font=dict(\n                    size=self.width / self.label_size_ratio_label,\n                    color=label_axe_color,\n                ),\n            ),\n            dict(\n                x=0,\n                y=min(df_fig[\"y\"]),\n                # y=-max_val,\n                xref=\"x\",\n                yref=\"y\",\n                text=y_bottom_name,\n                showarrow=False,\n                xanchor=\"left\",\n                yanchor=\"bottom\",\n                font=dict(\n                    size=self.width / self.label_size_ratio_label,\n                    color=label_axe_color,\n                ),\n            ),\n            dict(\n                x=max(df_fig[\"x\"]),\n                # x=max_val,\n                y=0,\n                xref=\"x\",\n                yref=\"y\",\n                text=x_left_name,\n                showarrow=False,\n                xanchor=\"right\",\n                yanchor=\"top\",\n                font=dict(\n                    size=self.width / self.label_size_ratio_label,\n                    color=label_axe_color,\n                ),\n            ),\n            dict(\n                x=min(df_fig[\"x\"]),\n                # x=-max_val,\n                y=0,\n                xref=\"x\",\n                yref=\"y\",\n                text=x_right_name,\n                showarrow=False,\n                xanchor=\"left\",\n                yanchor=\"bottom\",\n                font=dict(\n                    size=self.width / self.label_size_ratio_label,\n                    color=label_axe_color,\n                ),\n            ),\n        ]\n    )\n    if self.clustering:\n        topics_x = [x.x_centroid for x in bourdieu_topics]\n        topics_y = [x.y_centroid for x in bourdieu_topics]\n        topic_names = [x.name for x in bourdieu_topics]\n        topics_name_plotly = [wrap_by_word(x, 7) for x in topic_names]\n\n        # Display Topics\n        for x, y, label in zip(topics_x, topics_y, topics_name_plotly):\n            fig.add_annotation(\n                x=x,\n                y=y,\n                text=label,\n                font=dict(\n                    family=\"Courier New, monospace\",\n                    size=self.width / self.label_size_ratio_clusters,\n                    color=\"blue\",\n                ),\n                bordercolor=\"#c7c7c7\",\n                borderwidth=self.width / 1000,\n                borderpad=self.width / 500,\n                bgcolor=\"white\",\n                opacity=1,\n            )\n\n        if self.convex_hull:\n            try:\n                for topic in bourdieu_topics:\n                    # Create a Scatter plot with the convex hull coordinates\n                    trace = go.Scatter(\n                        x=topic.convex_hull.x_coordinates,\n                        y=topic.convex_hull.y_coordinates,  # Assuming y=0 for simplicity\n                        mode=\"lines\",\n                        name=\"Convex Hull\",\n                        line=dict(color=\"grey\", dash=\"dot\"),\n                        showlegend=False,\n                        hoverinfo=\"none\",\n                    )\n\n                    fig.add_trace(trace)\n            except Exception as e:\n                print(e)\n\n    if self.display_percent:\n        # Calculate the percentage for every box\n        opacity = 0.4\n        case1_count = len(df_fig[(df_fig[\"x\"] &lt; 0) &amp; (df_fig[\"y\"] &lt; 0)])\n        total_count = len(df_fig)\n        case1_percentage = str(round((case1_count / total_count) * 100, 1)) + \"%\"\n\n        fig.add_annotation(\n            x=min(df_fig[\"x\"]),\n            y=min(df_fig[\"y\"]),\n            text=case1_percentage,\n            font=dict(\n                family=\"Courier New, monospace\",\n                size=self.width / self.label_size_ratio_percent,\n                color=\"grey\",\n            ),\n            opacity=opacity,\n            xanchor=\"left\",\n        )\n\n        case2_count = len(df_fig[(df_fig[\"x\"] &lt; 0) &amp; (df_fig[\"y\"] &gt; 0)])\n        case2_percentage = str(round((case2_count / total_count) * 100, 1)) + \"%\"\n\n        fig.add_annotation(\n            x=min(df_fig[\"x\"]),\n            y=max(df_fig[\"y\"]),\n            text=case2_percentage,\n            font=dict(\n                family=\"Courier New, monospace\",\n                size=self.width / self.label_size_ratio_percent,\n                color=\"grey\",\n            ),\n            opacity=opacity,\n            xanchor=\"left\",\n        )\n\n        case3_count = len(df_fig[(df_fig[\"x\"] &gt; 0) &amp; (df_fig[\"y\"] &lt; 0)])\n        case3_percentage = str(round((case3_count / total_count) * 100, 1)) + \"%\"\n\n        fig.add_annotation(\n            x=max(df_fig[\"x\"]),\n            y=min(df_fig[\"y\"]),\n            text=case3_percentage,\n            font=dict(\n                family=\"Courier New, monospace\",\n                size=self.width / self.label_size_ratio_percent,\n                color=\"grey\",\n            ),\n            opacity=opacity,\n            xanchor=\"left\",\n        )\n\n        case4_count = len(df_fig[(df_fig[\"x\"] &gt; 0) &amp; (df_fig[\"y\"] &gt; 0)])\n        case4_percentage = str(round((case4_count / total_count) * 100, 1)) + \"%\"\n\n        fig.add_annotation(\n            x=max(df_fig[\"x\"]),\n            y=max(df_fig[\"y\"]),\n            text=case4_percentage,\n            font=dict(\n                family=\"Courier New, monospace\",\n                size=self.width / self.label_size_ratio_percent,\n                color=\"grey\",\n            ),\n            opacity=opacity,\n            xanchor=\"left\",\n        )\n\n    fig.update_layout(\n        font_size=25,\n        height=self.height,\n        width=self.width,\n        margin=dict(\n            t=self.width / 50,\n            b=self.width / 50,\n            r=self.width / 50,\n            l=self.width / 50,\n        ),\n    )\n\n    fig.update_layout(showlegend=False)\n    return fig\n</code></pre>"},{"location":"bunka-api/topic_modeling/terms_extractor.html","title":"Topic Extractor","text":"<p>Extracts terms from text using Textacy and SpaCy libraries.</p> <p>This class provides functionalities to extract terms from a given list of documents, considering various linguistic features like n-grams, named entities, and noun chunks.</p> Source code in <code>bunkatopics/topic_modeling/term_extractor.py</code> <pre><code>class TextacyTermsExtractor:\n    \"\"\"\n    Extracts terms from text using Textacy and SpaCy libraries.\n\n    This class provides functionalities to extract terms from a given list of documents,\n    considering various linguistic features like n-grams, named entities, and noun chunks.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ngrams: t.List[int] = [1, 2, 3],\n        ngs: bool = True,\n        ents: bool = False,\n        ncs: bool = False,\n        drop_emoji: bool = True,\n        include_pos: t.List[str] = [\"NOUN\"],\n        include_types: t.List[str] = [\"PERSON\", \"ORG\"],\n        language: str = \"en\",\n    ):\n        \"\"\"\n        Initializes the TextacyTermsExtractor with specified configuration.\n\n        Args:\n            ngrams (tuple[int, ...]): Tuple of n-gram lengths to consider. Defaults to (1, 2, 3).\n            ngs (bool): Include n-grams in extraction. Defaults to True.\n            ents (bool): Include named entities in extraction. Defaults to True.\n            ncs (bool): Include noun chunks in extraction. Defaults to True.\n            drop_emoji (bool): Remove emojis before extraction. Defaults to True.\n            include_pos (list[str]): POS tags to include. Defaults to [\"NOUN\"].\n            include_types (list[str]): Entity types to include. Defaults to [\"PERSON\", \"ORG\"].\n\n        Raises:\n            ValueError: If the specified language is not supported.\n        \"\"\"\n\n        self.ngs = ngs\n        self.ents = ents\n        self.ncs = ncs\n        self.drop_emoji = drop_emoji\n        self.include_pos = include_pos\n        self.include_types = include_types\n        self.ngrams = ngrams\n        self.language = language\n\n    def fit_transform(\n        self,\n        ids: t.List[DOC_ID],\n        sentences: t.List[str],\n    ) -&gt; t.Tuple[t.List[Term], t.Dict[DOC_ID, t.List[TERM_ID]]]:\n        \"\"\"\n        Extracts terms from the provided documents and returns them along with their indices.\n\n        Args:\n            ids (List[DOC_ID]): List of document IDs.\n            sentences (List[str]): List of sentences corresponding to the document IDs.\n\n        Notes:\n            - The method processes each document to extract relevant terms based on the configured\n            linguistic features such as n-grams, named entities, and noun chunks.\n            - It also handles pre-processing steps like normalizing text, removing brackets,\n            replacing currency symbols, removing HTML tags, and optionally dropping emojis.\n        \"\"\"\n\n        self.language_model = detect_language_to_spacy_model.get(self.language)\n\n        if self.language_model is None:\n            logger.info(\n                \"We could not find the adapted model, we set to en_core_web_sm (English) as default\"\n            )\n            self.language_model = \"en_core_web_sm\"\n\n        try:\n            spacy.load(self.language_model)\n        except OSError:\n            # The model is not installed, so download it\n            spacy.cli.download(self.language_model)\n\n        # Create a DataFrame from the provided document IDs and sentences\n        self.df = pd.DataFrame({\"content\": sentences, \"doc_id\": ids})\n\n        # Extract terms from the DataFrame\n        df_terms, df_terms_indexed = self.extract_terms_df(\n            self.df,\n            text_var=\"content\",\n            index_var=\"doc_id\",\n            ngs=self.ngs,\n            ents=self.ents,\n            ncs=self.ncs,\n            drop_emoji=self.drop_emoji,\n            ngrams=self.ngrams,\n            remove_punctuation=True,\n            include_pos=self.include_pos,\n            include_types=self.include_types,\n            language_model=self.language_model,\n        )\n\n        # Process and return the extracted terms\n        df_terms = df_terms.reset_index().rename(columns={\"terms_indexed\": \"term_id\"})\n        terms = [Term(**row) for row in df_terms.to_dict(orient=\"records\")]\n        self.terms: t.List[Term] = terms\n\n        df_terms_indexed = df_terms_indexed.reset_index().rename(\n            columns={\"text\": \"terms_indexed\"}\n        )\n        indexed_terms_dict = df_terms_indexed.set_index(\"doc_id\")[\n            \"terms_indexed\"\n        ].to_dict()\n\n        return terms, indexed_terms_dict\n\n    def extract_terms_df(\n        self,\n        data: pd.DataFrame,\n        text_var: str,\n        index_var: str,\n        ngs: bool = True,\n        ents: bool = True,\n        ncs: bool = False,\n        drop_emoji: bool = True,\n        ngrams: t.Tuple[int, int] = (2, 2),\n        remove_punctuation: bool = False,\n        include_pos: t.List[str] = [\"NOUN\", \"PROPN\", \"ADJ\"],\n        include_types: t.List[str] = [\"PERSON\", \"ORG\"],\n        language_model: str = \"en_core_web_sm\",\n    ) -&gt; t.Tuple[pd.DataFrame, pd.DataFrame]:\n        load_lang = textacy.load_spacy_lang(language_model, disable=())\n\n        def extract_terms(\n            tuple: t.Tuple[int, str],\n            ngs: bool,\n            ents: bool,\n            ncs: bool,\n            ngrams: t.Tuple[int, int],\n            drop_emoji: bool,\n            remove_punctuation: bool,\n            include_pos: t.List[str],\n            include_types: t.List[str],\n        ) -&gt; pd.DataFrame:\n            index = tuple[0]\n            text = tuple[1]\n\n            prepro_text = preproc(str(text))\n            if drop_emoji:\n                prepro_text = textacy.preprocessing.replace.emojis(prepro_text, repl=\"\")\n\n            if remove_punctuation:\n                prepro_text = textacy.preprocessing.remove.punctuation(prepro_text)\n\n            doc = textacy.make_spacy_doc(prepro_text, lang=load_lang)\n\n            terms = []\n\n            if ngs:\n                ngrams_terms = list(\n                    textacy.extract.terms(\n                        doc,\n                        ngs=partial(\n                            textacy.extract.ngrams,\n                            n=ngrams,\n                            filter_punct=True,\n                            filter_stops=True,\n                            include_pos=include_pos,\n                        ),\n                        dedupe=False,\n                    )\n                )\n\n                terms.append(ngrams_terms)\n\n            if ents:\n                ents_terms = list(\n                    textacy.extract.terms(\n                        doc,\n                        ents=partial(\n                            textacy.extract.entities, include_types=include_types\n                        ),\n                        dedupe=False,\n                    )\n                )\n                terms.append(ents_terms)\n\n            if ncs:\n                ncs_terms = list(\n                    textacy.extract.terms(\n                        doc,\n                        ncs=partial(textacy.extract.noun_chunks, drop_determiners=True),\n                        dedupe=False,\n                    )\n                )\n\n                noun_chunks = [x for x in ncs_terms if len(x) &gt;= 3]\n                terms.append(noun_chunks)\n\n            final = [item for sublist in terms for item in sublist]\n            final = list(set(final))\n\n            df = [\n                (term.text, term.lemma_.lower(), term.label_, term.__len__())\n                for term in final\n            ]\n            df = pd.DataFrame(df, columns=[\"text\", \"lemma\", \"ent\", \"ngrams\"])\n            df[\"text_index\"] = index\n\n            return df\n\n        data = data[data[text_var].notna()]\n\n        sentences = data[text_var].tolist()\n        indexes = data[index_var].tolist()\n        inputs = [(x, y) for x, y in zip(indexes, sentences)]\n\n        res = list(\n            tqdm(\n                map(\n                    partial(\n                        extract_terms,\n                        ngs=ngs,\n                        ents=ents,\n                        ncs=ncs,\n                        drop_emoji=drop_emoji,\n                        remove_punctuation=remove_punctuation,\n                        ngrams=ngrams,\n                        include_pos=include_pos,\n                        include_types=include_types,\n                    ),\n                    inputs,\n                ),\n                total=len(inputs),\n            )\n        )\n\n        final_res = pd.concat([x for x in res])\n\n        terms = (\n            final_res.groupby([\"text\", \"lemma\", \"ent\", \"ngrams\"])\n            .agg(count_terms=(\"text_index\", \"count\"))\n            .reset_index()\n        )\n\n        terms = terms.sort_values([\"text\", \"ent\"]).reset_index(drop=True)\n        terms = terms.drop_duplicates([\"text\"], keep=\"first\")\n        terms = terms.sort_values(\"count_terms\", ascending=False)\n        terms = terms.rename(columns={\"text\": \"terms_indexed\"})\n        terms = terms.set_index(\"terms_indexed\")\n\n        terms_indexed = final_res[[\"text\", \"text_index\"]].drop_duplicates()\n        terms_indexed = terms_indexed.rename(columns={\"text_index\": index_var})\n        terms_indexed = terms_indexed.groupby(index_var)[\"text\"].apply(list)\n        terms_indexed = terms_indexed.reset_index()\n        terms_indexed = terms_indexed.rename(columns={\"text\": \"terms_indexed\"})\n        terms_indexed = terms_indexed.set_index(index_var)\n\n        return terms, terms_indexed\n</code></pre>"},{"location":"bunka-api/topic_modeling/terms_extractor.html#bunkatopics.topic_modeling.term_extractor.TextacyTermsExtractor.__init__","title":"<code>__init__(ngrams=[1, 2, 3], ngs=True, ents=False, ncs=False, drop_emoji=True, include_pos=['NOUN'], include_types=['PERSON', 'ORG'], language='en')</code>","text":"<p>Initializes the TextacyTermsExtractor with specified configuration.</p> <p>Parameters:</p> Name Type Description Default <code>ngrams</code> <code>tuple[int, ...]</code> <p>Tuple of n-gram lengths to consider. Defaults to (1, 2, 3).</p> <code>[1, 2, 3]</code> <code>ngs</code> <code>bool</code> <p>Include n-grams in extraction. Defaults to True.</p> <code>True</code> <code>ents</code> <code>bool</code> <p>Include named entities in extraction. Defaults to True.</p> <code>False</code> <code>ncs</code> <code>bool</code> <p>Include noun chunks in extraction. Defaults to True.</p> <code>False</code> <code>drop_emoji</code> <code>bool</code> <p>Remove emojis before extraction. Defaults to True.</p> <code>True</code> <code>include_pos</code> <code>list[str]</code> <p>POS tags to include. Defaults to [\"NOUN\"].</p> <code>['NOUN']</code> <code>include_types</code> <code>list[str]</code> <p>Entity types to include. Defaults to [\"PERSON\", \"ORG\"].</p> <code>['PERSON', 'ORG']</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified language is not supported.</p> Source code in <code>bunkatopics/topic_modeling/term_extractor.py</code> <pre><code>def __init__(\n    self,\n    ngrams: t.List[int] = [1, 2, 3],\n    ngs: bool = True,\n    ents: bool = False,\n    ncs: bool = False,\n    drop_emoji: bool = True,\n    include_pos: t.List[str] = [\"NOUN\"],\n    include_types: t.List[str] = [\"PERSON\", \"ORG\"],\n    language: str = \"en\",\n):\n    \"\"\"\n    Initializes the TextacyTermsExtractor with specified configuration.\n\n    Args:\n        ngrams (tuple[int, ...]): Tuple of n-gram lengths to consider. Defaults to (1, 2, 3).\n        ngs (bool): Include n-grams in extraction. Defaults to True.\n        ents (bool): Include named entities in extraction. Defaults to True.\n        ncs (bool): Include noun chunks in extraction. Defaults to True.\n        drop_emoji (bool): Remove emojis before extraction. Defaults to True.\n        include_pos (list[str]): POS tags to include. Defaults to [\"NOUN\"].\n        include_types (list[str]): Entity types to include. Defaults to [\"PERSON\", \"ORG\"].\n\n    Raises:\n        ValueError: If the specified language is not supported.\n    \"\"\"\n\n    self.ngs = ngs\n    self.ents = ents\n    self.ncs = ncs\n    self.drop_emoji = drop_emoji\n    self.include_pos = include_pos\n    self.include_types = include_types\n    self.ngrams = ngrams\n    self.language = language\n</code></pre>"},{"location":"bunka-api/topic_modeling/terms_extractor.html#bunkatopics.topic_modeling.term_extractor.TextacyTermsExtractor.fit_transform","title":"<code>fit_transform(ids, sentences)</code>","text":"<p>Extracts terms from the provided documents and returns them along with their indices.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[DOC_ID]</code> <p>List of document IDs.</p> required <code>sentences</code> <code>List[str]</code> <p>List of sentences corresponding to the document IDs.</p> required Notes <ul> <li>The method processes each document to extract relevant terms based on the configured linguistic features such as n-grams, named entities, and noun chunks.</li> <li>It also handles pre-processing steps like normalizing text, removing brackets, replacing currency symbols, removing HTML tags, and optionally dropping emojis.</li> </ul> Source code in <code>bunkatopics/topic_modeling/term_extractor.py</code> <pre><code>def fit_transform(\n    self,\n    ids: t.List[DOC_ID],\n    sentences: t.List[str],\n) -&gt; t.Tuple[t.List[Term], t.Dict[DOC_ID, t.List[TERM_ID]]]:\n    \"\"\"\n    Extracts terms from the provided documents and returns them along with their indices.\n\n    Args:\n        ids (List[DOC_ID]): List of document IDs.\n        sentences (List[str]): List of sentences corresponding to the document IDs.\n\n    Notes:\n        - The method processes each document to extract relevant terms based on the configured\n        linguistic features such as n-grams, named entities, and noun chunks.\n        - It also handles pre-processing steps like normalizing text, removing brackets,\n        replacing currency symbols, removing HTML tags, and optionally dropping emojis.\n    \"\"\"\n\n    self.language_model = detect_language_to_spacy_model.get(self.language)\n\n    if self.language_model is None:\n        logger.info(\n            \"We could not find the adapted model, we set to en_core_web_sm (English) as default\"\n        )\n        self.language_model = \"en_core_web_sm\"\n\n    try:\n        spacy.load(self.language_model)\n    except OSError:\n        # The model is not installed, so download it\n        spacy.cli.download(self.language_model)\n\n    # Create a DataFrame from the provided document IDs and sentences\n    self.df = pd.DataFrame({\"content\": sentences, \"doc_id\": ids})\n\n    # Extract terms from the DataFrame\n    df_terms, df_terms_indexed = self.extract_terms_df(\n        self.df,\n        text_var=\"content\",\n        index_var=\"doc_id\",\n        ngs=self.ngs,\n        ents=self.ents,\n        ncs=self.ncs,\n        drop_emoji=self.drop_emoji,\n        ngrams=self.ngrams,\n        remove_punctuation=True,\n        include_pos=self.include_pos,\n        include_types=self.include_types,\n        language_model=self.language_model,\n    )\n\n    # Process and return the extracted terms\n    df_terms = df_terms.reset_index().rename(columns={\"terms_indexed\": \"term_id\"})\n    terms = [Term(**row) for row in df_terms.to_dict(orient=\"records\")]\n    self.terms: t.List[Term] = terms\n\n    df_terms_indexed = df_terms_indexed.reset_index().rename(\n        columns={\"text\": \"terms_indexed\"}\n    )\n    indexed_terms_dict = df_terms_indexed.set_index(\"doc_id\")[\n        \"terms_indexed\"\n    ].to_dict()\n\n    return terms, indexed_terms_dict\n</code></pre>"},{"location":"bunka-api/topic_modeling/topic-ranker.html","title":"Topic Extractor","text":"Source code in <code>bunkatopics/topic_modeling/document_topic_ranker.py</code> <pre><code>class DocumentRanker:\n    def __init__(self, ranking_terms: int = 20, max_doc_per_topic: int = 20) -&gt; None:\n        \"\"\"\n        Initialize the class with ranking_terms and max_doc_per_topic parameters.\n\n        Args:\n            ranking_terms (int): Number of ranking terms to be used.\n            max_doc_per_topic (int): Maximum number of documents per topic.\n\n        Returns:\n            None\n        \"\"\"\n        self.ranking_terms = ranking_terms\n        self.max_doc_per_topic = max_doc_per_topic\n\n    def fit_transform(\n        self,\n        docs: t.List[Document],\n        topics: t.List[Topic],\n    ) -&gt; t.Tuple[t.List[Document], t.List[Topic]]:\n        \"\"\"\n        Calculate top documents for each topic based on ranking terms.\n\n        Args:\n            docs (List[Document]): List of documents.\n            topics (List[Topic]): List of topics.\n\n        Returns:\n            Tuple[List[Document], List[Topic]]: Updated lists of documents and topics.\n        \"\"\"\n        # Create a DataFrame from the list of documents\n        df_docs = pd.DataFrame.from_records([doc.model_dump() for doc in docs])\n\n        # Explode the term_id column to have one row per term\n        df_docs = df_docs[[\"doc_id\", \"topic_id\", \"term_id\"]]\n        df_docs = df_docs.explode(\"term_id\").reset_index(drop=True)\n\n        # Create a DataFrame from the list of topics and truncate term_id\n        df_topics = pd.DataFrame.from_records([topic.model_dump() for topic in topics])\n        df_topics[\"term_id\"] = df_topics[\"term_id\"].apply(\n            lambda x: x[: self.ranking_terms]\n        )\n        df_topics = df_topics[[\"topic_id\", \"term_id\"]]\n        df_topics = df_topics.explode(\"term_id\").reset_index(drop=True)\n\n        # Merge documents and topics, and calculate term counts\n        df_rank = pd.merge(df_docs, df_topics, on=[\"topic_id\", \"term_id\"])\n        df_rank = (\n            df_rank.groupby([\"topic_id\", \"doc_id\"])[\"term_id\"]\n            .count()\n            .rename(\"count_topic_terms\")\n            .reset_index()\n        )\n\n        # Sort and rank documents within each topic\n        df_rank = df_rank.sort_values(\n            [\"topic_id\", \"count_topic_terms\"], ascending=(True, False)\n        ).reset_index(drop=True)\n        df_rank[\"rank\"] = df_rank.groupby(\"topic_id\")[\"count_topic_terms\"].rank(\n            method=\"first\", ascending=False\n        )\n\n        df_rank = df_rank[df_rank[\"rank\"] &lt;= self.max_doc_per_topic]\n\n        # Create a dictionary of TopicRanking objects for each document\n        final_dict = {}\n        for doc_id, topic_id, rank in zip(\n            df_rank[\"doc_id\"], df_rank[\"topic_id\"], df_rank[\"rank\"]\n        ):\n            res = TopicRanking(topic_id=topic_id, rank=rank)\n            final_dict[doc_id] = res\n\n        # Update each document with its topic ranking\n        for doc in docs:\n            doc.topic_ranking = final_dict.get(doc.doc_id)\n\n        # Create a DataFrame for document content\n        df_content = pd.DataFrame.from_records([doc.model_dump() for doc in docs])\n        df_content = df_content[[\"doc_id\", \"content\"]]\n\n        # Merge document content with topic information\n        df_topics_rank = pd.merge(df_rank, df_content, on=\"doc_id\")\n        df_topics_rank = df_topics_rank[[\"topic_id\", \"content\"]]\n        df_topics_rank = df_topics_rank.groupby(\"topic_id\")[\"content\"].apply(list)\n\n        # Create a dictionary of top document content for each topic\n        dict_topic_rank = df_topics_rank.to_dict()\n\n        # Update each topic with its top document content\n        for topic in topics:\n            topic.top_doc_content = dict_topic_rank.get(topic.topic_id)\n\n        return docs, topics\n</code></pre>"},{"location":"bunka-api/topic_modeling/topic-ranker.html#bunkatopics.topic_modeling.document_topic_ranker.DocumentRanker.__init__","title":"<code>__init__(ranking_terms=20, max_doc_per_topic=20)</code>","text":"<p>Initialize the class with ranking_terms and max_doc_per_topic parameters.</p> <p>Parameters:</p> Name Type Description Default <code>ranking_terms</code> <code>int</code> <p>Number of ranking terms to be used.</p> <code>20</code> <code>max_doc_per_topic</code> <code>int</code> <p>Maximum number of documents per topic.</p> <code>20</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>bunkatopics/topic_modeling/document_topic_ranker.py</code> <pre><code>def __init__(self, ranking_terms: int = 20, max_doc_per_topic: int = 20) -&gt; None:\n    \"\"\"\n    Initialize the class with ranking_terms and max_doc_per_topic parameters.\n\n    Args:\n        ranking_terms (int): Number of ranking terms to be used.\n        max_doc_per_topic (int): Maximum number of documents per topic.\n\n    Returns:\n        None\n    \"\"\"\n    self.ranking_terms = ranking_terms\n    self.max_doc_per_topic = max_doc_per_topic\n</code></pre>"},{"location":"bunka-api/topic_modeling/topic-ranker.html#bunkatopics.topic_modeling.document_topic_ranker.DocumentRanker.fit_transform","title":"<code>fit_transform(docs, topics)</code>","text":"<p>Calculate top documents for each topic based on ranking terms.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of documents.</p> required <code>topics</code> <code>List[Topic]</code> <p>List of topics.</p> required <p>Returns:</p> Type Description <code>Tuple[List[Document], List[Topic]]</code> <p>Tuple[List[Document], List[Topic]]: Updated lists of documents and topics.</p> Source code in <code>bunkatopics/topic_modeling/document_topic_ranker.py</code> <pre><code>def fit_transform(\n    self,\n    docs: t.List[Document],\n    topics: t.List[Topic],\n) -&gt; t.Tuple[t.List[Document], t.List[Topic]]:\n    \"\"\"\n    Calculate top documents for each topic based on ranking terms.\n\n    Args:\n        docs (List[Document]): List of documents.\n        topics (List[Topic]): List of topics.\n\n    Returns:\n        Tuple[List[Document], List[Topic]]: Updated lists of documents and topics.\n    \"\"\"\n    # Create a DataFrame from the list of documents\n    df_docs = pd.DataFrame.from_records([doc.model_dump() for doc in docs])\n\n    # Explode the term_id column to have one row per term\n    df_docs = df_docs[[\"doc_id\", \"topic_id\", \"term_id\"]]\n    df_docs = df_docs.explode(\"term_id\").reset_index(drop=True)\n\n    # Create a DataFrame from the list of topics and truncate term_id\n    df_topics = pd.DataFrame.from_records([topic.model_dump() for topic in topics])\n    df_topics[\"term_id\"] = df_topics[\"term_id\"].apply(\n        lambda x: x[: self.ranking_terms]\n    )\n    df_topics = df_topics[[\"topic_id\", \"term_id\"]]\n    df_topics = df_topics.explode(\"term_id\").reset_index(drop=True)\n\n    # Merge documents and topics, and calculate term counts\n    df_rank = pd.merge(df_docs, df_topics, on=[\"topic_id\", \"term_id\"])\n    df_rank = (\n        df_rank.groupby([\"topic_id\", \"doc_id\"])[\"term_id\"]\n        .count()\n        .rename(\"count_topic_terms\")\n        .reset_index()\n    )\n\n    # Sort and rank documents within each topic\n    df_rank = df_rank.sort_values(\n        [\"topic_id\", \"count_topic_terms\"], ascending=(True, False)\n    ).reset_index(drop=True)\n    df_rank[\"rank\"] = df_rank.groupby(\"topic_id\")[\"count_topic_terms\"].rank(\n        method=\"first\", ascending=False\n    )\n\n    df_rank = df_rank[df_rank[\"rank\"] &lt;= self.max_doc_per_topic]\n\n    # Create a dictionary of TopicRanking objects for each document\n    final_dict = {}\n    for doc_id, topic_id, rank in zip(\n        df_rank[\"doc_id\"], df_rank[\"topic_id\"], df_rank[\"rank\"]\n    ):\n        res = TopicRanking(topic_id=topic_id, rank=rank)\n        final_dict[doc_id] = res\n\n    # Update each document with its topic ranking\n    for doc in docs:\n        doc.topic_ranking = final_dict.get(doc.doc_id)\n\n    # Create a DataFrame for document content\n    df_content = pd.DataFrame.from_records([doc.model_dump() for doc in docs])\n    df_content = df_content[[\"doc_id\", \"content\"]]\n\n    # Merge document content with topic information\n    df_topics_rank = pd.merge(df_rank, df_content, on=\"doc_id\")\n    df_topics_rank = df_topics_rank[[\"topic_id\", \"content\"]]\n    df_topics_rank = df_topics_rank.groupby(\"topic_id\")[\"content\"].apply(list)\n\n    # Create a dictionary of top document content for each topic\n    dict_topic_rank = df_topics_rank.to_dict()\n\n    # Update each topic with its top document content\n    for topic in topics:\n        topic.top_doc_content = dict_topic_rank.get(topic.topic_id)\n\n    return docs, topics\n</code></pre>"},{"location":"bunka-api/topic_modeling/topic_llm_cleaning.html","title":"Topic LLM Cleaner","text":"<p>A class for cleaning topic labels using a generative model.</p> <p>This class utilizes a language model to generate cleaned and more coherent labels for a given list of topics. The cleaning process considers the top documents and terms associated with each topic and optionally includes the actual content of the top documents for a more context-rich label generation.</p> Source code in <code>bunkatopics/topic_modeling/llm_topic_representation.py</code> <pre><code>class LLMCleaningTopic:\n    \"\"\"\n    A class for cleaning topic labels using a generative model.\n\n    This class utilizes a language model to generate cleaned and more coherent labels for a given list of topics.\n    The cleaning process considers the top documents and terms associated with each topic and optionally includes\n    the actual content of the top documents for a more context-rich label generation.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: LLM,\n        language: str = \"english\",\n        top_doc: int = 3,\n        top_terms: int = 10,\n        use_doc: bool = False,\n        context: str = \"everything\",\n    ) -&gt; None:\n        \"\"\"\n        Initialize the LLMCleaningTopic instance.\n\n        Arguments:\n            llm: The generative model to use for label cleaning.\n            language (str): Language used for generating labels. Defaults to \"english\".\n            top_doc (int): Number of top documents to consider for each topic. Defaults to 3.\n            top_terms (int): Number of top terms to consider for each topic. Defaults to 10.\n            use_doc (bool): Whether to include document contents in label generation. Defaults to False.\n            context (str): Context for label generation. Defaults to \"everything\".\n        \"\"\"\n        self.llm = llm\n        self.language = language\n        self.top_doc = top_doc\n        self.top_terms = top_terms\n        self.use_doc = use_doc\n        self.context = context\n\n    def fit_transform(\n        self, topics: t.List[Topic], docs: t.List[Document]\n    ) -&gt; t.List[Topic]:\n        \"\"\"\n        Clean topic labels for a list of topics using the generative model.\n\n        This method processes each topic by generating a new, cleaned label based on the top terms and documents\n        associated with the topic. The cleaned labels are then assigned back to the topics.\n\n        Args:\n            topics (List[Topic]): List of topics to clean.\n            docs (List[Document]): List of documents related to the topics.\n\n        \"\"\"\n        df = _get_df_prompt(topics, docs)\n\n        topic_ids = list(df[\"topic_id\"])\n        specific_terms = list(df[\"keywords\"])\n        top_doc_contents = list(df[\"content\"])\n\n        final_dict = {}\n        pbar = tqdm(total=len(topic_ids), desc=\"Creating new labels for clusters\")\n        for topic_ic, x, y in zip(topic_ids, specific_terms, top_doc_contents):\n            clean_topic_name = _get_clean_topic(\n                llm=self.llm,\n                language=self.language,\n                specific_terms=x,\n                specific_documents=y,\n                use_doc=self.use_doc,\n                top_terms=self.top_terms,\n                top_doc=self.top_doc,\n                context=self.context,\n            )\n            final_dict[topic_ic] = clean_topic_name\n            pbar.update(1)\n\n        for topic in topics:\n            topic.name = final_dict.get(topic.topic_id)\n\n        return topics\n</code></pre>"},{"location":"bunka-api/topic_modeling/topic_llm_cleaning.html#bunkatopics.topic_modeling.llm_topic_representation.LLMCleaningTopic.__init__","title":"<code>__init__(llm, language='english', top_doc=3, top_terms=10, use_doc=False, context='everything')</code>","text":"<p>Initialize the LLMCleaningTopic instance.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>LLM</code> <p>The generative model to use for label cleaning.</p> required <code>language</code> <code>str</code> <p>Language used for generating labels. Defaults to \"english\".</p> <code>'english'</code> <code>top_doc</code> <code>int</code> <p>Number of top documents to consider for each topic. Defaults to 3.</p> <code>3</code> <code>top_terms</code> <code>int</code> <p>Number of top terms to consider for each topic. Defaults to 10.</p> <code>10</code> <code>use_doc</code> <code>bool</code> <p>Whether to include document contents in label generation. Defaults to False.</p> <code>False</code> <code>context</code> <code>str</code> <p>Context for label generation. Defaults to \"everything\".</p> <code>'everything'</code> Source code in <code>bunkatopics/topic_modeling/llm_topic_representation.py</code> <pre><code>def __init__(\n    self,\n    llm: LLM,\n    language: str = \"english\",\n    top_doc: int = 3,\n    top_terms: int = 10,\n    use_doc: bool = False,\n    context: str = \"everything\",\n) -&gt; None:\n    \"\"\"\n    Initialize the LLMCleaningTopic instance.\n\n    Arguments:\n        llm: The generative model to use for label cleaning.\n        language (str): Language used for generating labels. Defaults to \"english\".\n        top_doc (int): Number of top documents to consider for each topic. Defaults to 3.\n        top_terms (int): Number of top terms to consider for each topic. Defaults to 10.\n        use_doc (bool): Whether to include document contents in label generation. Defaults to False.\n        context (str): Context for label generation. Defaults to \"everything\".\n    \"\"\"\n    self.llm = llm\n    self.language = language\n    self.top_doc = top_doc\n    self.top_terms = top_terms\n    self.use_doc = use_doc\n    self.context = context\n</code></pre>"},{"location":"bunka-api/topic_modeling/topic_llm_cleaning.html#bunkatopics.topic_modeling.llm_topic_representation.LLMCleaningTopic.fit_transform","title":"<code>fit_transform(topics, docs)</code>","text":"<p>Clean topic labels for a list of topics using the generative model.</p> <p>This method processes each topic by generating a new, cleaned label based on the top terms and documents associated with the topic. The cleaned labels are then assigned back to the topics.</p> <p>Parameters:</p> Name Type Description Default <code>topics</code> <code>List[Topic]</code> <p>List of topics to clean.</p> required <code>docs</code> <code>List[Document]</code> <p>List of documents related to the topics.</p> required Source code in <code>bunkatopics/topic_modeling/llm_topic_representation.py</code> <pre><code>def fit_transform(\n    self, topics: t.List[Topic], docs: t.List[Document]\n) -&gt; t.List[Topic]:\n    \"\"\"\n    Clean topic labels for a list of topics using the generative model.\n\n    This method processes each topic by generating a new, cleaned label based on the top terms and documents\n    associated with the topic. The cleaned labels are then assigned back to the topics.\n\n    Args:\n        topics (List[Topic]): List of topics to clean.\n        docs (List[Document]): List of documents related to the topics.\n\n    \"\"\"\n    df = _get_df_prompt(topics, docs)\n\n    topic_ids = list(df[\"topic_id\"])\n    specific_terms = list(df[\"keywords\"])\n    top_doc_contents = list(df[\"content\"])\n\n    final_dict = {}\n    pbar = tqdm(total=len(topic_ids), desc=\"Creating new labels for clusters\")\n    for topic_ic, x, y in zip(topic_ids, specific_terms, top_doc_contents):\n        clean_topic_name = _get_clean_topic(\n            llm=self.llm,\n            language=self.language,\n            specific_terms=x,\n            specific_documents=y,\n            use_doc=self.use_doc,\n            top_terms=self.top_terms,\n            top_doc=self.top_doc,\n            context=self.context,\n        )\n        final_dict[topic_ic] = clean_topic_name\n        pbar.update(1)\n\n    for topic in topics:\n        topic.name = final_dict.get(topic.topic_id)\n\n    return topics\n</code></pre>"},{"location":"bunka-api/topic_modeling/topic_model_builder.html","title":"BunkaTopicModeling","text":"<p>A class to perform topic modeling on a set of documents.</p> <p>This class utilizes clustering (default KMeans) to identify topics within a collection of documents. Each document and term is represented by embeddings, and topics are formed based on these embeddings. Topics are named using the top terms associated with them.</p> Source code in <code>bunkatopics/topic_modeling/topic_model_builder.py</code> <pre><code>class BunkaTopicModeling:\n    \"\"\"\n    A class to perform topic modeling on a set of documents.\n\n    This class utilizes clustering (default KMeans) to identify topics within a collection of documents.\n    Each document and term is represented by embeddings, and topics are formed based on these embeddings.\n    Topics are named using the top terms associated with them.\"\"\"\n\n    def __init__(\n        self,\n        n_clusters: int = 10,\n        ngrams: list = [1, 2],\n        name_length: int = 15,\n        top_terms_overall: int = 1000,\n        min_count_terms: int = 2,\n        min_docs_per_cluster: int = 10,\n        x_column: str = \"x\",\n        y_column: str = \"y\",\n        custom_clustering_model=None,\n    ) -&gt; None:\n        \"\"\"Constructs all the necessary attributes for the BunkaTopicModeling object.\n\n        Arguments:\n            n_clusters (int, optional): Number of clusters for K-Means. Defaults to 10.\n            ngrams (list, optional): List of n-gram lengths to consider. Defaults to [1, 2].\n            name_length (int, optional): Maximum length of topic names. Defaults to 15.\n            top_terms_overall (int, optional): Number of top terms to consider overall. Defaults to 1000.\n            min_count_terms (int, optional): Minimum count of terms to be considered. Defaults to 2.\n            min_docs_per_cluster (int, optional): Minimum count of documents per topic\n            x_column (str, optional): Column name for x-coordinate in the DataFrame. Defaults to \"x\".\n            y_column (str, optional): Column name for y-coordinate in the DataFrame. Defaults to \"y\".\n            custom_clustering_model (optional): Custom clustering model instance, if any. Defaults to None.\n        \"\"\"\n\n        self.n_clusters = n_clusters\n        self.ngrams = ngrams\n        self.name_length = name_length\n        self.top_terms_overall = top_terms_overall\n        self.min_count_terms = min_count_terms\n        self.x_column = x_column\n        self.y_column = y_column\n        self.custom_clustering_model = custom_clustering_model\n        self.min_docs_per_cluster = min_docs_per_cluster\n\n    def fit_transform(\n        self,\n        docs: t.List[Document],\n        terms: t.List[Term],\n    ) -&gt; t.List[Topic]:\n        \"\"\"\n        Analyzes documents and terms to form topics, assigns names to these topics based on the top terms,\n        and returns a list of Topic instances.\n\n        This method performs clustering on the document embeddings to identify distinct topics.\n        Each topic is named based on the top terms associated with it. The method also calculates\n        additional topic properties such as centroid coordinates and convex hulls.\n\n        Arguments:\n            docs (List[[Document]): List of Document objects representing the documents to be analyzed.\n            terms (List[Term]): List of Term objects representing the terms to be considered in topic naming.\n        Returns:\n            List[Topic]: A list of Topic objects, each representing a discovered topic with attributes\n                     like name, size, centroid coordinates, and convex hull.\n\n        Notes:\n            - If a custom clustering model is not provided, the method defaults to using KMeans for clustering.\n            - Topics are named using the most significant terms within each cluster.\n            - The method calculates the centroid and convex hull for each topic based on the document embeddings.\n        \"\"\"\n\n        # Rest of the function remains the same...\n\n        x_values = [getattr(doc, self.x_column) for doc in docs]\n        y_values = [getattr(doc, self.y_column) for doc in docs]\n\n        # Rest of the function remains unchanged...\n\n        df_embeddings_2D = pd.DataFrame(\n            {\n                \"doc_id\": [doc.doc_id for doc in docs],\n                self.x_column: x_values,\n                self.y_column: y_values,\n            }\n        )\n        df_embeddings_2D = df_embeddings_2D.set_index(\"doc_id\")\n\n        if self.custom_clustering_model is None:\n            clustering_model = KMeans(\n                n_clusters=self.n_clusters, n_init=\"auto\", random_state=42\n            )\n\n        else:\n            clustering_model = self.custom_clustering_model\n\n        df_embeddings_2D[\"topic_number\"] = clustering_model.fit(\n            df_embeddings_2D\n        ).labels_.astype(str)\n\n        df_embeddings_2D[\"topic_id\"] = \"bt\" + \"-\" + df_embeddings_2D[\"topic_number\"]\n\n        topic_doc_dict = df_embeddings_2D[\"topic_id\"].to_dict()\n        for doc in docs:\n            doc.topic_id = topic_doc_dict.get(doc.doc_id, [])\n\n        terms = [x for x in terms if x.count_terms &gt;= self.min_count_terms]\n\n        df_terms = pd.DataFrame.from_records([term.model_dump() for term in terms])\n        df_terms = df_terms.sort_values(\"count_terms\", ascending=False)\n        df_terms = df_terms.head(self.top_terms_overall)\n        df_terms = df_terms[df_terms[\"ngrams\"].isin(self.ngrams)]\n\n        df_terms_indexed = pd.DataFrame.from_records([doc.model_dump() for doc in docs])\n\n        df_terms_indexed = df_terms_indexed[[\"doc_id\", \"term_id\", \"topic_id\"]]\n        df_terms_indexed = df_terms_indexed.explode(\"term_id\").reset_index(drop=True)\n\n        df_terms_topics = pd.merge(df_terms_indexed, df_terms, on=\"term_id\")\n\n        df_topics_rep = specificity(\n            df_terms_topics, X=\"topic_id\", Y=\"term_id\", Z=None, top_n=500\n        )\n        df_topics_rep = (\n            df_topics_rep.groupby(\"topic_id\")[\"term_id\"].apply(list).reset_index()\n        )\n        df_topics_rep[\"name\"] = df_topics_rep[\"term_id\"].apply(lambda x: x[:100])\n        df_topics_rep[\"name\"] = df_topics_rep[\"name\"].apply(lambda x: clean_terms(x))\n\n        df_topics_rep[\"name\"] = df_topics_rep[\"name\"].apply(\n            lambda x: x[: self.name_length]\n        )\n        df_topics_rep[\"name\"] = df_topics_rep[\"name\"].apply(lambda x: \" | \".join(x))\n\n        topics = [Topic(**x) for x in df_topics_rep.to_dict(orient=\"records\")]\n\n        df_topics_docs = pd.DataFrame.from_records([doc.model_dump() for doc in docs])\n        df_topics_docs = df_topics_docs[[\"doc_id\", \"x\", \"y\", \"topic_id\"]]\n        df_topics_docs = df_topics_docs.groupby(\"topic_id\").agg(\n            size=(\"doc_id\", \"count\"), x_centroid=(\"x\", \"mean\"), y_centroid=(\"y\", \"mean\")\n        )\n\n        topic_dict = df_topics_docs[[\"size\", \"x_centroid\", \"y_centroid\"]].to_dict(\n            \"index\"\n        )\n\n        for topic in topics:\n            topic.size = topic_dict[topic.topic_id][\"size\"]\n            topic.x_centroid = topic_dict[topic.topic_id][\"x_centroid\"]\n            topic.y_centroid = topic_dict[topic.topic_id][\"y_centroid\"]\n\n        # remove too small clusters\n        topics = [x for x in topics if x.size &gt;= self.min_docs_per_cluster]\n\n        try:\n            for x in topics:\n                topic_id = x.topic_id\n                x_points = [doc.x for doc in docs if doc.topic_id == topic_id]\n                y_points = [doc.y for doc in docs if doc.topic_id == topic_id]\n\n                points = pd.DataFrame({\"x\": x_points, \"y\": y_points}).values\n\n                x_ch, y_ch = get_convex_hull_coord(points, interpolate_curve=True)\n                x_ch = list(x_ch)\n                y_ch = list(y_ch)\n\n                res = ConvexHullModel(x_coordinates=x_ch, y_coordinates=y_ch)\n                x.convex_hull = res\n        except Exception as e:\n            print(e)\n\n        # Remove in case of HDBSCAN ?\n\n        return topics\n</code></pre>"},{"location":"bunka-api/topic_modeling/topic_model_builder.html#bunkatopics.topic_modeling.topic_model_builder.BunkaTopicModeling.__init__","title":"<code>__init__(n_clusters=10, ngrams=[1, 2], name_length=15, top_terms_overall=1000, min_count_terms=2, min_docs_per_cluster=10, x_column='x', y_column='y', custom_clustering_model=None)</code>","text":"<p>Constructs all the necessary attributes for the BunkaTopicModeling object.</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>int</code> <p>Number of clusters for K-Means. Defaults to 10.</p> <code>10</code> <code>ngrams</code> <code>list</code> <p>List of n-gram lengths to consider. Defaults to [1, 2].</p> <code>[1, 2]</code> <code>name_length</code> <code>int</code> <p>Maximum length of topic names. Defaults to 15.</p> <code>15</code> <code>top_terms_overall</code> <code>int</code> <p>Number of top terms to consider overall. Defaults to 1000.</p> <code>1000</code> <code>min_count_terms</code> <code>int</code> <p>Minimum count of terms to be considered. Defaults to 2.</p> <code>2</code> <code>min_docs_per_cluster</code> <code>int</code> <p>Minimum count of documents per topic</p> <code>10</code> <code>x_column</code> <code>str</code> <p>Column name for x-coordinate in the DataFrame. Defaults to \"x\".</p> <code>'x'</code> <code>y_column</code> <code>str</code> <p>Column name for y-coordinate in the DataFrame. Defaults to \"y\".</p> <code>'y'</code> <code>custom_clustering_model</code> <code>optional</code> <p>Custom clustering model instance, if any. Defaults to None.</p> <code>None</code> Source code in <code>bunkatopics/topic_modeling/topic_model_builder.py</code> <pre><code>def __init__(\n    self,\n    n_clusters: int = 10,\n    ngrams: list = [1, 2],\n    name_length: int = 15,\n    top_terms_overall: int = 1000,\n    min_count_terms: int = 2,\n    min_docs_per_cluster: int = 10,\n    x_column: str = \"x\",\n    y_column: str = \"y\",\n    custom_clustering_model=None,\n) -&gt; None:\n    \"\"\"Constructs all the necessary attributes for the BunkaTopicModeling object.\n\n    Arguments:\n        n_clusters (int, optional): Number of clusters for K-Means. Defaults to 10.\n        ngrams (list, optional): List of n-gram lengths to consider. Defaults to [1, 2].\n        name_length (int, optional): Maximum length of topic names. Defaults to 15.\n        top_terms_overall (int, optional): Number of top terms to consider overall. Defaults to 1000.\n        min_count_terms (int, optional): Minimum count of terms to be considered. Defaults to 2.\n        min_docs_per_cluster (int, optional): Minimum count of documents per topic\n        x_column (str, optional): Column name for x-coordinate in the DataFrame. Defaults to \"x\".\n        y_column (str, optional): Column name for y-coordinate in the DataFrame. Defaults to \"y\".\n        custom_clustering_model (optional): Custom clustering model instance, if any. Defaults to None.\n    \"\"\"\n\n    self.n_clusters = n_clusters\n    self.ngrams = ngrams\n    self.name_length = name_length\n    self.top_terms_overall = top_terms_overall\n    self.min_count_terms = min_count_terms\n    self.x_column = x_column\n    self.y_column = y_column\n    self.custom_clustering_model = custom_clustering_model\n    self.min_docs_per_cluster = min_docs_per_cluster\n</code></pre>"},{"location":"bunka-api/topic_modeling/topic_model_builder.html#bunkatopics.topic_modeling.topic_model_builder.BunkaTopicModeling.fit_transform","title":"<code>fit_transform(docs, terms)</code>","text":"<p>Analyzes documents and terms to form topics, assigns names to these topics based on the top terms, and returns a list of Topic instances.</p> <p>This method performs clustering on the document embeddings to identify distinct topics. Each topic is named based on the top terms associated with it. The method also calculates additional topic properties such as centroid coordinates and convex hulls.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[[Document]</code> <p>List of Document objects representing the documents to be analyzed.</p> required <code>terms</code> <code>List[Term]</code> <p>List of Term objects representing the terms to be considered in topic naming.</p> required <p>Returns:     List[Topic]: A list of Topic objects, each representing a discovered topic with attributes              like name, size, centroid coordinates, and convex hull.</p> Notes <ul> <li>If a custom clustering model is not provided, the method defaults to using KMeans for clustering.</li> <li>Topics are named using the most significant terms within each cluster.</li> <li>The method calculates the centroid and convex hull for each topic based on the document embeddings.</li> </ul> Source code in <code>bunkatopics/topic_modeling/topic_model_builder.py</code> <pre><code>def fit_transform(\n    self,\n    docs: t.List[Document],\n    terms: t.List[Term],\n) -&gt; t.List[Topic]:\n    \"\"\"\n    Analyzes documents and terms to form topics, assigns names to these topics based on the top terms,\n    and returns a list of Topic instances.\n\n    This method performs clustering on the document embeddings to identify distinct topics.\n    Each topic is named based on the top terms associated with it. The method also calculates\n    additional topic properties such as centroid coordinates and convex hulls.\n\n    Arguments:\n        docs (List[[Document]): List of Document objects representing the documents to be analyzed.\n        terms (List[Term]): List of Term objects representing the terms to be considered in topic naming.\n    Returns:\n        List[Topic]: A list of Topic objects, each representing a discovered topic with attributes\n                 like name, size, centroid coordinates, and convex hull.\n\n    Notes:\n        - If a custom clustering model is not provided, the method defaults to using KMeans for clustering.\n        - Topics are named using the most significant terms within each cluster.\n        - The method calculates the centroid and convex hull for each topic based on the document embeddings.\n    \"\"\"\n\n    # Rest of the function remains the same...\n\n    x_values = [getattr(doc, self.x_column) for doc in docs]\n    y_values = [getattr(doc, self.y_column) for doc in docs]\n\n    # Rest of the function remains unchanged...\n\n    df_embeddings_2D = pd.DataFrame(\n        {\n            \"doc_id\": [doc.doc_id for doc in docs],\n            self.x_column: x_values,\n            self.y_column: y_values,\n        }\n    )\n    df_embeddings_2D = df_embeddings_2D.set_index(\"doc_id\")\n\n    if self.custom_clustering_model is None:\n        clustering_model = KMeans(\n            n_clusters=self.n_clusters, n_init=\"auto\", random_state=42\n        )\n\n    else:\n        clustering_model = self.custom_clustering_model\n\n    df_embeddings_2D[\"topic_number\"] = clustering_model.fit(\n        df_embeddings_2D\n    ).labels_.astype(str)\n\n    df_embeddings_2D[\"topic_id\"] = \"bt\" + \"-\" + df_embeddings_2D[\"topic_number\"]\n\n    topic_doc_dict = df_embeddings_2D[\"topic_id\"].to_dict()\n    for doc in docs:\n        doc.topic_id = topic_doc_dict.get(doc.doc_id, [])\n\n    terms = [x for x in terms if x.count_terms &gt;= self.min_count_terms]\n\n    df_terms = pd.DataFrame.from_records([term.model_dump() for term in terms])\n    df_terms = df_terms.sort_values(\"count_terms\", ascending=False)\n    df_terms = df_terms.head(self.top_terms_overall)\n    df_terms = df_terms[df_terms[\"ngrams\"].isin(self.ngrams)]\n\n    df_terms_indexed = pd.DataFrame.from_records([doc.model_dump() for doc in docs])\n\n    df_terms_indexed = df_terms_indexed[[\"doc_id\", \"term_id\", \"topic_id\"]]\n    df_terms_indexed = df_terms_indexed.explode(\"term_id\").reset_index(drop=True)\n\n    df_terms_topics = pd.merge(df_terms_indexed, df_terms, on=\"term_id\")\n\n    df_topics_rep = specificity(\n        df_terms_topics, X=\"topic_id\", Y=\"term_id\", Z=None, top_n=500\n    )\n    df_topics_rep = (\n        df_topics_rep.groupby(\"topic_id\")[\"term_id\"].apply(list).reset_index()\n    )\n    df_topics_rep[\"name\"] = df_topics_rep[\"term_id\"].apply(lambda x: x[:100])\n    df_topics_rep[\"name\"] = df_topics_rep[\"name\"].apply(lambda x: clean_terms(x))\n\n    df_topics_rep[\"name\"] = df_topics_rep[\"name\"].apply(\n        lambda x: x[: self.name_length]\n    )\n    df_topics_rep[\"name\"] = df_topics_rep[\"name\"].apply(lambda x: \" | \".join(x))\n\n    topics = [Topic(**x) for x in df_topics_rep.to_dict(orient=\"records\")]\n\n    df_topics_docs = pd.DataFrame.from_records([doc.model_dump() for doc in docs])\n    df_topics_docs = df_topics_docs[[\"doc_id\", \"x\", \"y\", \"topic_id\"]]\n    df_topics_docs = df_topics_docs.groupby(\"topic_id\").agg(\n        size=(\"doc_id\", \"count\"), x_centroid=(\"x\", \"mean\"), y_centroid=(\"y\", \"mean\")\n    )\n\n    topic_dict = df_topics_docs[[\"size\", \"x_centroid\", \"y_centroid\"]].to_dict(\n        \"index\"\n    )\n\n    for topic in topics:\n        topic.size = topic_dict[topic.topic_id][\"size\"]\n        topic.x_centroid = topic_dict[topic.topic_id][\"x_centroid\"]\n        topic.y_centroid = topic_dict[topic.topic_id][\"y_centroid\"]\n\n    # remove too small clusters\n    topics = [x for x in topics if x.size &gt;= self.min_docs_per_cluster]\n\n    try:\n        for x in topics:\n            topic_id = x.topic_id\n            x_points = [doc.x for doc in docs if doc.topic_id == topic_id]\n            y_points = [doc.y for doc in docs if doc.topic_id == topic_id]\n\n            points = pd.DataFrame({\"x\": x_points, \"y\": y_points}).values\n\n            x_ch, y_ch = get_convex_hull_coord(points, interpolate_curve=True)\n            x_ch = list(x_ch)\n            y_ch = list(y_ch)\n\n            res = ConvexHullModel(x_coordinates=x_ch, y_coordinates=y_ch)\n            x.convex_hull = res\n    except Exception as e:\n        print(e)\n\n    # Remove in case of HDBSCAN ?\n\n    return topics\n</code></pre>"},{"location":"bunka-api/visualization/topic-visualization.html","title":"Topic Modeling Visualization","text":"<p>A class for visualizing topics and their associated documents in a 2D density Map.</p> <p>This visualizer plots documents and topics on a 2D space with an option to show text labels, contour density representations, and topic centroids. The visualization is useful for understanding the distribution and clustering of topics in a document corpus.</p> Source code in <code>bunkatopics/visualization/topic_visualizer.py</code> <pre><code>class TopicVisualizer:\n    \"\"\"\n    A class for visualizing topics and their associated documents in a 2D density Map.\n\n    This visualizer plots documents and topics on a 2D space with an option to show text labels,\n    contour density representations, and topic centroids. The visualization is useful for\n    understanding the distribution and clustering of topics in a document corpus.\n    \"\"\"\n\n    def __init__(\n        self,\n        show_text=False,\n        width=1000,\n        height=1000,\n        label_size_ratio=100,\n        point_size_ratio=150,\n        colorscale=\"delta\",\n        density: bool = False,\n        convex_hull: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the TopicVisualizer with specified parameters.\n\n        Args:\n            show_text (bool): If True, text labels are displayed on the plot. Defaults to False.\n            width (int): The width of the plot in pixels. Defaults to 1000.\n            height (int): The height of the plot in pixels. Defaults to 1000.\n            label_size_ratio (int): The size ratio for label text. Defaults to 100.\n            colorscale (str): The color scale for contour density representation. Defaults to \"delta\".\n            density (bool): Whether to display a density map\n            convex_hull (bool): Whether to display lines around the clusters\n        \"\"\"\n        self.show_text = show_text\n        self.width = width\n        self.height = height\n        self.label_size_ratio = label_size_ratio\n        self.point_size_ratio = point_size_ratio\n        self.colorscale = colorscale\n        self.density = density\n        self.convex_hull = convex_hull\n\n        self.colorscale_list = [\n            \"Greys\",\n            \"YlGnBu\",\n            \"Greens\",\n            \"YlOrRd\",\n            \"Bluered\",\n            \"RdBu\",\n            \"Reds\",\n            \"Blues\",\n            \"Picnic\",\n            \"Rainbow\",\n            \"Portland\",\n            \"Jet\",\n            \"Hot\",\n            \"Blackbody\",\n            \"Earth\",\n            \"Electric\",\n            \"Viridis\",\n            \"Cividis\",\n            \"Inferno\",\n            \"Magma\",\n            \"Plasma\",\n        ]\n\n    def fit_transform(\n        self,\n        docs: t.List[Document],\n        topics: t.List[Topic],\n        color: str = None,\n    ) -&gt; go.Figure:\n        \"\"\"\n        Generates a Plotly figure visualizing the given documents and topics.\n\n        This method processes the documents and topics to create a 2D scatter plot,\n        showing the distribution and clustering of topics. It supports displaying text labels,\n        contour density, and centroids for topics.\n\n        Args:\n            docs (List[Document]): A list of Document objects to be visualized.\n            topics (List[Topic]): A list of Topic objects for clustering visualization.\n            color (str): The metadata field to use for coloring the documents. Defaults to None.\n\n        Returns:\n            go.Figure: A Plotly figure object representing the visualized documents and topics.\n        \"\"\"\n\n        docs_x = [doc.x for doc in docs]\n        docs_y = [doc.y for doc in docs]\n        docs_topic_id = [doc.topic_id for doc in docs]\n        docs_content = [doc.content for doc in docs]\n        docs_content_plotly = [wrap_by_word(x, 10) for x in docs_content]\n\n        topics_x = [topic.x_centroid for topic in topics]\n        topics_y = [topic.y_centroid for topic in topics]\n        topics_name = [topic.name for topic in topics]\n        topics_name_plotly = [wrap_by_word(x, 6) for x in topics_name]\n\n        if color is not None:\n            self.density = None\n\n        if self.density:\n            # Create a figure with Histogram2dContour\n            fig_density = go.Figure(\n                go.Histogram2dContour(\n                    x=docs_x,\n                    y=docs_y,\n                    colorscale=self.colorscale,\n                    showscale=False,\n                    hoverinfo=\"none\",\n                )\n            )\n\n            fig_density.update_traces(\n                contours_coloring=\"fill\", contours_showlabels=False\n            )\n\n        else:\n            fig_density = go.Figure()\n\n        # Update layout settings\n        fig_density.update_layout(\n            font_size=25,\n            width=self.width,\n            height=self.height,\n            margin=dict(\n                t=self.width / 50,\n                b=self.width / 50,\n                r=self.width / 50,\n                l=self.width / 50,\n            ),\n            title=dict(font=dict(size=self.width / 40)),\n        )\n\n        nk = np.empty(shape=(len(docs_content), 3, 1), dtype=\"object\")\n        nk[:, 0] = np.array(docs_topic_id).reshape(-1, 1)\n        nk[:, 1] = np.array(docs_content_plotly).reshape(-1, 1)\n\n        if color is not None:\n            list_color = [x.metadata[color] for x in docs]\n            nk[:, 2] = np.array(list_color).reshape(-1, 1)\n            hovertemplate = f\"&lt;br&gt;%{{customdata[1]}}&lt;br&gt;{color}: %{{customdata[2]}}\"\n        else:\n            hovertemplate = \"&lt;br&gt;%{customdata[1]}&lt;br&gt;\"\n\n        def extend_color_palette(number_of_categories):\n            list_of_colors = px.colors.qualitative.Dark24\n            extended_list_of_colors = (\n                list_of_colors * (number_of_categories // len(list_of_colors))\n                + list_of_colors[: number_of_categories % len(list_of_colors)]\n            )\n            return extended_list_of_colors\n\n        if color is not None:\n            if len(list_color) &gt; 24:\n                list_of_colors = extend_color_palette(len(list_color))\n            else:\n                list_of_colors = px.colors.qualitative.Dark24\n\n        if color is not None:\n            if check_list_type(list_color) == \"string\":\n                unique_categories = list(set(list_color))\n                colormap = {\n                    category: list_of_colors[i]\n                    for i, category in enumerate(unique_categories)\n                }\n                list_color_figure = [colormap[value] for value in list_color]\n                colorscale = None\n                colorbar = None\n\n            else:\n                list_color_figure = list_color\n                colorscale = \"RdBu\"\n                colorbar = dict(title=color)\n\n        # if search is not None:\n        #     from .visualization_utils import normalize_list\n\n        #     docs_search = self.vectorstore.similarity_search_with_score(\n        #         search, k=len(self.vectorstore.get()[\"documents\"])\n        #     )\n        #     similarity_score = [doc[1] for doc in docs_search]\n        #     similarity_score_norm = normalize_list(similarity_score)\n        #     similarity_score_norm = [1 - doc for doc in similarity_score_norm]\n\n        #     docs_search = {\n        #         \"doc_id\": [doc[0].metadata[\"doc_id\"] for doc in docs_search],\n        #         \"score\": [score for score in similarity_score_norm],\n        #         \"page_content\": [doc[0].page_content for doc in docs_search],\n        #     }\n\n        #     list_color_figure = docs_search[\"score\"]\n        #     colorscale = \"RdBu\"\n        #     colorbar = dict(title=\"Semantic Similarity\")\n\n        else:\n            list_color_figure = None\n            colorscale = None\n            colorbar = None\n\n        if self.show_text:\n            # Add points with information\n            fig_density.add_trace(\n                go.Scatter(\n                    x=docs_x,\n                    y=docs_y,\n                    mode=\"markers\",\n                    marker=dict(\n                        color=list_color_figure,  # Assigning colors based on the list_color\n                        size=self.width / self.point_size_ratio,\n                        # size=10,  # Adjust the size of the markers as needed\n                        opacity=0.5,  # Adjust the opacity of the markers as needed\n                        colorscale=colorscale,  # You can specify a colorscale if needed\n                        colorbar=colorbar,  # Optional colorbar title\n                    ),\n                    showlegend=False,\n                    customdata=nk,\n                    hovertemplate=hovertemplate,\n                ),\n            )\n\n        if color is not None:\n            if check_list_type(list_color) == \"string\":\n                # Create legend based on categories\n                legend_items = []\n                for category, color_item in colormap.items():\n                    legend_items.append(\n                        go.Scatter(\n                            x=[None],\n                            y=[None],\n                            mode=\"markers\",\n                            marker=dict(color=color_item),\n                            name=category,\n                        )\n                    )\n\n                # Add legend items to the figure\n                for item in legend_items:\n                    fig_density.add_trace(item)\n\n        # Add centroids labels\n        for x, y, label in zip(topics_x, topics_y, topics_name_plotly):\n            fig_density.add_annotation(\n                x=x,\n                y=y,\n                text=label,\n                showarrow=True,\n                arrowhead=1,\n                font=dict(\n                    family=\"Courier New, monospace\",\n                    size=self.width / self.label_size_ratio,\n                    color=\"blue\",\n                ),\n                bordercolor=\"#c7c7c7\",\n                borderwidth=self.width / 1000,\n                borderpad=self.width / 500,\n                bgcolor=\"white\",\n                opacity=1,\n                arrowcolor=\"#ff7f0e\",\n            )\n\n        if self.convex_hull:\n            try:\n                for topic in topics:\n                    # Create a Scatter plot with the convex hull coordinates\n                    trace = go.Scatter(\n                        x=topic.convex_hull.x_coordinates,\n                        y=topic.convex_hull.y_coordinates,\n                        mode=\"lines\",\n                        name=\"Convex Hull\",\n                        line=dict(color=\"grey\", dash=\"dot\"),\n                        hoverinfo=\"none\",\n                        showlegend=False,\n                    )\n                    fig_density.add_trace(trace)\n            except Exception as e:\n                print(e)\n\n        if color is not None:\n            fig_density.update_layout(\n                legend_title_text=color,\n                legend=dict(\n                    font=dict(\n                        family=\"Arial\",\n                        size=int(self.width / 60),  # Adjust font size of the legend\n                        color=\"black\",\n                    ),\n                ),\n            )\n\n            fig_density.update_layout(plot_bgcolor=\"white\")\n\n        # fig_density.update_layout(showlegend=True)\n        fig_density.update_xaxes(showgrid=False, showticklabels=False, zeroline=False)\n        fig_density.update_yaxes(showgrid=False, showticklabels=False, zeroline=False)\n        fig_density.update_yaxes(showticklabels=False)\n\n        return fig_density\n</code></pre>"},{"location":"bunka-api/visualization/topic-visualization.html#bunkatopics.visualization.topic_visualizer.TopicVisualizer.__init__","title":"<code>__init__(show_text=False, width=1000, height=1000, label_size_ratio=100, point_size_ratio=150, colorscale='delta', density=False, convex_hull=False)</code>","text":"<p>Initializes the TopicVisualizer with specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>show_text</code> <code>bool</code> <p>If True, text labels are displayed on the plot. Defaults to False.</p> <code>False</code> <code>width</code> <code>int</code> <p>The width of the plot in pixels. Defaults to 1000.</p> <code>1000</code> <code>height</code> <code>int</code> <p>The height of the plot in pixels. Defaults to 1000.</p> <code>1000</code> <code>label_size_ratio</code> <code>int</code> <p>The size ratio for label text. Defaults to 100.</p> <code>100</code> <code>colorscale</code> <code>str</code> <p>The color scale for contour density representation. Defaults to \"delta\".</p> <code>'delta'</code> <code>density</code> <code>bool</code> <p>Whether to display a density map</p> <code>False</code> <code>convex_hull</code> <code>bool</code> <p>Whether to display lines around the clusters</p> <code>False</code> Source code in <code>bunkatopics/visualization/topic_visualizer.py</code> <pre><code>def __init__(\n    self,\n    show_text=False,\n    width=1000,\n    height=1000,\n    label_size_ratio=100,\n    point_size_ratio=150,\n    colorscale=\"delta\",\n    density: bool = False,\n    convex_hull: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initializes the TopicVisualizer with specified parameters.\n\n    Args:\n        show_text (bool): If True, text labels are displayed on the plot. Defaults to False.\n        width (int): The width of the plot in pixels. Defaults to 1000.\n        height (int): The height of the plot in pixels. Defaults to 1000.\n        label_size_ratio (int): The size ratio for label text. Defaults to 100.\n        colorscale (str): The color scale for contour density representation. Defaults to \"delta\".\n        density (bool): Whether to display a density map\n        convex_hull (bool): Whether to display lines around the clusters\n    \"\"\"\n    self.show_text = show_text\n    self.width = width\n    self.height = height\n    self.label_size_ratio = label_size_ratio\n    self.point_size_ratio = point_size_ratio\n    self.colorscale = colorscale\n    self.density = density\n    self.convex_hull = convex_hull\n\n    self.colorscale_list = [\n        \"Greys\",\n        \"YlGnBu\",\n        \"Greens\",\n        \"YlOrRd\",\n        \"Bluered\",\n        \"RdBu\",\n        \"Reds\",\n        \"Blues\",\n        \"Picnic\",\n        \"Rainbow\",\n        \"Portland\",\n        \"Jet\",\n        \"Hot\",\n        \"Blackbody\",\n        \"Earth\",\n        \"Electric\",\n        \"Viridis\",\n        \"Cividis\",\n        \"Inferno\",\n        \"Magma\",\n        \"Plasma\",\n    ]\n</code></pre>"},{"location":"bunka-api/visualization/topic-visualization.html#bunkatopics.visualization.topic_visualizer.TopicVisualizer.fit_transform","title":"<code>fit_transform(docs, topics, color=None)</code>","text":"<p>Generates a Plotly figure visualizing the given documents and topics.</p> <p>This method processes the documents and topics to create a 2D scatter plot, showing the distribution and clustering of topics. It supports displaying text labels, contour density, and centroids for topics.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>A list of Document objects to be visualized.</p> required <code>topics</code> <code>List[Topic]</code> <p>A list of Topic objects for clustering visualization.</p> required <code>color</code> <code>str</code> <p>The metadata field to use for coloring the documents. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>go.Figure: A Plotly figure object representing the visualized documents and topics.</p> Source code in <code>bunkatopics/visualization/topic_visualizer.py</code> <pre><code>def fit_transform(\n    self,\n    docs: t.List[Document],\n    topics: t.List[Topic],\n    color: str = None,\n) -&gt; go.Figure:\n    \"\"\"\n    Generates a Plotly figure visualizing the given documents and topics.\n\n    This method processes the documents and topics to create a 2D scatter plot,\n    showing the distribution and clustering of topics. It supports displaying text labels,\n    contour density, and centroids for topics.\n\n    Args:\n        docs (List[Document]): A list of Document objects to be visualized.\n        topics (List[Topic]): A list of Topic objects for clustering visualization.\n        color (str): The metadata field to use for coloring the documents. Defaults to None.\n\n    Returns:\n        go.Figure: A Plotly figure object representing the visualized documents and topics.\n    \"\"\"\n\n    docs_x = [doc.x for doc in docs]\n    docs_y = [doc.y for doc in docs]\n    docs_topic_id = [doc.topic_id for doc in docs]\n    docs_content = [doc.content for doc in docs]\n    docs_content_plotly = [wrap_by_word(x, 10) for x in docs_content]\n\n    topics_x = [topic.x_centroid for topic in topics]\n    topics_y = [topic.y_centroid for topic in topics]\n    topics_name = [topic.name for topic in topics]\n    topics_name_plotly = [wrap_by_word(x, 6) for x in topics_name]\n\n    if color is not None:\n        self.density = None\n\n    if self.density:\n        # Create a figure with Histogram2dContour\n        fig_density = go.Figure(\n            go.Histogram2dContour(\n                x=docs_x,\n                y=docs_y,\n                colorscale=self.colorscale,\n                showscale=False,\n                hoverinfo=\"none\",\n            )\n        )\n\n        fig_density.update_traces(\n            contours_coloring=\"fill\", contours_showlabels=False\n        )\n\n    else:\n        fig_density = go.Figure()\n\n    # Update layout settings\n    fig_density.update_layout(\n        font_size=25,\n        width=self.width,\n        height=self.height,\n        margin=dict(\n            t=self.width / 50,\n            b=self.width / 50,\n            r=self.width / 50,\n            l=self.width / 50,\n        ),\n        title=dict(font=dict(size=self.width / 40)),\n    )\n\n    nk = np.empty(shape=(len(docs_content), 3, 1), dtype=\"object\")\n    nk[:, 0] = np.array(docs_topic_id).reshape(-1, 1)\n    nk[:, 1] = np.array(docs_content_plotly).reshape(-1, 1)\n\n    if color is not None:\n        list_color = [x.metadata[color] for x in docs]\n        nk[:, 2] = np.array(list_color).reshape(-1, 1)\n        hovertemplate = f\"&lt;br&gt;%{{customdata[1]}}&lt;br&gt;{color}: %{{customdata[2]}}\"\n    else:\n        hovertemplate = \"&lt;br&gt;%{customdata[1]}&lt;br&gt;\"\n\n    def extend_color_palette(number_of_categories):\n        list_of_colors = px.colors.qualitative.Dark24\n        extended_list_of_colors = (\n            list_of_colors * (number_of_categories // len(list_of_colors))\n            + list_of_colors[: number_of_categories % len(list_of_colors)]\n        )\n        return extended_list_of_colors\n\n    if color is not None:\n        if len(list_color) &gt; 24:\n            list_of_colors = extend_color_palette(len(list_color))\n        else:\n            list_of_colors = px.colors.qualitative.Dark24\n\n    if color is not None:\n        if check_list_type(list_color) == \"string\":\n            unique_categories = list(set(list_color))\n            colormap = {\n                category: list_of_colors[i]\n                for i, category in enumerate(unique_categories)\n            }\n            list_color_figure = [colormap[value] for value in list_color]\n            colorscale = None\n            colorbar = None\n\n        else:\n            list_color_figure = list_color\n            colorscale = \"RdBu\"\n            colorbar = dict(title=color)\n\n    # if search is not None:\n    #     from .visualization_utils import normalize_list\n\n    #     docs_search = self.vectorstore.similarity_search_with_score(\n    #         search, k=len(self.vectorstore.get()[\"documents\"])\n    #     )\n    #     similarity_score = [doc[1] for doc in docs_search]\n    #     similarity_score_norm = normalize_list(similarity_score)\n    #     similarity_score_norm = [1 - doc for doc in similarity_score_norm]\n\n    #     docs_search = {\n    #         \"doc_id\": [doc[0].metadata[\"doc_id\"] for doc in docs_search],\n    #         \"score\": [score for score in similarity_score_norm],\n    #         \"page_content\": [doc[0].page_content for doc in docs_search],\n    #     }\n\n    #     list_color_figure = docs_search[\"score\"]\n    #     colorscale = \"RdBu\"\n    #     colorbar = dict(title=\"Semantic Similarity\")\n\n    else:\n        list_color_figure = None\n        colorscale = None\n        colorbar = None\n\n    if self.show_text:\n        # Add points with information\n        fig_density.add_trace(\n            go.Scatter(\n                x=docs_x,\n                y=docs_y,\n                mode=\"markers\",\n                marker=dict(\n                    color=list_color_figure,  # Assigning colors based on the list_color\n                    size=self.width / self.point_size_ratio,\n                    # size=10,  # Adjust the size of the markers as needed\n                    opacity=0.5,  # Adjust the opacity of the markers as needed\n                    colorscale=colorscale,  # You can specify a colorscale if needed\n                    colorbar=colorbar,  # Optional colorbar title\n                ),\n                showlegend=False,\n                customdata=nk,\n                hovertemplate=hovertemplate,\n            ),\n        )\n\n    if color is not None:\n        if check_list_type(list_color) == \"string\":\n            # Create legend based on categories\n            legend_items = []\n            for category, color_item in colormap.items():\n                legend_items.append(\n                    go.Scatter(\n                        x=[None],\n                        y=[None],\n                        mode=\"markers\",\n                        marker=dict(color=color_item),\n                        name=category,\n                    )\n                )\n\n            # Add legend items to the figure\n            for item in legend_items:\n                fig_density.add_trace(item)\n\n    # Add centroids labels\n    for x, y, label in zip(topics_x, topics_y, topics_name_plotly):\n        fig_density.add_annotation(\n            x=x,\n            y=y,\n            text=label,\n            showarrow=True,\n            arrowhead=1,\n            font=dict(\n                family=\"Courier New, monospace\",\n                size=self.width / self.label_size_ratio,\n                color=\"blue\",\n            ),\n            bordercolor=\"#c7c7c7\",\n            borderwidth=self.width / 1000,\n            borderpad=self.width / 500,\n            bgcolor=\"white\",\n            opacity=1,\n            arrowcolor=\"#ff7f0e\",\n        )\n\n    if self.convex_hull:\n        try:\n            for topic in topics:\n                # Create a Scatter plot with the convex hull coordinates\n                trace = go.Scatter(\n                    x=topic.convex_hull.x_coordinates,\n                    y=topic.convex_hull.y_coordinates,\n                    mode=\"lines\",\n                    name=\"Convex Hull\",\n                    line=dict(color=\"grey\", dash=\"dot\"),\n                    hoverinfo=\"none\",\n                    showlegend=False,\n                )\n                fig_density.add_trace(trace)\n        except Exception as e:\n            print(e)\n\n    if color is not None:\n        fig_density.update_layout(\n            legend_title_text=color,\n            legend=dict(\n                font=dict(\n                    family=\"Arial\",\n                    size=int(self.width / 60),  # Adjust font size of the legend\n                    color=\"black\",\n                ),\n            ),\n        )\n\n        fig_density.update_layout(plot_bgcolor=\"white\")\n\n    # fig_density.update_layout(showlegend=True)\n    fig_density.update_xaxes(showgrid=False, showticklabels=False, zeroline=False)\n    fig_density.update_yaxes(showgrid=False, showticklabels=False, zeroline=False)\n    fig_density.update_yaxes(showticklabels=False)\n\n    return fig_density\n</code></pre>"}]}